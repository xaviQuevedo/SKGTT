{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Analizador import AnalisisTexto\n",
    "import pymysql\n",
    "import sys\n",
    "import spacy\n",
    "from DBdata2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datos:\n",
    "    con= None;\n",
    "    global analisis \n",
    "    global data\n",
    "    data = Datos2()\n",
    "    analisis= AnalisisTexto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conexionDB():\n",
    "    try:\n",
    "        conexion = pymysql.connect(host='localhost', user='root', password='', db='data')\n",
    "    except (pymysql.err.OperationalError, pymysql.err.InternalError) as e:\n",
    "        print(\"OcurriÃ³ un error al conectar: \", e)\n",
    "    return conexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardarTripletas(id,s,p,o):\n",
    "    db = pymysql.connect(host='localhost', user='root', password='', db='data')\n",
    "    cur = db.cursor()\n",
    "    sql = \"\"\"INSERT INTO tripletas(Id ,sujeto, predicado, objeto) VALUES (%s, %s, %s, %s) \"\"\"\n",
    "    datos= (id,s,p,o)\n",
    "    cur.execute(sql, datos)\n",
    "    db.commit()\n",
    "    print (\"tripletas guardadas....\")\n",
    "    cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corregirResumen():\n",
    "    db = pymysql.connect(host='localhost', user='root', password='', db='data')\n",
    "    cur = db.cursor()\n",
    "    cur.execute (u\"\"\" UPDATE tripletas\n",
    "SET objeto = trim(substr(objeto, 1, INSTR(objeto, 'Keywords:')-1))\n",
    "WHERE predicado = 'https://w3id.org/scholarlydata/ontology/conference-ontology.owl#abstract'\n",
    "AND objeto LIKE '%Keywords:%'; \"\"\")\n",
    "    db.commit()\n",
    "    cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraerDatos():\n",
    "    db = pymysql.connect(host='localhost', user='root', password='', db='data')\n",
    "    cur = db.cursor()\n",
    "    cur.execute(u\"\"\"SELECT DISTINCT sujeto, group_concat(IF(predicado = 'https://w3id.org/scholarlydata/ontology/conference-ontology.owl#title',\n",
    "        objeto, '')) titulo, group_concat(IF(predicado = 'https://w3id.org/scholarlydata/ontology/conference-ontology.owl#abstract', objeto, ''))\n",
    "        resumen FROM tripletas WHERE predicado = 'https://w3id.org/scholarlydata/ontology/conference-ontology.owl#abstract' OR  predicado = 'https://w3id.org/scholarlydata/ontology/conference-ontology.owl#title' GROUP BY sujeto;\"\"\")\n",
    "    filas = cur.fetchall()\n",
    "        for fila in filas:\n",
    "            paper = fila[0]\n",
    "            titulo = fila[1].strip(',')\n",
    "            resumen = fila[2].strip(',')\n",
    "            listAbs = resumen.split('.')\n",
    "            print(titulo,\"lista\")\n",
    "            pos = 1\n",
    "            for frase in listAbs:\n",
    "                longitud=len(frase.split(' '))\n",
    "                if longitud > 3:\n",
    "                    entidad= analisis.get_entities(frase)\n",
    "                    relacion=analisis.get_relacion(frase)\n",
    "                    print (40 * \"*\", frase)\n",
    "                    print(\"Sujeto ->\", entidad[0], \"Predicado ->\", relacion, \"Objeto ->\",entidad[1])\n",
    "                    print(longitud,\"longitud\")\n",
    "                    data.guardarTriplesGenerados(paper,frase,pos, entidad[0], relacion, entidad[1], 'resumen', longitud)\n",
    "                    pos = pos + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-desert",
   "metadata": {},
   "source": [
    " # Codigo reutilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string \n",
    "import nltk \n",
    "\n",
    "#from spacy.lang.en import English\n",
    "import spacy\n",
    "#import textacy\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math \n",
    "from tqdm import tqdm \n",
    "\n",
    "from spacy.matcher import Matcher, DependencyMatcher\n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy\n",
    "\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalisisTexto:\n",
    "    global nlp \n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(self,sent):\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "        prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "        prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"  \n",
    "        for tok in nlp(sent): \n",
    "            if tok.dep_ != \"punct\":\n",
    "\n",
    "            if tok.dep_ == \"compound\":\n",
    "                prefix = tok.text\n",
    "\n",
    "            if prv_tok_dep == \"compound\":\n",
    "                prefix = prv_tok_text + \" \"+ tok.text\n",
    "                \n",
    "            if tok.dep_.endswith(\"mod\") == True:\n",
    "                modifier = tok.text\n",
    "\n",
    "            if prv_tok_dep == \"compound\":\n",
    "                modifier = prv_tok_text + \" \"+ tok.text\n",
    "                \n",
    "            if tok.dep_.find(\"subj\") == True:\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "            prefix = \"\"\n",
    "            modifier = \"\"\n",
    "            prv_tok_dep = \"\"\n",
    "            prv_tok_text = \"\" \n",
    "                \n",
    "            if tok.dep_.find(\"obj\") == True:\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text       \n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text  \n",
    "        return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relacion(self,sent):\n",
    "    doc = nlp(sent)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{'DEP':'ROOT'},\n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},\n",
    "                {'POS':'ADJ','OP':\"?\"}]\n",
    "\n",
    "        matcher.add('matching_1',[pattern])\n",
    "\n",
    "        matches = matcher(doc)\n",
    "        if len(matches):\n",
    "            k = len(matches) - 1\n",
    "            span = doc[matches[k][1]:matches[k][2]]\n",
    "            return(span.text)\n",
    "        else:\n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
