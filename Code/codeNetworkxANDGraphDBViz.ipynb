{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "square-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON, RDF\n",
    "from rdflib import Graph \n",
    "from pyvis.network import Network\n",
    "import json\n",
    "\n",
    "import networkx as nx\n",
    "import csv\n",
    "\n",
    "#endPoint = \"http://192.168.1.12:7200/repositories/TFT\"\n",
    "endPoint = \"http://DESKTOP-TTM16LQ:7200/repositories/TFT\"\n",
    "\n",
    "sparql = SPARQLWrapper(endPoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8db127-c350-45ea-9466-e6d33437892c",
   "metadata": {},
   "source": [
    "# USE CASE 1: Comparisson of three index based on predicate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1ba81-ce80-4e2a-bdf4-427cdb76fc9c",
   "metadata": {},
   "source": [
    "##########\n",
    "\n",
    "En las siguintes consultas cambiar el nombre del índice PSIM2 por PredicationIndex\n",
    "\n",
    "##########\n",
    "\n",
    "\n",
    "## 1. Index based on basic metadata\n",
    "## 2. Index based on basic metadata base + dbr + dbc\n",
    "## 3. Index based on basic metadata base + dbr + dbc + spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a4a9c4d-13ff-4040-bc0a-404240fafc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries for indexes:\n",
    "\n",
    "# Query based on basic metadata such as title, abstract, keywords and authors:\n",
    "queryIdx1 = \"\"\"\n",
    "    PREFIX : <http://www.ontotext.com/graphdb/similarity/>\n",
    "    PREFIX inst: <http://www.ontotext.com/graphdb/similarity/instance/>\n",
    "    PREFIX psi: <http://www.ontotext.com/graphdb/similarity/psi/>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX schema: <http://schema.org/>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX dct: <http://purl.org/dc/terms/>\n",
    "    PREFIX skg: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "    PREFIX pred: <http://www.ontotext.com/graphdb/similarity/psi/>\n",
    "    \n",
    "    SELECT distinct ?entity ?id ?title (GROUP_CONCAT(DISTINCT ?keywords; SEPARATOR=\",\") AS ?keys) ?description \n",
    "            (GROUP_CONCAT(DISTINCT ?author; SEPARATOR=\",\") AS ?authors) ?score WHERE {\n",
    "        ?search a inst:PredicationIndex1 ; psi:searchEntity <%s>; \n",
    "        psi:searchPredicate <http://www.ontotext.com/graphdb/similarity/psi/any>;\n",
    "            psi:entityResult ?result .\n",
    "        ?result :value ?entity ; \n",
    "            :score ?score.\n",
    "        ?entity rdf:type schema:Article ; schema:identifier ?id .\n",
    "        OPTIONAL { ?entity skg:title ?title .}\n",
    "        OPTIONAL { ?entity skg:abstract ?description . }\n",
    "        OPTIONAL { ?entity skg:keyword ?keywords . }\n",
    "        OPTIONAL { ?entity dct:creator ?author . }\n",
    "    }group by ?entity ?id ?title ?description ?score\n",
    "    \"\"\"\n",
    "\n",
    "queryIdx2 = \"\"\"\n",
    "    PREFIX : <http://www.ontotext.com/graphdb/similarity/>\n",
    "    PREFIX inst: <http://www.ontotext.com/graphdb/similarity/instance/>\n",
    "    PREFIX psi: <http://www.ontotext.com/graphdb/similarity/psi/>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX schema: <http://schema.org/>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX dct: <http://purl.org/dc/terms/>\n",
    "    PREFIX skg: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "    PREFIX pred: <http://www.ontotext.com/graphdb/similarity/psi/>\n",
    "    \n",
    "    SELECT distinct ?entity ?id ?title ?description \n",
    "    (GROUP_CONCAT(DISTINCT ?dbr1; SEPARATOR=\",\") AS ?dbr) \n",
    "    (GROUP_CONCAT(DISTINCT ?dbc1; SEPARATOR=\",\") AS ?dbc) \n",
    "    (GROUP_CONCAT(DISTINCT ?keywords; SEPARATOR=\",\") AS ?keys) \n",
    "    (GROUP_CONCAT(DISTINCT ?author; SEPARATOR=\",\") AS ?authors) ?score\n",
    "WHERE {\n",
    "        ?search a inst:PredicationIndex2; psi:searchEntity <%s>; psi:searchPredicate <http://www.ontotext.com/graphdb/similarity/psi/any>;\n",
    "            :searchParameters \"\"; psi:entityResult ?result .\n",
    "        ?result :value ?entity ;\n",
    "                :score ?score.\n",
    "        ?entity rdf:type schema:Article ; schema:identifier ?id .\n",
    "        OPTIONAL { ?entity skg:title ?title .}\n",
    "        OPTIONAL { ?entity skg:abstract ?description . }\n",
    "        OPTIONAL { ?entity skg:keyword ?keywords . }\n",
    "        OPTIONAL { ?entity dct:creator ?author . }\n",
    "        OPTIONAL { ?entity schema:mentions/rdfs:label ?dbr1 . }\n",
    "        OPTIONAL { ?entity dct:subject/rdfs:label ?dbc1 . }\n",
    "    }group by ?entity ?id ?title ?description ?score\n",
    "    \"\"\"\n",
    "\n",
    "queryIdx3 = \"\"\"\n",
    "    PREFIX : <http://www.ontotext.com/graphdb/similarity/>\n",
    "    PREFIX inst: <http://www.ontotext.com/graphdb/similarity/instance/>\n",
    "    PREFIX psi: <http://www.ontotext.com/graphdb/similarity/psi/>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX schema: <http://schema.org/>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX dct: <http://purl.org/dc/terms/>\n",
    "    PREFIX skg: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "    PREFIX pred: <http://www.ontotext.com/graphdb/similarity/psi/>\n",
    "    \n",
    "    SELECT distinct ?id ?entity ?score ?title ?description \n",
    "(GROUP_CONCAT(DISTINCT ?spacySu; SEPARATOR=\",\") AS ?spacySujeto)\n",
    "(GROUP_CONCAT(DISTINCT ?spacyPre; SEPARATOR=\",\") AS ?spacyPredicado)\n",
    "(GROUP_CONCAT(DISTINCT ?spacyOb; SEPARATOR=\",\") AS ?spacyObjeto)\n",
    "(GROUP_CONCAT(DISTINCT ?dbr1; SEPARATOR=\",\") AS ?dbr) \n",
    "(GROUP_CONCAT(DISTINCT ?dbc1; SEPARATOR=\",\") AS ?dbc) \n",
    "(GROUP_CONCAT(DISTINCT ?keywords; SEPARATOR=\",\") AS ?keys) \n",
    "(GROUP_CONCAT(DISTINCT ?author; SEPARATOR=\",\") AS ?authors) \n",
    "WHERE {\n",
    "        ?search a inst:PredicationIndex3; psi:searchEntity <%s>; psi:searchPredicate <http://www.ontotext.com/graphdb/similarity/psi/any>;\n",
    "            :searchParameters \"\"; psi:entityResult ?result .\n",
    "        ?result :value ?entity ; :score ?score . \n",
    "        ?entity rdf:type schema:Article ; schema:identifier ?id .\n",
    "        OPTIONAL { ?entity skg:title ?title .}\n",
    "        OPTIONAL { ?entity skg:abstract ?description . }\n",
    "        OPTIONAL { ?entity skg:keyword ?keywords . }\n",
    "        OPTIONAL { ?entity dct:creator ?author . }\n",
    "        OPTIONAL { ?entity schema:mentions/rdfs:label ?dbr1 . }\n",
    "        OPTIONAL { ?entity dct:subject/rdfs:label ?dbc1 . }\n",
    "        OPTIONAL {?entity schema:hasPart/schema:about/schema:subjectOf/schema:name ?spacyP .}\n",
    "        OPTIONAL {?entity schema:hasPart/schema:about/schema:name ?spacyS .}\n",
    "    }group by ?id ?entity ?title ?description ?score\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "outstanding-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To query:\n",
    "def getData(idPaper, Q):\n",
    "    resultList = []\n",
    "    sparql.setQuery(Q%(idPaper))\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    description = ''\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        if 'description' in result:\n",
    "            description = result['description']['value']\n",
    "        resultList.append([result['entity']['value'], result['id']['value'], result['title']['value'], description, result['authors']['value'], result['keys']['value'], result['score']['value'],])\n",
    "        description = ' '\n",
    "        #no permite sacar la variable \"description\"\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098c31a-0223-45bc-81b8-1e1c6729e700",
   "metadata": {},
   "source": [
    "## To run 3 index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b82e6c-bd97-47e1-ab29-9b29d0cad554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paperList = [\"https://w3id.org/scholarlydata/inproceedings/www2011/demo/yago2-exploring-and-querying-world-knowledge-in-ti\",\n",
    "             #\"https://w3id.org/scholarlydata/inproceedings/eswc2012/paper/research/73\"]\n",
    "paperList = [\"https://w3id.org/scholarlydata/inproceedings/www2011/demo/yago2-exploring-and-querying-world-knowledge-in-ti\"]\n",
    "\n",
    "for uriPaper in paperList:\n",
    "\n",
    "    print('\\n\\n ', 60*'-', 'Índex 1')\n",
    "    resultsIdx1 = getData(uriPaper, queryIdx1)\n",
    "    print(resultsIdx1[1:4])\n",
    "\n",
    "    print('\\n\\n ', 60*'-', 'Índex 2')\n",
    "    resultsIdx2 = getData(uriPaper, queryIdx2)\n",
    "    print(resultsIdx2[1:4])\n",
    "\n",
    "    print('\\n\\n ', 60*'-', 'Índex 3')\n",
    "    resultsIdx3 = getData(uriPaper, queryIdx3)\n",
    "    print(resultsIdx3[1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8e09c-a867-47ec-b1bc-1c944a4e2b8d",
   "metadata": {},
   "source": [
    "# Index based on text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641fec19-ae6c-45b6-9fa1-714fa5fa1cb5",
   "metadata": {},
   "source": [
    "# USE CASE 2: Search based on text index\n",
    "\n",
    "## Query the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd61da-641e-4c2e-8702-a25b96a0eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResults(keyword):\n",
    "    SimilarityKeywords = \"\"\"\n",
    "    PREFIX :<http://www.ontotext.com/graphdb/similarity/>\n",
    "    PREFIX inst:<http://www.ontotext.com/graphdb/similarity/instance/>\n",
    "    PREFIX pubo: <http://ontology.ontotext.com/publishing#>\n",
    "    PREFIX schema: <http://schema.org/>\n",
    "    PREFIX scholarly: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "\n",
    "    SELECT ?documentID ?documentSec ?title ?abstract (GROUP_CONCAT(?keyword; SEPARATOR='; ') AS ?keywords) ?score {\n",
    "    ?search a inst:SSTAK ;\n",
    "        :searchTerm \"%s\";\n",
    "        :searchParameters \"\";\n",
    "        :documentResult ?result .\n",
    "    ?result :value ?documentID ;\n",
    "        :score ?score.\n",
    "    ?documentID schema:identifier ?documentSec \n",
    "    OPTIONAL {\n",
    "            ?documentID scholarly:title ?title .\n",
    "    }\n",
    "    OPTIONAL {\n",
    "            ?documentID scholarly:title ?title .\n",
    "    }\n",
    "    OPTIONAL {\n",
    "            ?documentID scholarly:abstract ?abstract .\n",
    "    }\n",
    "    OPTIONAL {\n",
    "            ?documentID scholarly:keyword ?keyword .\n",
    "    }\n",
    "    } GROUP BY ?documentID ?documentSec ?title ?abstract  ?score ORDER BY DESC(?score)\n",
    "    \"\"\"\n",
    "    resultList = []\n",
    "    sparql.setQuery(SimilarityKeywords%(keyword))\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        resultList.append([result['documentID']['value'], result['documentSec']['value'], result['title']['value'],\n",
    "                           result['abstract']['value'], result['keywords']['value'], result['score']['value']])\n",
    "    return resultList\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99ced9-3323-4047-9996-7ee108332abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resultados despues de ejecutar la funcion\n",
    "results = getResults(\"Knowledge Representation\")\n",
    "results[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5657ffb-ec09-4491-b24a-552db987b0b9",
   "metadata": {},
   "source": [
    "## Computing simmilarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c78304-651c-4328-840a-8b9bc57242ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tsvfile = pd.read_csv(open(\"papersT.tsv\", \"r\"), delimiter=\"\\t\", header=None)\n",
    "tsvfile = pd.read_csv(open(\"papersD.tsv\", \"r\"), delimiter=\"\\t\", header=None) # cambiar\n",
    "\n",
    "tsvfile = tsvfile.values.tolist()\n",
    "tsvfile[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f65d14e-129c-4350-bc22-5028973a6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(e):\n",
    "    # To select triplets where ids are retrieved from GraphDB\n",
    "    if e in ids:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def createSubGraph():\n",
    "    # To create the subgraph based on ids of papers:\n",
    "    subset = [[line[0], line[2]] for line in tsvfile if verify(int(line[0])) or verify(int(line[2]))]; print(subset[0:20])\n",
    "    for line in subset:\n",
    "        x, y = line\n",
    "        g.add_edge(x, y)\n",
    "    #nx.draw_circular(g, with_labels = True)\n",
    "    #plt.savefig(\"filename1.png\")\n",
    "    #plt.clf()\n",
    "\n",
    "def calculateSimm(min=0.2):\n",
    "    # To compute the simmilarity matrix between selected entities.\n",
    "    simMatrix = nx.simrank_similarity(g)\n",
    "    simList = []\n",
    "    for d in simMatrix:\n",
    "        for dd in simMatrix[d]:\n",
    "            if simMatrix[d][dd] >= min and simMatrix[d][dd] < 1:\n",
    "                simList.append([d, dd, simMatrix[d][dd]])\n",
    "    return simList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda503aa-65e0-46c7-9f8f-616c48910264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run:\n",
    "results = getResults(\"Semantic Web\")\n",
    "\n",
    "ids = [int(i[1]) for i in results]\n",
    "print(ids)\n",
    "g = nx.Graph()\n",
    "createSubGraph()\n",
    "print(g)\n",
    "sim = calculateSimm()\n",
    "print(sim[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa934e7e-e010-46e0-ae39-dc97dca6e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtra solo papers\n",
    "\n",
    "print(len(sim)) # 8622\n",
    "\n",
    "simSel = [[line[0], line[1], line[2]] for line in sim if verify(line[0]) or verify(line[1])]; len(simSel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ec2c2-50a7-4b4f-9d0b-407376976752",
   "metadata": {},
   "source": [
    "## Getting metadata from KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938575d-88a4-4d88-9d47-f9191d203494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetadata(idSec):\n",
    "    searchMetada = \"\"\"\n",
    "    PREFIX schema: <http://schema.org/>\n",
    "    PREFIX skg: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "    select distinct ?entity ?title ?abstract \n",
    "    (GROUP_CONCAT(DISTINCT ?keywords; SEPARATOR=\",\") AS ?keys)\n",
    "    where{  \n",
    "        ?entity <http://schema.org/identifier> \"%s\".\n",
    "        ?entity skg:title ?title.\n",
    "        ?entity skg:abstract ?abstract .\n",
    "        ?entity skg:keyword ?keywords .   \n",
    "    }GROUP BY ?entity ?title ?abstract\n",
    "    \"\"\"\n",
    "    resultList = []\n",
    "    sparql.setQuery(searchMetada%(idSec))\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        resultList.append([idSec, result['entity']['value'], result['title']['value'], result['abstract']['value'], result['keys']['value']])\n",
    "    return resultList[0]\n",
    "\n",
    "results = getMetadata(328)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f635d93-6672-45a5-9569-fef9fb44f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los ids de los papers que se encuentran en los nodos de la matriz de similitud (obtener valores únicos)\n",
    "simSelDF = pd.DataFrame(simSel, columns=['id1', 'id2', 'score'])\n",
    "idsSim = list(set(simSelDF['id1'].tolist() + simSelDF['id2'].tolist())); idsSim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-crest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#obtener los metadatos de todos los papers que salieron en la matriz de similitud:\n",
    "metadataSim = pd.DataFrame()\n",
    "for i in idsSim:\n",
    "    m = getMetadata(i)\n",
    "    metadataSim = metadataSim.append({'id':m[0], 'uri':m[1], 'title':m[2], 'abstract':m[3]}, ignore_index=True)\n",
    "        \n",
    "metadataSim.head() # Metadata de los papers de la matriz de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f523da4-e80e-48d7-9f28-46a420593188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add metadata a la matriz de similitud y convertir a lista:\n",
    "simSelMeta = pd.merge(pd.DataFrame(simSel, columns=['id1', 'id2', 'score']), metadataSim, left_on=\"id1\", right_on=\"id\")\n",
    "print(simSelMeta,\"1\")\n",
    "simSelMeta.head()\n",
    "simSelMeta = pd.merge(simSelMeta, metadataSim, left_on=\"id2\", right_on=\"id\")\n",
    "print(simSelMeta,\"2\")\n",
    "simSelMeta.head()\n",
    "#simSelMeta = pd.merge(pd.DataFrame(simSel, columns=['id', 'id2', 'score']), metadataSim)\n",
    "print(simSelMeta.head())\n",
    "simSelMeta = simSelMeta.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ace3ef-1822-488e-bbd4-6b2d5533820c",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Source:\n",
    "\n",
    "https://towardsdatascience.com/python-interactive-network-visualization-using-networkx-plotly-and-dash-e44749161ed7\n",
    "\n",
    "https://pyvis.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73591c-5ee0-472f-a71e-c1244ccbf609",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network('1000px','1500px')\n",
    "listUri = []\n",
    "unoSolo = set()\n",
    "for lines in simSelMeta:\n",
    "    x = lines[6].replace('https://w3id.org/scholarlydata/', '')\n",
    "    y = lines[10].replace('https://w3id.org/scholarlydata/', '')\n",
    "    score = lines[2]\n",
    "    # etiquetas:\n",
    "    titlex = str(lines[5])\n",
    "    print(titlex)\n",
    "    titley = str(lines[9])\n",
    "\n",
    "    title_x = '<a href='+ str(lines[4]) + ' target=\"_blank\" >' + 'Paper Link' + '</a> <br> <b> Title: </b>'+ titlex\n",
    "    title_y = '<a href='+ str(lines[8]) + ' target=\"_blank\" >' + 'Paper Link' + '</a> <br> <b> Title: </b>'+ titley\n",
    "    net.add_node(x, title=title_x, label=x)\n",
    "    net.add_node(y, title=title_y, label=y) \n",
    "    net.add_edge(x, y, width=score*5, label = round(score, 2))\n",
    "net.show_buttons(filter_=['physics'])\n",
    "\n",
    "net.show(\"dot.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da604a9-f0c9-4480-8c33-0c62d333a69a",
   "metadata": {},
   "source": [
    "## To run a complete case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "038763fe-3b25-407f-9ae5-0a2d636fc803",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getResults' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2a079469d190>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Search:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetResults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Machine Learning\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Create graph:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'getResults' is not defined"
     ]
    }
   ],
   "source": [
    "# Search:\n",
    "results = getResults(\"Machine Learning\")\n",
    "\n",
    "#Create graph:\n",
    "ids = [int(i[1]) for i in results]\n",
    "g = nx.Graph()\n",
    "createSubGraph()\n",
    "sim = calculateSimm()\n",
    "\n",
    "# filtra solo papers\n",
    "simSel = [[line[0], line[1], line[2]] for line in sim if verify(line[0]) or verify(line[1])]; len(simSel)\n",
    "\n",
    "# Obtener los ids de los papers que se encuentran en los nodos de la matriz de similitud (obtener valores únicos)\n",
    "simSelDF = pd.DataFrame(simSel, columns=['id1', 'id2', 'score'])\n",
    "idsSim = list(set(simSelDF['id1'].tolist() + simSelDF['id2'].tolist())); idsSim\n",
    "\n",
    "#obtener los metadatos de todos los papers que salieron en la matriz de similitud:\n",
    "metadataSim = pd.DataFrame()\n",
    "for i in idsSim:\n",
    "    m = getMetadata(i)\n",
    "    metadataSim = metadataSim.append({'id':m[0], 'uri':m[1], 'title':m[2], 'abstract':m[3]}, ignore_index=True)\n",
    "        \n",
    "metadataSim.head() # Metadata de los papers de la matriz de similitud\n",
    "\n",
    "\n",
    "#add metadata a la matriz de similitud y convertir a lista:\n",
    "simSelMeta = pd.merge(pd.DataFrame(simSel, columns=['id1', 'id2', 'score']), metadataSim, left_on=\"id1\", right_on=\"id\")\n",
    "\n",
    "simSelMeta = pd.merge(simSelMeta, metadataSim, left_on=\"id2\", right_on=\"id\")\n",
    "\n",
    "#simSelMeta = pd.merge(pd.DataFrame(simSel, columns=['id', 'id2', 'score']), metadataSim)\n",
    "simSelMeta = simSelMeta.values.tolist()\n",
    "\n",
    "net = Network('1000px','1500px')\n",
    "for lines in simSelMeta:\n",
    "    x = lines[6].replace('https://w3id.org/scholarlydata/', '')\n",
    "    y = lines[10].replace('https://w3id.org/scholarlydata/', '')\n",
    "    score = lines[2]\n",
    "    # etiquetas:\n",
    "    title_x = '<a href='+ str(lines[4]) + ' target=\"_blank\" >' + 'Paper Link' + '</a> <br> <b> Title: </b>'+str(lines[5])\n",
    "    title_y = '<a href='+ str(lines[8]) + ' target=\"_blank\" >' + 'Paper Link' + '</a> <br> <b> Title: </b>'+str(lines[9])\n",
    "    net.add_node(x, title=title_x, label=x)\n",
    "    net.add_node(y, title=title_y, label=y) \n",
    "    net.add_edge(x, y, width=score*5, label = round(score, 2))\n",
    "net.show_buttons(filter_=['physics'])\n",
    "\n",
    "net.show(\"case2_1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-nicholas",
   "metadata": {},
   "source": [
    "# Index based on predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-earthquake",
   "metadata": {},
   "source": [
    "## USE CASE 1: Similarity on predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-static",
   "metadata": {},
   "source": [
    "# Validacion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-sauce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "first-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://goodboychan.github.io/python/datacamp/natural_language_processing/2020/07/17/04-TF-IDF-and-similarity-scores.html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#def valoracionManual(resumen1, resumen2, titulo1, titulo2, palabrasClave1, palabraClave2):\n",
    "def scoreSPACY(resumen1, resumen2):\n",
    "    #nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    print(resumen1,\"1111111\")\n",
    "    print(resumen2,\"2222222\")\n",
    "    tokens1 = nlp(resumen1)\n",
    "    tokens2 = nlp(resumen2)\n",
    "    oraciones1 = []\n",
    "    oraciones2 = []\n",
    "    for sent in tokens1.sents:\n",
    "        ora = sent.text.strip()\n",
    "        oraciones1.append(ora)\n",
    "\n",
    "    for sent in tokens2.sents:\n",
    "        ora = sent.text.strip()\n",
    "        oraciones2.append(ora)\n",
    "    if(resumen2==' '):\n",
    "        valorSim = 0\n",
    "    else:\n",
    "        docus = (oraciones1, oraciones2)\n",
    "        # Initialize an instance of tf-idf Vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        # Generate the tf-idf vectors for the corpus\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(docus[0], docus[1])\n",
    "        # compute and print the cosine similarity matrix\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "        valorSim = cosine_sim[0][0]    \n",
    "    #print(\"similaridad de coseno -->\", cosine_sim)\n",
    "    return valorSim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "820494c9-9a13-478a-ad51-4c916d8e51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exPapers():\n",
    "    extractPapers = \"\"\"\n",
    "    prefix scholarly: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "    select distinct ?s { \n",
    "        ?s a <http://schema.org/Article> ;\n",
    "        scholarly:abstract [];\n",
    "        scholarly:title [];\n",
    "    }\n",
    "    \"\"\"\n",
    "    resultList = []\n",
    "    sparql.setQuery(extractPapers)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        resultList.append(result['s']['value'])\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-attention",
   "metadata": {},
   "source": [
    "## Seleccion aleatoria de papers y guardado en archivo Excel \"VALIDACION MANUAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eea5c9a4-3949-463d-bdec-ae28d71cfcb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://w3id.org/scholarlydata/inproceedings/iswc2013/poster-demo-proceedings/paper-08'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/322'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/313'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/iswc2012/proceedings-2/paper-26'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/851'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/eswc2008/paper/90'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/www2011/phd/sentence-level-contextual-opinion-retrieval'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/eswc2016/paper/research/219'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/www2011/poster/leveraging-auxiliary-text-terms-for-automatic-imag'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/648'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/324'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/786'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/www2012/paper/505'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/515'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/eswc2010/paper/phd_symposium/5'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/www2007/paper/main/395'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/www2012/industry/2'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/iswc-2019-workshop-7'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/iswc2007+aswc2007/tracks/posters-demos/papers/363'\n",
      " 'https://w3id.org/scholarlydata/inproceedings/eswc2010/paper/onto/43'] papers Aleatorios\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2016/paper/research/research-127', '5353', 'Optimizing Aggregate SPARQL Queries using Materialized RDF Views', ' ', '', '', '0.46156279410210954'], ['https://w3id.org/scholarlydata/inproceedings/iswc2002/poster-proceedings/paper-45', '1210', 'MyCampus: A Semantic Web Environment for Context-Aware Mobile Services', ' ', '', '', '0.42292251136985276'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2018-research-157', '5542', \"Revisiting SPARQL's Semantics for Blank Nodes\", ' ', '', '', '0.41238987314939']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/149', '2732', 'On the Role of the NIMITEK Corpus in Developing an Emotion Adaptive Spoken Dialogue System', 'This paper reports on the creation of the multimodal NIMITEK corpus of affected behavior in human-machine interaction and its role in the development of the NIMITEK prototype system. The NIMITEK prototype system is a spoken dialogue system for supporting users while they solve problems in a graphics system. The central feature of the system is adaptive dialogue management. The system dynamically defines a dialogue strategy according to the current state of the interaction (including also the emotional state of the user). Particular emphasis is devoted to the level of naturalness of interaction. We discuss that a higher level of naturalness can be achieved by combining a habitable natural language interface and an appropriate dialogue strategy. The role of the NIMITEK multimodal corpus in achieving these requirements is twofold: (1) in developing the model of attentional state on the level of user s commands that facilitates processing of flexibly formulated commands, and (2) in defining the dialogue strategy that takes the emotional state of the user into account. Finally, we sketch the implemented prototype system and describe the incorporated dialogue management module. Whereas the prototype system itself is task-specific, the described underlying concepts are intended to be task-independent.', 'https://w3id.org/scholarlydata/person/milan-gnjatovic,https://w3id.org/scholarlydata/person/dietmar-roesner', 'Corpus (creation, annotation, etc.),Dialogue & Natural Interactivity,Emotions', '0.29393980353931126']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc-2018-poster_and_demo-388', '5508', 'Machine Translation vs. Multilingual Entity Linking', ' ', '', '', '0.3278992769081458'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2017-iswc-2017-journal-667', '4659', 'Language and Domain Aware Lightweight Ontology Matching', 'Concepts and relations in ontologies and in other knowledge organisation systems are usually annotated with natural language labels. Most ontology matchers rely on such labels in element-level matching techniques. State-of-the-art approaches, however, tend to make implicit assumptions about the language used in labels (usually English) and are either domain-agnostic or are built for a specific domain. When faced with labels in different languages, most approaches resort to general-purpose machine translation services to reduce the problem to monolingual English-only matching. We investigate a thoroughly different and highly extensible solution based on semantic matching where labels are parsed by multilingual natural language processing and then matched using language-independent and domain aware background knowledge acting as an interlingua. The method is implemented in NuSM, the language and domain aware evolution of the SMATCH semantic matcher, and is evaluated against a translation-based approach. We also design and evaluate a fusion matcher that combines the outputs of the two techniques in order to boost precision or recall beyond the results produced by either technique alone.', 'https://w3id.org/scholarlydata/person/fausto-giunchiglia,https://w3id.org/scholarlydata/person/gabor-bella,https://w3id.org/scholarlydata/person/fiona-mcneill', 'Cross-lingual matching,Multilingual matching,Domains,Ontology matching,Semantic matching, Machine translation', '0.28165020760943354'], ['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/399', '2921', 'Odds of Successful Transfer of Low-Level Concepts: a Key Metric for Bidirectional Speech-to-Speech Machine Translation in DARPA s TRANSTAC Program', 'The Spoken Language Communication and Translation System for Tactical Use (TRANSTAC) program is a Defense Advanced Research Agency (DARPA) program to create bidirectional speech-to-speech machine translation (MT) that will allow U.S. Soldiers and Marines, speaking only English, to communicate, in tactical situations, with civilian populations who speak only other languages (for example, Iraqi Arabic). A key metric for the program is the odds of successfully transferring low-level concepts, defined as the source-language content words. The National Institute of Standards and Technology (NIST) has now carried out two large-scale evaluations of TRANSTAC systems, using that metric. In this paper we discuss the merits of that metric. It has proven to be quite informative. We describe exactly how we defined this metric and how we obtained values for it from panels of bilingual judges allowing others to do what we have done. We compare results on this metric to results on Likert-type judgments of semantic adequacy, from the same panels of bilingual judges, as well as to a suite of typical automated MT metrics (BLEU, TER, METEOR).', 'https://w3id.org/scholarlydata/person/gregory-sanders,https://w3id.org/scholarlydata/person/sebastien-bronsart,https://w3id.org/scholarlydata/person/sherri-condon,https://w3id.org/scholarlydata/person/craig-schlenoff', 'Multilinguality,Evaluation methodologies,Machine Translation, SpeechToSpeech Translation', '0.2799995636753051']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2013/doctoral-consortium-proceedings/paper-02', '2535', 'Semantic Interpretation of Mobile Phone Records Exploiting Background Knowledge', ' ', '', '', '0.4066552718535421'], ['https://w3id.org/scholarlydata/inproceedings/iswc2005/poster-demo-proceedings/paper-70', '1593', \"Toward Task Ontology-based Modeling for Mobile Phone Users' Activity\", ' ', '', '', '0.3523370821214993'], ['https://w3id.org/scholarlydata/inproceedings/iswc2013/data.semanticweb.org/workshop/wop/2013/proceedings/paper-06', '2528', 'Terminology-Based Patterns for Natural Language Definitions in Ontologies', ' ', '', '', '0.3171311558622146']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/dh2010/abstracts/poster/ab-797', '134', 'Generation of Emotional Dance Motion for Virtual Dance Collaboration System', ' ', 'https://w3id.org/scholarlydata/person/choi-woong,https://w3id.org/scholarlydata/person/hachimura-kozaburo,https://w3id.org/scholarlydata/person/tsuruta-seiya', '', '0.2779412268172163'], ['https://w3id.org/scholarlydata/inproceedings/iswc2010/paper/414', '2191', 'Mapping Master: a Flexible Approach for Mapping Spreadsheets to OWL', 'We describe a mapping language for converting data contained in spreadsheets into the Web Ontology Language (OWL). The developed language, called M2, overcomes shortcomings with existing mapping techniques, including their restriction to well-formed spreadsheets reminiscent of a single relational database table and verbose syntax for expressing mapping rules when transforming spreadsheet contents into OWL. The M2 language provides expressive, yet concise mechanisms to create both individual and class axioms when generating OWL ontologies. We additionally present an implementation of the mapping approach, Mapping Master, which is available as a plug-in for the Protege ontology editor.', 'https://w3id.org/scholarlydata/person/mark-a-musen,https://w3id.org/scholarlydata/person/christian-halaschek-wiener,https://w3id.org/scholarlydata/person/martin-j-oconnor', 'semantic web', '0.2724500195499906']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/i-semantics2009/paper/main/23', '1053', 'Integrated Ontologies for the Semantic Web: Experiences from Modeling a Research Context Ontology', ' ', 'https://w3id.org/scholarlydata/person/carola-carstens', '', '0.41964319334816713'], ['https://w3id.org/scholarlydata/inproceedings/iswc2006/paper-26', '1699', 'Using Ontologies for Extracting Product Features from Web Pages', 'In this paper, we will show how we use ontologies to bootstrap a knowledge acquisition process that extracts product knowledge from tabular data on Web pages. Furthermore, we use logical rules to reason about product specific properties and to derive higher-order knowledge about product features. We will also explain the knowledge acquisition process, covering both ontological and procedural aspects. Finally, we will give an qualitative and quantitative evaluation of our results.', '', '', '0.3956896548977234'], ['https://w3id.org/scholarlydata/inproceedings/iswc2016/paper/posterdemo/poster-57', '5336', 'Ontologies Guidelines for Best Practice and a Process to Evaluate Existing Ontologies Mapping Tools and Algorithms', ' ', '', '', '0.36935683446645096']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/xmllondon2014/paper/bina', '4494', 'XML Authoring On Mobile Devices', ' ', 'https://w3id.org/scholarlydata/person/george-bina', '', '0.2797585653556175'], ['https://w3id.org/scholarlydata/inproceedings/www2008/paper/592', '3781', 'Unsupervised Query Segmentation using Generative Language Models and Wikipedia', 'In this paper, we propose a novel unsupervised approach to query segmentation, which is an important task in Web search. We use a generative query model to recover a query’s underlying concepts that compose its original segmented form. The model’s parameters are estimated using an expectation-maximization (EM) algorithm, optimizing the minimum description length objective function on a partial corpus that is specific to the query. To augment this unsupervised learning, we incorporate evidence from Wikipedia. Experiments show that our approach dramatically improves performance over the traditional approach based on mutual information, and produces comparable results with a supervised method. In particular, the basic generative language model contributes a 7.4% improvement over the mutual information based method (measured by segment F1 on the Intersection test set). EM optimization further improves the performance by 14.3%. Additional knowledge from Wikipedia provides another improvement of 24.3%, adding up to a total of 46% improvement (from 0.530 to 0.774).', 'https://w3id.org/scholarlydata/person/bin-tan,https://w3id.org/scholarlydata/person/fuchun-peng', 'content discovery,query segmentation', '0.27170515140638074'], ['https://w3id.org/scholarlydata/inproceedings/www2009/paper/49', '3942', \"Discovering Users' Specific Geo Intention in Web Search\", \"Discovering users' specific and implicit geographic intention in web search can greatly help satisfy users' information needs. We build a geo intent analysis system that uses minimal supervision to learn a model from large amounts of web-search logs for this discovery. We build a city language model, which is a probabilistic representation of the language surrounding the mention of a city in web queries. We use several features derived from these language models to: (1) identify users' implicit geo intent and pinpoint the city corresponding to this intent, (2) determine whether the geo-intent is localized around the users' current geographic location, (3) predict cities for queries that have a mention of an entity that is located in a specific place. Experimental results demonstrate the effectiveness of using features derived from the city language model. We find that (1) the system has over 90% precision and more than 74% accuracy for the task of detecting users' implicit city level geo intent (2) the system achieves more than 96% accuracy in determining whether implicit geo queries are local geo queries, neighbor region geo queries or none-of these (3) the city language model can effectively retrieve cities in locationspecific queries with high precision (88%) and recall (74%); human evaluation shows that the language model predicts city labels for location-specific queries with high accuracy (84.5%).\", '', 'Search', '0.2700605116318877']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2012/poster-demo-proceedings/paper-29', '2320', 'The Linked Data Visualization Model', 'The potential of the semantic data available in the Web is enormous but in most cases it is very difficult for users to explore and use this data. Applying information visualization techniques to the Semantic Web helps users to easily explore large amounts of data and interact with them. We devise a formal Linked Data Visualization model (LDVM), which allows to dynamically connect data with visualizations.', '', '', '0.3743416077410525'], ['https://w3id.org/scholarlydata/inproceedings/www2012/phd/26', '4415', 'From Linked Data to Linked Entities: A Migration Path', 'Entities have been deserved special attention in the latest years; however their identification is still troublesome. Existing approaches exploit ad hoc services or centralized architectures. In this paper we present a novel distributed approach natively built on top of Linked Data concepts and protocols.', 'https://w3id.org/scholarlydata/person/giovanni-bartolomeo,https://w3id.org/scholarlydata/person/stefano-salsano', 'Linked Data,Entity,Identity', '0.3332768950889104'], ['https://w3id.org/scholarlydata/inproceedings/eswc-2017-poster-pd_39', '5104', 'Dynamic Semantic Music Notation: Using Linked Data to Enhance Music Performance', 'The Music Encoding Initiative (MEI) XML schema expresses musical structure addressing score elements at musically meaningful levels of granularity (e.g., individual systems, measures, or notes). While this provides a comprehensive representation of music content, only concepts and relationships provided by the MEI schema can be encoded. Here, we present our Music Encoding and Linked Data (MELD) framework which applies RDF Web Annotations to targetted portions of the MEI structure. Concepts and relationships from the Semantic Web can be included alongside MEI in an expanded musical knowledge graph. We have implemented a music performance scenario which collects, distributes, and displays semantic annotations, enhancing a digital musical score used by performers in a live music jam session.', 'https://w3id.org/scholarlydata/person/david-m-weigl,https://w3id.org/scholarlydata/person/kevin-r-page', 'Linked Data,Multimedia,Music Encoding Initiative,Web Annotations', '0.33222728049254807']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/www2007/paper/main/759', '3715', 'Compare&Contrast: Using the Web to Discover Comparable Cases for News Stories', 'Comparing and contrasting is an important strategy people adopt to understand new situations and create solutions to for new problems. Similar events can provide hints for problem solving, as well as larger contexts for understanding the specific circumstances of an event. Lessons from past experience can be applied, insights can be gained about the new situation from familiar examples, and trends can be discovered among similar events. As the largest knowledge base for human beings, the Web provides both an opportunity and a challenge to discover comparable cases in order to facilitate situation analysis and problem solving. In this paper, we present Compare&Contrast, a system that uses the Web to discover comparable cases for news stories, documents about similar situations but involving distinct entities. The system analyzes a news story given by the user and builds a model of the situation. With the situation model, the system dynamically discovers entities comparable to the main entity in the original story and uses these comparable entities as seeds to retrieve web pages about comparable cases. The system is domain independent, does not require any knowledge engineering efforts, and deals with the complexity of unstructured text and noise on the web in a robust way. We evaluated the system with both a systematic experiment on a collection of news articles and a user study.', 'https://w3id.org/scholarlydata/person/earl-wagner,https://w3id.org/scholarlydata/person/larry-birnbaum,https://w3id.org/scholarlydata/person/jiahui-liu', '', '0.3279880116131771'], ['https://w3id.org/scholarlydata/inproceedings/iswc2004/proceedings/paper-03', '1462', 'A Method for Converting Thesauri to RDF/OWL', ' ', '', '', '0.26398464626035684'], ['https://w3id.org/scholarlydata/inproceedings/iswc2008/paper/research/342', '2008', 'Improving an RCC-derived Geospatial Approximation by OWL Axioms', 'An approach to improve an RCC-derived geospatial approximation is presented which makes use of OWL class inclusion axioms. The algorithm used to control the approximation combines hypothesis testing with consistency checking provided by a knowledge representation system based on description logics. Propositions about the consistency of the refined ABox w.r.t. the associated TBox when compared to baseline ABox and TBox are made. Formal proves of the divergent consistency results when checking either of both are provided. The application of the approach to a geospatial setting results in a roughly tenfold improved approximation when using the refined ABox and TBox. Ways to further improve the approximation and to automate the detection of falsely calculated relations are discussed.', 'https://w3id.org/scholarlydata/person/thomas-scharrenbach,https://w3id.org/scholarlydata/person/rolf-gruetter,https://w3id.org/scholarlydata/person/bettina-bauer-messmer', 'Web Ontology Language,Consistency Checking,Geospatial Approximation,Hypothesis Testing,Region Connection Calculus', '0.26122428054264635']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2007+aswc2007/tracks/posters-demos/papers/395', '1799', 'Ontology Merging using Answer Set Programming and Linguistic Knowledge', 'We present a novel merging algorithm for light-weight ontologies using answer set programming and linguistic background knowledge. The semi-automatic method provides a number of solutions for the user to choose from, by straightforwardly applying intuitive merging rules in a declarative programming environment.', 'https://w3id.org/scholarlydata/person/raphael-volz,https://w3id.org/scholarlydata/person/rodney-topor,https://w3id.org/scholarlydata/person/juergen-bock', '', '0.3436949210996539'], ['https://w3id.org/scholarlydata/inproceedings/eswc2013/paper/eswc-2013/118', '574', 'Querying RDFS with Attribute Equations via SPARQL Rewriting', 'We introduce a technique to determine implicit information in an RDF graph. In addition to taxonomic knowledge about concepts and properties typically expressible in languages such as RDFS and OWL, we focus on knowledge determined by arithmetic equations. The main use case is exploiting knowledge about functional dependencies among numerical properties expressible by means of such arithmetic equations. While some of this knowledge is expressible for instance in rule extensions to  ontology languages, we provide a more flexible framework that treats property equations as first class citizens in the ontology language. The combination of ontological reasoning and property equations is realized by extending query rewriting techniques already successfully applied for ontology languages such as (the DL-fragment of) RDFS, or also OWL QL, respectively. We deploy this technique for rewriting SPARQL queries and discuss the feasibility of alternative implementations, such as rule-based approaches.', 'https://w3id.org/scholarlydata/person/axel-polleres,https://w3id.org/scholarlydata/person/stefan-bischof', 'SPARQL,query rewriting,equations', '0.30992490575562764'], ['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/516', '3006', 'Exploiting Multiply Annotated Corpora in Biomedical Information Extraction Tasks', 'This paper discusses the problem of utilising multiply annotated data in training biomedical information extraction systems. Two corpora, annotated with entities and relations, and containing a number of multiply annotated documents, are used to train named entity recognition and relation extraction systems. Several methods of automatically combining the multiple annotations to produce a single annotation are compared, but none produces better results than simply picking one of the annotated versions at random. It is also shown that adding extra singly annotated documents produces faster performance gains than adding extra multiply annotated documents.', 'https://w3id.org/scholarlydata/person/barry-haddow,https://w3id.org/scholarlydata/person/beatrice-alex', 'Corpus (creation, annotation, etc.),Information Extraction, Information Retrieval,Named Entity recognition', '0.2651312065509988']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2013/proceedings-2/paper-28', '2694', 'The effects of Licensing on Open Data: Computing a measure of health for our Scholarly Record', 'As data collections become established in key disciplines, some of the longstanding barriers to data sharing become to dissolve; yet others remain. While metadata and ontologies help overcome the problems of finding and interpreting data, the lack of clarity over licensing remains a real impediment to data reuse. Freedom from legal restriction and uncertainty is essential for the effective sharing, combining and deriving of data from these distributed collections. Reuse and recombination of data will be greatly facilitated by expanding the definition of the semantic web to include the semantics of data licensing. We aim to express licensing terms in a computable manner, within the context of research practice, enabling us to infer the resulting state of rights, obligations and conditions that are inherited by derived and recombined datasets, using a mixed bag of licenses. Building off this we aim to simulate the effects of varying licensing practices within communities, proposing a measure of health of our scholarly record based on compatibility and restrictiveness of the licenses contained therein.', '', '', '0.27338062927253437'], ['https://w3id.org/scholarlydata/inproceedings/iswc2012/poster-demo-proceedings/paper-05', '2296', 'Building Large Scale Relation KB from Text', 'Recently more and more structured data in form of RDF triples have been published and integrated into Linked Open Data (LOD). While the current LOD contains hundreds of data sources with billions of triples, it has a small number of distinct relations compared with the large number of entities. On the other hand, Web pages are growing rapidly, which results in much larger number of textual contents to be exploited. With the popularity and wide adoption of open information extraction technology, extracting entities and relations among them from text at the Web scale is possible. In this paper, we present an approach to extract the subject individuals and the object counterparts for the relations from text and determine the most appropriate domain and range as well as the most confident dependency path patterns for the given relation based on the EM algorithm. As a preliminary results, we built a knowledge base for relations extracted from Chinese encyclopedias. The experimental results show the effectiveness of our approach to extract relations with reasonable domain, range and path pattern restrictions as well as high-quality triples.', '', '', '0.2665516026268954'], ['https://w3id.org/scholarlydata/inproceedings/iswc2005/poster-demo-proceedings/paper-44', '1567', 'RDFReactor – From Ontologies to Programmatic Data Access', ' ', '', '', '0.26574288182356576']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/574', '3055', 'F0 of Adolescent Speakers   First Results for the German Ph@ttSessionz Database', 'The first release of the German Ph@ttSessionz speech database contains read and spontaneous speech from 864 adolescent speakers and is the largest database of its kind for German. It was recorded via the WWW in over 40 public schools in all dialect regions of Germany. In this paper, we present a cross-sectional study of f0 measurements on this database. The study documents the profound changes in male voices at the age 13-15. Furthermore, it shows that on a perceptive mel-scale, there is little difference in the relative f0 variability for male and female speakers. A closer analysis reveals that f0 variability is dependent on the speech style and both the length and the type of the utterance. The study provides statistically reliable voice parameters of adolescent speakers for German. The results may contribute to making spoken dialog systems more robust by restricting user input to utterances with low f0 variability.', 'https://w3id.org/scholarlydata/person/florian-schiel,https://w3id.org/scholarlydata/person/tania-ellbogen,https://w3id.org/scholarlydata/person/christoph-draxler', 'Speech recognition and understanding,Speech resource/database,Phonetic Databases, Phonology', '0.3579232128062594'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2019-demo-462', '4845', 'Smart Topics Miner 2: Improving Proceedings Retrievability at Springer Nature', \"Producing a robust and comprehensive representation of the research topics covered by a scientific publication is a crucial task that has a major impact on its retrievability and consequently on the diffusion of the relevant scientific ideas. Springer Nature, the world's largest academic book publisher, has typically entrusted this task to the most expert editors, which had to manually analyse new books and produce a list of the most relevant topics. To support Springer Nature in this task, we developed Smart Topic Miner, an application which assists the editorial team in annotating proceedings books according to a large-scale ontology of research areas. Over the past three years we evolved this application according to the editors feedback and developed a new engine, a new interface, and several other functionalities. In this demo paper, we present Smart Topic Miner 2, the most recent version of the tool, which is being regularly utilized by editors in Germany, China, Brazil, and Japan to annotate all book series covering conference proceedings in Computer Science, for a total of about 800 volumes per year.\", 'https://w3id.org/scholarlydata/person/enrico-motta,https://w3id.org/scholarlydata/person/aliaksandr-birukou,https://w3id.org/scholarlydata/person/francesco-osborne,https://w3id.org/scholarlydata/person/angelo-antonio-salatino', '', '0.35385757329438394'], ['https://w3id.org/scholarlydata/inproceedings/eswc2016/paper/inuse/14', '935', 'Semantically Enhanced Quality Assurance in the JURION Business Use Case', \"The publishing industry is undergoing major changes. ↵These changes are mainly based on technical developments and related habits of information consumption. ↵Wolters Kluwer already engaged in new solutions to meet these challenges and to improve all processes of generating good quality content in the backend on the one hand and to deliver information and software in the frontend that facilitates the customer's life on the other hand.↵JURION is an innovative legal information platform developed by Wolters Kluwer Germany (WKD) that merges and interlinks over one million documents of content and data from diverse sources such as national and European legislation and court judgments, extensive internally authored content and local customer data, as well as social media and web data (e.g. DBpedia). ↵In collecting and managing this data, all stages of the Data Lifecycle are present – extraction, storage, authoring, interlinking, enrichment, quality analysis, repair and publication. ↵Ensuring data quality is a key step in the JURION data lifecycle.↵In this industry paper we present two use cases for verifying quality: ↵1) integrating quality tools in the existing software infrastructure and ↵2) improving the data enrichment step by checking the external sources before importing them in JURION.↵We open-source part of our extensions and provide a screencast with our prototype in action.\", 'https://w3id.org/scholarlydata/person/jens-lehmann,https://w3id.org/scholarlydata/person/sebastian-hellmann,https://w3id.org/scholarlydata/person/christian-mader,https://w3id.org/scholarlydata/person/dimitris-kontokostas,https://w3id.org/scholarlydata/person/christian-dirschl,https://w3id.org/scholarlydata/person/katja-eck,https://w3id.org/scholarlydata/person/michael-leuthold', '', '0.3147590631591008']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2013/poster-demo-proceedings/paper-36', '2585', 'KbQAS: A Knowledge-based QA System', 'In this demo paper, we present the first ontology-based Vietnamese question answeringsystem KbQAS in which a knowledge acquisition approach for question analysis is integrated.', '', '', '0.28627728215183795'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2018-workshop_and_tutorial-305', '5601', '1st International Workshop on Contextualized Knowledge Graphs (CKG 2018)', ' ', '', '', '0.2497264817805189']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2010/paper/455', '2209', 'Visual Reasoning about Ontologies', 'We explore a diagrammatic logic suitable for specifying ontologies using a case study. Diagrammatic reasoning is used to establish consequences of the ontology.', 'https://w3id.org/scholarlydata/person/gem-stapleton,https://w3id.org/scholarlydata/person/ian-oliver,https://w3id.org/scholarlydata/person/john-howse', 'Semantic Web', '0.28437995099020263'], ['https://w3id.org/scholarlydata/inproceedings/ekaw2012/paper/demos/1', '154', 'Key-concept extraction for ontology engineering', 'We describe an on-line environment in which the ontology development process can be performed collaboratively in a Wiki-like fashion. To start the construction (or the extension) of an ontology, the user can exploit a domain corpus, from which the terminological component of the system automatically extracts a set of domain-specific key-concepts. These key-concepts are further disambiguated in order to be linked to existing external resources and obtain additional information such as the concept definition, the synonyms and the hypernyms. Finally, the user can easily select through the interface which concepts should be imported into the ontology. The system support several ontology engineering tasks, including (i) boosting the construction or extension of ontologies, (ii) terminologically evaluating and ranking ontologies, and (iii) ranking the concepts defined in an ontology according to their relevance with respect to the domain described by the corpus.', 'https://w3id.org/scholarlydata/person/emanuele-pianta,https://w3id.org/scholarlydata/person/luciano-serafini,https://w3id.org/scholarlydata/person/marco-rospocher,https://w3id.org/scholarlydata/person/sara-tonelli', '', '0.2726591627990873'], ['https://w3id.org/scholarlydata/inproceedings/eswc2009/paper/34', '356', 'Fuzzy annotation of web data tables using a domain ontology', 'We propose an automatic system for annotating accurately data tables extracted from the web. This system is designed to provide additional data to an existing querying system called MIEL, which relies on a common vocabulary used to query local relational databases. We will use the same vocabulary, translated into an OWL ontology, to annotate the tables. Our annotation system is unsupervised. It uses only the knowledge defined in the ontology to automatically annotate the entire content of tables, using an aggregation approach: first annotate cells, then columns, then relations between those columns. The annotations are fuzzy: instead of linking an element of the table with a precise concept of the ontology, the elements of the table are annotated with several concepts, associated with their relevance degree. Our annotation process has been validated experimentally on scientific domains (microbial risk in food, chemical risk in food) and a technical domain (aeronautics).', 'https://w3id.org/scholarlydata/person/gaelle-hignette,https://w3id.org/scholarlydata/person/juliette-dibie-barthelemy,https://w3id.org/scholarlydata/person/ollivier-haemmerle,https://w3id.org/scholarlydata/person/patrice-buche', 'fuzzy set theory,ontology driven method,semantic relation instanciation,semantic relation recognition', '0.27099062401023666']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2010/paper/529', '2255', 'A Semantic Web Repository for Managing and Querying Aligned Knowledge', 'Ontology alignment is the task of matching concepts and terminology from multiple ontologies. Ontology alignment is especially relevant in the semantic web domain as RDF documents and OWL ontologies are quite heterogeneous, yet often describe related concepts. The end goal for ontology matching is to allow the knowledge sets to interoperate. To achieve this goal, it is necessary for queries to return results that include knowledge, and inferred knowledge, from multiple datasets and terminologies, using the alignment information. Furthermore, ontology alignment is not an exact science, and concept matchings often involve uncertainty. The goal of this paper is to provide a semantic web repository that supports applying alignments to the dataset and reasoning with alignments. Our goal is to provide high performance queries that return results that include inference across alignment matchings, and rank results using certainty information. Our semantic web repository uses distributed inference and probabilistic reasoning to allow datasets to be efficiently updated with ontology alignments. We materialize the inferred, aligned data and make it available in efficient queries.', 'https://w3id.org/scholarlydata/person/latifur-khan,https://w3id.org/scholarlydata/person/james-p-mcglothlin', '', '0.2781612243166766'], ['https://w3id.org/scholarlydata/inproceedings/eswc2010/paper/phd_symposium/19', '396', 'A Contextualized Knowledge Framework for SemanticWeb', 'This thesis focuses on developing an efficient framework for contextualized knowledge representation on Semantic Web. We bring about the drawbacks of existing context formalisms which hinders an efficient implementation and propose a formalism for contexts that enables for the development of framework with desired properties. Some of the future milestones to be achieved for this thesis work are (i) develop a proof theory for the logical framework based on Description Logics (DL) (ii) developing reasoning algorithms (iii) verify and compare the performance of these algorithms to existing distributed reasoning formalisms and (iv) system implementation', 'https://w3id.org/scholarlydata/person/mathew-joseph', 'ontology,reasoning,semantic web,contextualized knowledge representation,contexts', '0.2774241057604504'], ['https://w3id.org/scholarlydata/inproceedings/iswc2007+aswc2007/tracks/research/papers/85', '1864', 'Logical Foundations of (e)RDF(S): Complexity and Reasoning', 'An important open question in the semantic Web is the precise relationship between the RDF(S) semantics and the semantics of standard knowledge representation formalisms such as logic programming and description logics. In this paper we address this issue by considering embeddings of RDF and RDFS in logic. Using these embeddings, combined with existing results about various fragments of logic, we establish several novel complexity results. The embeddings we consider show how techniques from deductive databases and description logics can be used for reasoning with RDF(S). Finally, we consider querying RDF graphs and establish the data complexity of conjunctive querying for the various RDF entailment regimes.', 'https://w3id.org/scholarlydata/person/jos-de-bruijn,https://w3id.org/scholarlydata/person/stijn-heymans', '', '0.27286582308878604']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2016/paper/applications/application-21', '5211', 'Extracting Semantic Information for e-Commerce', ' ', '', '', '0.3111735753096431'], ['https://w3id.org/scholarlydata/inproceedings/iswc2008/paper/poster_demo/54', '1933', 'Creating a Semantic Integration System using Spatial Data', 'Data integration is complex often requiring much technical knowledge and expert understanding of the data’s meaning. In this paper we investigate the use of current semantic tools as an aid to data integration, and identify the need to modify these tools to meet the needs of spatial data. We create a demonstrator based on the real world problem of predicting sources of diffuse pollution, illustrating the benefits of exposing the semantics of integration.', 'https://w3id.org/scholarlydata/person/catherine-dolbear,https://w3id.org/scholarlydata/person/glen-hart,https://w3id.org/scholarlydata/person/john-goodwin,https://w3id.org/scholarlydata/person/jennifer-green,https://w3id.org/scholarlydata/person/paula-engelbrecht', 'RDF,Semantic Web,OWL,Semantic Integration,Geospatial', '0.2738108130988495'], ['https://w3id.org/scholarlydata/inproceedings/eswc2012/paper/demonstation/325', '493', 'Exploring History through Newspaper Archives', 'This demo presents a web application which implements a pipeline for searching and browsing through newspaper archives. It uses a combination of information extraction, enrichment and visualization algorithms to help the user to grasp large amount of articles normally collected in archives. Illustrative results show appropriateness of the proposed pipeline for searching and brows-ing news archives.', 'https://w3id.org/scholarlydata/person/blaz-fortuna,https://w3id.org/scholarlydata/person/marko-grobelnik,https://w3id.org/scholarlydata/person/jasna-skrbec', 'visualization,data mining,newspaper archives', '0.2716958037096562']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/dh2010/abstracts/paper/ab-605', '34', 'Entropy and Divergence in a Modern Fiction Corpus', ' ', 'https://w3id.org/scholarlydata/person/hugh-craig', '', '0.2941097110842597']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2005/poster-demo-proceedings/paper-67', '1590', 'The Protégé Ontoling Plugin: Linguistic Enrichment of Ontologies in the Semantic Web', ' ', '', '', '0.32619351776760364'], ['https://w3id.org/scholarlydata/inproceedings/eswc-2017-poster-pd_1', '5100', 'Ontology-based photogrammetric survey in underwater archaeology', 'This work addresses the problem of underwater archaeological surveys from the point of view of knowledge. We propose an approach based on underwater pho-togrammetry guided by a representation of the used knowledge, structured by on-tologies. Ontologies and photogrammetry are used together from the survey pro-cess to the graphical results. This paper focuses on the use of ontologies during the 3D results exploitation. A JAVA software dedicated to photogrammetry and archaeological survey has been mapped onto an OWL formalism. The use of pro-cedural attachment in a dual representation (JAVA - OWL) of the involved con-cepts allows to access computational facilities directly from OWL. The use of rules as SWRL illustrates very well this double formalism, also the use of com-putational capabilities of rules logical expression. We present an application that is able to read the ontology populated with a photogrammetric survey. Once the ontology is read, it is possible to produce a 3D representation of the individuals and observing graphically the results of logical spatial queries on the ontology. This work is done on a very important archaeological wreck in Malta named Xlendi, probably the most ancient Phoenician wreck of western Mediterranean Sea. ', 'https://w3id.org/scholarlydata/person/jean-christophe-sourisseau,https://w3id.org/scholarlydata/person/odile-papini,https://w3id.org/scholarlydata/person/pierre-drap,https://w3id.org/scholarlydata/person/timmy-gambin', 'Ontology,JAVA,Photogrammetry,Underwater Archaeology', '0.31321052742488514'], ['https://w3id.org/scholarlydata/inproceedings/iswc2012/poster-demo-proceedings/paper-29', '2320', 'The Linked Data Visualization Model', 'The potential of the semantic data available in the Web is enormous but in most cases it is very difficult for users to explore and use this data. Applying information visualization techniques to the Semantic Web helps users to easily explore large amounts of data and interact with them. We devise a formal Linked Data Visualization model (LDVM), which allows to dynamically connect data with visualizations.', '', '', '0.31169516987374357']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2004/poster-proceedings/paper-21', '1432', 'Open Ontologies and Open Knowledge Bases for Modeling the Social Layer of the Semantic Web', ' ', '', '', '0.38795423682367247'], ['https://w3id.org/scholarlydata/inproceedings/iswc2013/poster-demo-proceedings/paper-12', '2561', 'NoHR: Querying EL with Non-monotonic rules', 'We present NoHR, a Protege plug-in that allows the user to take an EL ontology, add a set of non-monotonic (logic programming) rules - suitable e.g. to express defaults and exceptions - and query the combined knowledge base. Provided the given ontology alone is consistent, the system is capable of dealing with potential inconsistencies between the ontology and the rules, and, after an initial brief pre-processing period utilizing OWL 2 EL reasoner ELK, returns answers to queries at an interactive response time by means of XSB Prolog.', '', '', '0.3670510074289632'], ['https://w3id.org/scholarlydata/inproceedings/iswc2016/paper/resource/resource-88', '5408', 'OntoBench: Generating Custom OWL 2 Benchmark Ontologies', ' ', '', '', '0.3627877182384841']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 1\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc-2018-research-1', '5526', 'Towards Empty Answers in SPARQL: Approximating Querying with RDF Embedding', ' ', '', '', '0.36763177988066525'], ['https://w3id.org/scholarlydata/inproceedings/iswc2016/paper/research/research-127', '5353', 'Optimizing Aggregate SPARQL Queries using Materialized RDF Views', ' ', '', '', '0.3608480957160358'], ['https://w3id.org/scholarlydata/inproceedings/iswc2006/paper-32', '1706', 'A Framework for Ontology Evolution in Collaborative Environments', 'With the wider use of ontologies in the Semantic Web and as part of production systems, multiple scenarios for ontology maintenance and evolution are emerging. For example, successive ontology versions can be posted on the (Semantic) Web, with users discovering the new versions serendipitously; ontology-development in a collaborative environment can be synchronous or asynchronous; managers of projects may exercise quality control, examining the changes from previous baseline versions and accepting or rejecting them before a new baseline is published, and so on. In this paper, we present the different scenarios for ontology maintenance and evolution that we have encountered in our own projects and in those of our collaborators. We define several dimensions that categorize different scenarios. For each scenario, we discuss the high-level tasks that an editing environment must support. We then present a unified comprehensive set of tools to support the different scenarios in a single framework, allowing users to switch between different modes easily.', '', '', '0.3576917912273633']] indice 1\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc-2018-research-165', '5544', 'Efficient Handling of SPARQL OPTIONAL for OBDA', ' ', '', '', '0.4117635933372835'], ['https://w3id.org/scholarlydata/inproceedings/eswc2016/paper/research/45', '994', 'VOLT: A Provenance-Producing, Transparent SPARQL Proxy for the On-Demand Computation of Linked Data and its Application to Spatiotemporally Dependent Data', \"Powered by Semantic Web technologies the Linked Data paradigm aims at weaving a globally interconnected graph of raw data that transforms the ways we publish, retrieve, share, reuse, and integrate data from a variety of distributed and heterogeneous sources. In practice, however, this vision faces substantial challenges with respect to data quality, coverage, and longevity,  the amount of background knowledge required to query distant data, the  reproducibility of query results and their derived (scientific) findings, and the lack of computational capabilities required for many tasks.  One key issues underlying these challenges is the trade-off between storing data and computing them. Intuitively, data that is derived from already stored data, changes frequently in space and time,  or is the result of some workflow or procedure, should be compute. However, this functionality is not readily available on the Linked Data cloud and its technology stack. In this work, we introduce a  proxy that can transparently run on top of arbitrary SPARQL endpoints and enables the on-demand computation of Linked Data together with the provenance information required to understand how they were derived. While our work can be generalized to multiple domains, we focus on two geographic use case to showcase the proxy's capabilities.\", 'https://w3id.org/scholarlydata/person/krzysztof-janowicz,https://w3id.org/scholarlydata/person/blake-regalia,https://w3id.org/scholarlydata/person/song-gao', '', '0.38400940320378213'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2018-poster_and_demo-329', '5459', 'Generating SPARQL Query Containment Benchmarks using the SQCFramework', ' ', '', '', '0.38261903774130596']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/366', '2897', 'Recording Speech of Children, Non-Natives and Elderly People for HLT Applications: the JASMIN-CGN Corpus.', 'Within the framework of the Dutch-Flemish programme STEVIN, the JASMIN-CGN (Jongeren, Anderstaligen en Senioren in Mens-machine Interactie  Corpus Gesproken Nederlands) project was carried out, which was aimed at collecting speech of children, non-natives and elderly people. The JASMIN-CGN project is an extension of the Spoken Dutch Corpus (CGN) along three dimensions. First, by collecting a corpus of contemporary Dutch as spoken by children of different age groups, elderly people and non-natives with different mother tongues, an extension along the age and mother tongue dimensions was achieved. In addition, we collected speech material in a communication setting that was not envisaged in the CGN: human-machine interaction. One third of the data was collected in Flanders and two thirds in the Netherlands. In this paper we report on our experiences in collecting this corpus and we describe some of the important decisions that we made in the attempt to combine efficiency and high quality.', 'https://w3id.org/scholarlydata/person/hugo-van-hamme,https://w3id.org/scholarlydata/person/eric-sanders,https://w3id.org/scholarlydata/person/joris-driesen,https://w3id.org/scholarlydata/person/catia-cucchiarini', 'Corpus (creation, annotation, etc.),Speech resource/database,Other', '0.26636265726908737'], ['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/319', '2861', 'Thai Broadcast News Corpus Construction and Evaluation', 'Large speech and text corpora are crucial to the development of a state-of-the-art speech recognition system. This paper reports on the construction and evaluation of the first Thai broadcast news speech and text corpora. Specifications and conventions used in the transcription process are described in the paper. The speech corpus contains about 17 hours of speech data while the text corpus was transcribed from around 35 hours of television broadcast news. The characteristics of the corpus were analyzed and shown in the paper. The speech corpus was split according to the evaluation focus condition used in the DARPA Hub-4 evaluation. An 18K-word Thai speech recognition system was setup to test with this speech corpus as a preliminary experiment. Acoustic model adaptations were performed to improve the system performance. The best system yielded a word error rate of about 20% for clean and planned speech, and below 30% for the overall condition.', 'https://w3id.org/scholarlydata/person/koji-iwano,https://w3id.org/scholarlydata/person/markpong-jongtaveesataporn,https://w3id.org/scholarlydata/person/sadaoki-furui,https://w3id.org/scholarlydata/person/chai-wutiwiwatchai', 'Corpus (creation, annotation, etc.),Speech recognition and understanding,Speech resource/database', '0.2638652660560045']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/swws2001/proceedings/paper-34', '1164', 'UML and the Semantic Web', ' ', '', '', '0.32100605146427114'], ['https://w3id.org/scholarlydata/inproceedings/eswc2014/paper/ws/KnowLOD/6', '776', 'Interlinking English and Chinese RDF Data Sets Using Machine Translation', ' ', 'https://w3id.org/scholarlydata/person/jerome-david,https://w3id.org/scholarlydata/person/jerome-euzenat,https://w3id.org/scholarlydata/person/tatiana-lesnikova', '', '0.3180616712331859'], ['https://w3id.org/scholarlydata/inproceedings/eswc2014/paper/demo/38', '617', 'Aether - Generating and Viewing Extended VoID Statistical Descriptions of RDF Datasets', ' ', 'https://w3id.org/scholarlydata/person/eetu-makela', '', '0.3126412505542184']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2005/proceedings/paper-09', '1610', 'A Semantic Search Engine for the International Relation Sector', ' ', '', '', '0.30610103561425384'], ['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/510', '3001', 'Talking and Looking: the SmartWeb Multimodal Interaction Corpus', 'Nowadays portable devices such as smart phones can be used to capture the face of a user simultaneously with the voice input. Server based or even embedded dialogue system might utilize this additional information to detect whether the speaking user addresses the system or other parties or whether the listening user is focused on the display or not. Depending on these findings the dialogue system might change its strategy to interact with the user improving the overall communication between human and system. To develop and test methods for On/Off-Focus detection a multimodal corpus of user-machine interactions was recorded within the German SmartWeb project. The corpus comprises 99 recording sessions of a triad communication between the user, the system and a human companion. The user can address/watch/listen to the system but also talk to his companion, read from the display or simply talk to herself. Facial video is captured with a standard built-in video camera of a smart phone while voice input in being recorded by a high quality close microphone as well as over a realistic transmission line via Bluetooth and WCDMA. The resulting SmartWeb Video Corpus (SVC) can be obtained from the Bavarian Archive for Speech Signals.', 'https://w3id.org/scholarlydata/person/florian-schiel,https://w3id.org/scholarlydata/person/hannes-moegele', 'Corpus (creation, annotation, etc.),Dialogue & Natural Interactivity,Speech resource/database', '0.2915394676911214'], ['https://w3id.org/scholarlydata/inproceedings/eswc2012/paper/research/218', '555', 'Voting Theory for Concept Detection', 'This paper explores the issue of detecting concepts for ontology learning from text. We investigate various metrics from graph theory and propose various voting schemes based on these metrics. The idea draws its root in social choice theory, and our objective is to mimic consensus in automatic learning methods and increase the confidence in concept extraction through the identification of the best performing metrics, the comparison of these metrics with standard information retrieval metrics (such as TF-IDF) and the evaluation of various voting schemes. Our results show that three graph-based metrics Degree, Reachability and HITS-hub were the most successful in identifying relevant concepts contained in two gold standard ontologies.', 'https://w3id.org/scholarlydata/person/dragan-gasevic,https://w3id.org/scholarlydata/person/marek-hatala,https://w3id.org/scholarlydata/person/amal-zouaq', 'machine learning,ontology,concepts,voting theory', '0.2782529575153049']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/734', '3172', 'A Linguistic Resource for Discovering Event Structures and Resolving Event Coreference', 'In this paper, we present a linguistic resource that annotates event structures in texts. We consider an event structure as a collection of events that interact with each other in a given situation. We interpret the interactions between events as event relations. In this regard, we propose and annotate a set of six relations that best capture the concept of event structure. These relations are: subevent, reason, purpose, enablement, precedence and related. A document from this resource can encode multiple event structures and an event structure can be described across multiple documents. In order to unify event structures, we also annotate inter- and intra-document event coreference. Moreover, we provide methodologies for automatic discovery of event structures from texts. First, we group the events that constitute an event structure into event clusters and then, we use supervised learning frameworks to classify the relations that exist between events from the same cluster', 'https://w3id.org/scholarlydata/person/sanda-harabagiu,https://w3id.org/scholarlydata/person/cosmin-bejan', 'Ontologies,Knowledge representation,Corpus (creation, annotation, etc.)', '0.25038815262458974']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2006/paper-42', '1717', 'Ranking Ontologies with AKTiveRank', 'Ontology search and reuse is becoming increasingly important as the quest for methods to reduce the cost of constructing such knowledge structures continues. To this end, a number of ontology libraries and search engines are coming to existence to facilitate locating and retrieving potentially relevant ontologies. The number of ontologies available for reuse is steadily growing, and so is the need for methods to evaluate and rank existing ontologies in terms of their relevance to the needs of the knowledge engineer. This paper presents AKTiveRank, a prototype system for ranking ontologies based on a number of structural metrics. The paper describes those metrics, a ranking experiment and an evaluation of results.', '', '', '0.29718002432076485'], ['https://w3id.org/scholarlydata/inproceedings/eswc2011/paper/semantic-data-management/32', '469', 'Benchmarking Matching Applications on the Semantic Web', 'The evaluation of matching applications is becoming a major issue in the semantic web and it requires a suitable methodological approach as well as appropriate benchmarks. In particular, in order to evaluate a matching application under different experimental conditions, it is crucial to provide a test dataset characterized by a controlled variety of different heterogeneities among data that rarely occurs in real data repositories. In this paper, we propose SWING (Semantic Web INstance Generation), a disciplined approach to the semi-automatic generation of benchmarks to be used for the evaluation of matching applications. SWING is illustrated in the paper by presenting the specific benchmark we generated for the international instance matching contest at OAEI 2010 (called IIMB 2010) and by discussing the experimental results obtained on it with different matching algorithms.', 'https://w3id.org/scholarlydata/person/heiner-stuckenschmidt,https://w3id.org/scholarlydata/person/alfio-ferrara,https://w3id.org/scholarlydata/person/jan-noessner,https://w3id.org/scholarlydata/person/stefano-montanelli', 'Semantic Web,Evaluation,Semantic Matching', '0.2738924242875355'], ['https://w3id.org/scholarlydata/inproceedings/eswc-2017-research-71', '5144', 'Mapping Natural Language to Description Logic', 'While much work on automated ontology enrichment has focused on mining text for concepts and relations, little attention has been paid to the task of enriching ontologies with complex axioms. In this paper, we focus on a form of text that is frequent in industry, namely system installation design principle (SIDP) and we present a framework which can be used both to map SIDPs to OWL DL axioms and to assess the quality of these automatically derived axioms. We present experimental results on a set of 960 SIDPs provided by Airbus which demonstrate (i) that the approach is robust (98% of the SIDPs can be parsed) and (ii) that DL axioms  assigned to full parses are very likely to be correct in 96% of the cases.', 'https://w3id.org/scholarlydata/person/anastasia-shimorina,https://w3id.org/scholarlydata/person/bikash-gyawali,https://w3id.org/scholarlydata/person/claire-gardent', 'natural language processing,Description logic,ontology enrichment,quality check', '0.268441778962406']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/ekaw-2016-research%2Binuse-155', '4570', 'VoCol: An Integrated Environment to Support Version-Controlled Vocabulary Development', 'Vocabularies are increasingly developed on platforms for hosting version-controlled repositories, such as GitHub. However, these platforms lack important features that have proven useful in vocabulary development. We present VoCol, an integrated environment that supports the Git-based development of vocabularies. VoCol is based on a fundamental model of vocabulary development, consisting of the three core activities modeling, population and testing. We implemented VoCol using a loose coupling of validation, querying, analytics, visualization, and documentation generation components on top of a standard Git repository. All components, including the version-controlled repository, can be configured and replaced with little effort to cater for various use cases. We show the applicability of VoCol with an example and report on a user study that confirms its usability and usefulness.', 'https://w3id.org/scholarlydata/person/christoph-lange,https://w3id.org/scholarlydata/person/steffen-lohmann,https://w3id.org/scholarlydata/person/gokhan-coskun,https://w3id.org/scholarlydata/person/irlan-grangel,https://w3id.org/scholarlydata/person/lavdim-halilaj,https://w3id.org/scholarlydata/person/niklas-petersen,https://w3id.org/scholarlydata/person/soren-auer', 'Git,GitHub,Integrated Development Environment,Version Control Systems,Vocabulary Development', '0.32543638954813975'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2019-resource-389', '5033', 'SemanGit: A Linked Dataset from git', 'The growing interest in free and open-source software which occurred over the last decades has accelerated the usage of versioning systems to help developers collaborating together in the same projects. As a consequence, specific tools such as git and specialized open-source on-line platforms gained importance. In this study, we introduce and share SemanGit which provides a resource at the crossroads of both Semantic Web and git web-based version control systems. SemanGit is actually the first collection of linked data extracted from GitHub based on a git ontology we designed and extended to include specific GitHub features. In this article, we present the dataset, describe the extraction process according to the ontology, show  some promising analyses of the data and outline how SemanGit could be linked with external datasets or enriched with new sources to allow for more complex analyses.', 'https://w3id.org/scholarlydata/person/dennis-oliver-kubitza,https://w3id.org/scholarlydata/person/matthias-bockmann,https://w3id.org/scholarlydata/person/damien-graux', 'RDF dataset,Git protocol,Big Linked Data', '0.28812143571789595'], ['https://w3id.org/scholarlydata/inproceedings/eswc2014/paper/poster/111', '684', 'A companion screen application for TV broadcasts annotated with Linked Open Data', ' ', 'https://w3id.org/scholarlydata/person/lynda-hardman,https://w3id.org/scholarlydata/person/lilia-perez-romero,https://w3id.org/scholarlydata/person/lotte-baltussen,https://w3id.org/scholarlydata/person/lyndon-nixon', '', '0.26314101953599883']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2010/paper/525', '2253', 'Automated Mapping Generation for Converting Databases into Linked Data', \"Most of the data on the Web is stored in relational databases. In order to make the Semantic Web grow we need to provide easy-to-use tools to convert those databases into linked data, so that even people with little knowledge of the semantic web can use them. Some programs able to convert relational databases into RDF files have been developed, but the user still has to link manually the database attribute names to existing ontology properties and this generated 'linked data' is not actually linked with external relevant data. We propose here a method to associate automatically attribute names to existing ontology entities in order to complete the automation of the conversion of databases. We also present a way - rather basic, but with low error rate - to add links automatically to relevant data from other data sets.\", 'https://w3id.org/scholarlydata/person/ryutaro-ichise,https://w3id.org/scholarlydata/person/simeon-polfliet', 'Semantic Web,Linked Data,Semantic Integration,Database', '0.2823905629712053'], ['https://w3id.org/scholarlydata/inproceedings/www2007/paper/main/464', '3663', 'Using Google Distance to weight approximate ontology matches', 'Discovering mappings between concept hierarchies is widely regarded as one of the hardest and most urgent problems facing the Semantic Web. The problem is even harder in domains where concepts are inherently vague and ill-defined, and cannot be given a crisp definition. A notion of approximate concept mapping is required in such domains, but until now, no such notion is available.<br /><br /> The first contribution of this paper is a definition for concepts is decomposed into a number of submappings, and a emph{sloppiness value} determines the fraction of these submappings that can be ignored when establishing the mapping.<br /><br /> A potential problem of such a definition is that with an increasing sloppiness value, it will gradually allow mappings between any two arbitrary concepts. To improve on this trivial behaviour, we need to design a heuristic weighting which minimises the sloppiness required to conclude desirable matches, but at the same time maximises the sloppiness required to conclude undesirable matches. The second contribution of this paper is to show that a emph{Google-based similarity measure} has exactly these desirable properties.<br /><br /> We establish these results by emph{experimental validation in the domain of musical genres}. We show that this domain does suffer from ill-defined concepts. We take two real-life genre hierarchies from the Web, we compute approximate mappings between them at varying levels of sloppiness, and we validate our results against a hand-crafted Gold Standard.<br /><br /> Our method makes use of the huge amount of knowledge that is implicit in the current Web, and exploits this knowledge as a heuristic for establishing approximate mappings between ill-defined concepts.', 'https://w3id.org/scholarlydata/person/frank-van-harmelen,https://w3id.org/scholarlydata/person/risto-risto-gligorov,https://w3id.org/scholarlydata/person/warner-ten-kate,https://w3id.org/scholarlydata/person/zharko-aleksovski', '', '0.27720250458248064'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2018-resources-201', '5568', 'Browsing Linked Data Catalogs with LODAtlas', ' ', '', '', '0.27583614069949897']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/www2011/poster/a-feature-pair-based-associative-classification-ap', '4208', 'A Feature-Pair-Based Associative Classification Approach to Look-Alike Modeling for Conversion-Oriented User-Targeting in Tail Campaigns', 'Online advertising offers significantly finer granularity, which has been leveraged in state-of-the-art targeting methods, like Behavioral Targeting (BT). Such methods have been further complemented by recent work in Look-alike Modeling (LAM) which helps in creating models which are customized according to each advertiser s requirements and each campaign s characteristics, and which show ads to users who are most likely to convert on them, not just click them. In Look-alike Modeling given data about converters and nonconverters, obtained from advertisers, we would like to train models automatically for each ad campaign. Such custom models would help target more users who are similar to the set of converters the advertiser provides. The advertisers get more freedom to define their preferred sets of users which should be used as a basis to build custom targeting models. In behavioral data, the number of conversions (positive class) per campaign is very small (conversions per impression for the advertisers in our data set are much less than 10e 4), giving rise to a highly skewed training dataset, which has most records pertaining to the negative class. Campaigns with very few conversions are called as tail campaigns, and those with many conversions are called head campaigns. Creation of Look-alike Models for tail campaigns is very challenging and tricky using popular classifiers like Linear SVM and GBDT, because of the very few number of positive class examples such campaigns contain. In this paper, we present an Associative Classification (AC) approach to LAM for tail campaigns. Pairs of features are used to derive rules to build a Rule-based Associative Classifier, with the rules being sorted by frequency-weighted log-likelihood ratio (F-LLR). The top k rules, sorted by F-LLR, are then applied to any test record to score it. Individual features can also form rules by themselves, though the number of such rules in the top k rules and the whole rule-set is very small. Our algorithm is based on Hadoop, and is thus very efficient in terms of speed.', 'https://w3id.org/scholarlydata/person/abraham-bagherjeiran,https://w3id.org/scholarlydata/person/andrew-o-hatch,https://w3id.org/scholarlydata/person/ashish-mangalampalli,https://w3id.org/scholarlydata/person/rajesh-parekh,https://w3id.org/scholarlydata/person/vikram-pudi,https://w3id.org/scholarlydata/person/adwait-ratnaparkhi', '', '0.2923363464695192'], ['https://w3id.org/scholarlydata/inproceedings/eswc2014/paper/demo/33', '614', 'Durchblick - A Conference Assistance System for Augmented Reality Devices', ' ', 'https://w3id.org/scholarlydata/person/michael-schmidt,https://w3id.org/scholarlydata/person/peter-haase,https://w3id.org/scholarlydata/person/georg-lausen,https://w3id.org/scholarlydata/person/anna-gossen,https://w3id.org/scholarlydata/person/anas-alzoghbi,https://w3id.org/scholarlydata/person/beibei-hu,https://w3id.org/scholarlydata/person/christoph-pinkel,https://w3id.org/scholarlydata/person/peter-fischer,https://w3id.org/scholarlydata/person/thomas-hornung', '', '0.2784600748745001'], ['https://w3id.org/scholarlydata/inproceedings/iswc2004/poster-proceedings/paper-07', '1418', 'Educational Topic Maps', ' ', '', '', '0.26430201670426606']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/832', '3253', 'Assessing the Costs of Machine-Assisted Corpus Annotation through a User Study', 'Fixed, limited budgets often constrain the amount of expert annotation that can go into the construction of annotated corpora. Estimating the cost of annotation is the first step toward using annotation resources wisely. We present here a study of the cost of annotation. This study includes the participation of annotators at various skill levels and with varying backgrounds. Conducted over the web, the study consists of tests that simulate machine-assisted pre-annotation, requiring correction by the annotator rather than annotation from scratch. The study also includes tests representative of an annotation scenario involving Active Learning as it progresses from a naïve model to a knowledgeable model; in particular, annotators encounter pre-annotation of varying degrees of accuracy. The annotation interface lists tags considered likely by the annotation model in preference to other tags. We present the experimental parameters of the study and report both descriptive and inferential statistics on the results of the study. We conclude with a model for estimating the hourly cost of annotation for annotators of various skill levels. We also present models for two granularities of annotation: sentence at a time and word at a time.', 'https://w3id.org/scholarlydata/person/james-carroll,https://w3id.org/scholarlydata/person/kevin-seppi,https://w3id.org/scholarlydata/person/peter-mcclanahan,https://w3id.org/scholarlydata/person/eric-ringger,https://w3id.org/scholarlydata/person/marc-carmen,https://w3id.org/scholarlydata/person/noel-ellison,https://w3id.org/scholarlydata/person/robbie-haertel,https://w3id.org/scholarlydata/person/deryle-lonsdale', 'Tagging,Corpus (creation, annotation, etc.),Acquisition, Machine Learning', '0.3068300747913127'], ['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/115', '2709', 'Modeling Document Dynamics: an Evolutionary Approach', 'News articles about the same event published over time have properties that challenge NLP and IR applications. A cluster of such texts typically exhibits instances of paraphrase and contradiction, as sources update the facts surrounding the story, often due to an ongoing investigation. The current hypothesis is that the stories  evolve  over time, beginning with the first text published on a given topic. This is tested using a phylogenetic approach as well as one based on language modeling. The fit of the evolutionary models is evaluated with respect to how well they facilitate the recovery of chronological relationships between the documents. Over all data clusters, the language modeling approach consistently outperforms the phylogenetics model. However, on manually collected clusters in which the documents are published within short time spans of one another, both have a similar performance, and produce statistically significant results on the document chronology recovery evaluation.', 'https://w3id.org/scholarlydata/person/jahna-otterbacher,https://w3id.org/scholarlydata/person/dragomir-radev', 'Tools, systems, applications,Document Classification, Text categorisation', '0.2545779048314008']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2007+aswc2007/tracks/posters-demos/papers/377', '1782', 'Semantic Personal Media Management System using Uniform Representation based on Ontology', 'Everyday individual wants to record, annotate, and manipulate digital media in their own ways. Semantic web supports users to represent annotations in forms that bear personal semantic meaning. An essential feature of personal media management systems is to give individual users significant control over representation, annotation and query their media information. In our framework, na�ve users create their personalized ontology for describing their media information and construct semantically rich metadata collections using the personalized media ontology. Our system allows users to provide expressive queries in their own ontology. Queries are represented by OWL classes or instances. The system uses a uniform representation of personalized ontology, metadata, and queries. Such a uniform representation enables the system to exploit description logic reasoner.', 'https://w3id.org/scholarlydata/person/jung-hwa-choi,https://w3id.org/scholarlydata/person/sung-chan-kim,https://w3id.org/scholarlydata/person/young-tack-park', '', '0.34467556768656876'], ['https://w3id.org/scholarlydata/inproceedings/iswc2004/demo-proceedings/paper-10', '1380', 'Bringing Discussion to Documents for the Creation of Richer Metadata', ' ', '', '', '0.33007235340199753'], ['https://w3id.org/scholarlydata/inproceedings/iswc2009/paper/research/391', '2125', 'Investigating the Semantic Gap through Query Log Analysis', 'Significant efforts have focused in the past years on bringing large amounts of metadata online and the success of these efforts can be seen by the impressive number of web sites exposing data in RDFa or RDF/XML. However, little is known about the extent to which this data fits the needs of ordinary web users with everyday information needs. In this paper we study what we perceive as the semantic gap between the supply of data on the Semantic Web and the needs of web users as expressed in the queries submitted to a major Web search engine. We perform our analysis on both the level of instances and ontologies. First, we first look at how much data is actually relevant to Web queries and what kind of data is it. Second, we provide a generic method to extract the attributes that Web users are searching for regarding particular classes of entities. This method allows to contrast class definitions found in Semantic Web vocabularies with the attributes of objects that users are interested in. Our findings are crucial to measuring the potential of semantic search, but also speak to the state of the Semantic Web in general.', 'https://w3id.org/scholarlydata/person/peter-mika,https://w3id.org/scholarlydata/person/edgar-meij,https://w3id.org/scholarlydata/person/hugo-zaragoza', '', '0.32882910640342594']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/www2009/paper/105', '3825', 'A Geographical Analysis of Knowledge Production in Computer Science', 'We analyze knowledge production in Computer Science by means of coauthorship networks. For this, we consider 30 graduate programs of different regions of the world, being 8 programs in Brazil, 16 in North America (3 in Canada and 13 in the United States), and 6 in Europe (2 in France, 1 in Switzerland and 3 in the United Kingdom). We use a dataset that consists of 176,537 authors and 352,766 publication entries distributed among 2,176 publication venues. The results obtained for different metrics of collaboration social networks indicate the process of knowledge production has a ˘changed differently for each region. Research is increasingly done in teams across different fields of Computer Science. The size of the giant component indicates the existence of isolated collaboration groups in the European network, contrasting to the degree of connectivity found in the Brazilian and North-American counterparts. We also analyzed the temporal evolution of the social networks representing the three regions. The number of authors per paper experienced an increase in a time span of 12 years. We observe that the number of collaborations between authors grows faster than the number of authors, benefiting from the existing network structure. The temporal evolution shows differences between well-established fields, such as Databases and Computer Architecture, and emerging fields, like Bioinformatics and Geoinformatics. The patterns of collaboration analyzed in this paper contribute to an overall understanding of Computer Science research in different geographical regions that could not be achieved without the use of complex networks and a large publication database.', '', 'WWW in lbero-America', '0.35985126675644513'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2017-iswc-2017-industry-488', '4647', 'Adopting Semantic Technologies in Public Health Documentation', 'We present a success story on the adoption of semantic technologies for the library of the second biggest university hospital of France. This project was divided into three parts: preprocessing, semantic enrichment and data integration. This abstract introduces the research challenges faced in the project as well as the outcomes obtained so far.', 'https://w3id.org/scholarlydata/person/frederic-riondet,https://w3id.org/scholarlydata/person/joffrey-decourselle', 'semantics,enrichment,health documentation,migration', '0.3577018434983397'], ['https://w3id.org/scholarlydata/inproceedings/iswc2009/paper/poster_demo/139', '2055', 'The alpha Urban LarKC, a Semantic Urban Computing application', 'This paper describes the alpha Urban LarKC, one of the first Urban Computing applications built with Semantic Web technologies. It is based on the LarKC platform and makes use of the publicly available data sources on the Web which refer to interesting information about a urban environment (the city of Milano in Italy).', 'https://w3id.org/scholarlydata/person/emanuele-della-valle,https://w3id.org/scholarlydata/person/irene-celino,https://w3id.org/scholarlydata/person/daniele-dellaglio', '', '0.3505558200581445']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/eswc2011/paper/ontologies/24', '457', 'Using Pseudo Feedback to Improve Cross-Lingual Ontology Mapping', 'While ontologies are widely accepted internationally as knowledge management mechanism across disciplines, the ability to reason over knowledge bases regardless of the natural languages used in them has become a pressing issue in digital content management. To enable knowledge sharing and reuse, ontology mapping techniques must be able to work with otherwise isolated ontologies that are labelled in diverse natural languages. Machine translation techniques are often employed by cross-lingual ontology mapping approaches to turn a cross-lingual mapping problem into a monolingual mapping problem which can then be solved by state of the art monolingual ontology matching tools. However in the process of doing so, complications introduced by machine translation tools can compromise the performance of the subsequent monolingual matching techniques. In this paper, a novel approach to improve the quality of cross-lingual ontology mapping is presented and evaluated. The proposed approach adopts the pseudo feedback technique that is similar to the well understood relevance feedback mechanism used in the field of information retrieval. It is shown through the evaluation that the pseudo feedback feature can enhance the effectiveness of machine translation and monolingual matching techniques in a cross-lingual ontology mapping scenario.', 'https://w3id.org/scholarlydata/person/declan-osullivan,https://w3id.org/scholarlydata/person/rob-brennan,https://w3id.org/scholarlydata/person/bo-fu', 'Machine Translation,Cross-Lingual Ontology Mapping,Pseudo Feedback', '0.3543070464678009'], ['https://w3id.org/scholarlydata/inproceedings/eswc2014/paper/ws/FEOSW/2', '767', 'Analyzing Stock Market Fraud Cases Using a Linguistics-Based Text Mining Approach', ' ', 'https://w3id.org/scholarlydata/person/babis-theodoulidis,https://w3id.org/scholarlydata/person/mohamed-zaki', '', '0.3530228495326331'], ['https://w3id.org/scholarlydata/inproceedings/www2011/paper/inverted-index-compression-via-online-document-rou', '4153', 'Inverted Index Compression via Online Document Routing', 'Modern search engines are expected to make documents searchable shortly after they appear on the ever changing Web. To satisfy this requirement, the Web is frequently crawled. Due to the sheer size of their indexes, search engines distribute the crawled documents among thousands of servers in a scheme called local index-partitioning, such that each server indexes only several million pages. To ensure documents from the same host (e.g., www.nytimes.com) are distributed uniformly over the servers, for load balancing purposes, random routing of documents to servers is common. To expedite the time documents become searchable after being crawled, documents may be simply appended to the existing index partitions. However, indexing by merely appending documents, results in larger index sizes since document reordering for index compactness is no longer performed. This, in turn, degrades search query processing performance which depends heavily on index sizes. A possible way to balance quick document indexing with efficient query processing, is to deploy online document routing strategies that are designed to reduce index sizes. This work considers the effects of several online document routing strategies on the aggregated partitioned index size. We show that there exists a tradeoff between the compression of a partitioned index and the distribution of documents from the same host across the index partitions (i.e., host distribution). We suggest and evaluate several online routing strategies with regard to their compression, host distribution, and complexity. In particular, we present a term based routing algorithm which is shown analytically to provide better compression results than the industry standard random routing scheme. In addition, our algorithm demonstrates comparable compression performance and host distribution while having much better running time complexity than other document routing heuristics. Our findings are validated by experimental evaluation performed on a large benchmark collection of Web pages.', 'https://w3id.org/scholarlydata/person/edo-liberty,https://w3id.org/scholarlydata/person/gal-lavee,https://w3id.org/scholarlydata/person/oren-somekh,https://w3id.org/scholarlydata/person/ronny-lempel', '', '0.31457189410140785']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/iswc-2017-iswc2017-research-230', '4816', 'A Decidable Very Expressive Description Logic for Databases', 'We introduce DLR+, an extension of the n-ary propositionally closed description logic DLR to deal with attribute-labelled tuples (generalising the positional notation), projections of relations, and global and local objectification of relations, able to express inclusion, functional, key, and external uniqueness dependencies. The logic is equipped with both TBox and ABox axioms. We show how a simple syntactic restriction on the appearance of projections sharing common attributes in a DLR+ knowledge base makes reasoning in the language decidable with the same computational complexity as DLR. The obtained DLR+- n-ary description logic is able to encode more thoroughly conceptual data models such as EER, UML, and ORM.', 'https://w3id.org/scholarlydata/person/alessandro-artale,https://w3id.org/scholarlydata/person/enrico-franconi,https://w3id.org/scholarlydata/person/rafael-penaloza,https://w3id.org/scholarlydata/person/francesco-sportelli', 'Description Logics,Computational Complexity,Foundation of Databases', '0.3245369620555854'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2018-workshop_and_tutorial-283', '5585', 'THE THIRTEENTH INTERNATIONAL WORKSHOP ON ONTOLOGY MATCHING (OM-2018)', ' ', '', '', '0.3184022844127792'], ['https://w3id.org/scholarlydata/inproceedings/iswc2008/paper/doctoral_consortium/20', '1868', 'End-User Assisted Ontology Evolution in Uncertain Domains', 'Statical ontology learning from large text corpora is a well understood task while evolving ontologies dynamically from user-input has rarely been adressed so far. Evolution of ontologies has to deal with vague or incomplete information. Accordingly, the formalism used for knowledge representation must be able to deal with this kind of information. Classical logical approaches such as description logics are particularly poor in adressing uncertainty. Ontology evolution may benefit from exploring probabilistic or fuzzy approaches to knowledge representation. In this thesis an approach to evolve and update ontologies is developed which uses explicit and implicit user-input and extends probabilistic approaches to ontology engineering.', 'https://w3id.org/scholarlydata/person/thomas-scharrenbach', 'Ontology Evolution,Ontology Learning,FuzzyOWL,Probabilistic Description Logics,User-Interaction', '0.31769490132102823']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/www2012/phd/42', '3587', 'Building reputation and trust using federated search and opinion mining', 'Online reputation addresses trust relationships amongst agents in the dynamic open systems which can appear as ratings, recommendations, referrals and feedbacks. Several reputation models and rating aggregation algorithms have been proposed. However, finding a trusted entity on the web is still an issue as all reputation systems work individually. The aim of this project is to introduce a global reputation system that aggregates people’s opinions from different resources (e.g. e-commerce websites, review websites) with the help federated search techniques. Then it will analyze opinions using sentiment analysis approach to extract high quality opinions and generate trust in the search result.', 'https://w3id.org/scholarlydata/person/somayeh-khatiban', 'information retrieval,social media,opinion mining,Reputation system,federated search', '0.30366361064006125'], ['https://w3id.org/scholarlydata/inproceedings/iswc2016/paper/applications/application-21', '5211', 'Extracting Semantic Information for e-Commerce', ' ', '', '', '0.2778268032658262'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2019-industry-561', '4918', 'Using Event Graph to Improve Question Answering in E-commerce Customer Service', 'AliMe is an intelligent assistant that offers question answering service in the E-commerce customer service field. By representing knowledge as question answer (QA) pairs, AliMe is able to serve millions of customer questions per day and address 90%+ of them. However, in regulation-oriented scenarios, questions of type “why”, “whether”, “what if ” and “how next” often require knowledge reasoning to obtain a specific or precise answer, and QA-style knowledge representation turns out to be insufficient. To enable AliMe to better understand and serve customer questions, we propose to represent regulation knowledge as event graph, design systematic approach to map customer questions to events, and perform reasoning on event graph according to business rules. We launch our new system in the “counterfeiting penalty” scenario. Online results suggest that our new approach is able to gain better resolution.', 'https://w3id.org/scholarlydata/person/feng-lin-li,https://w3id.org/scholarlydata/person/kehan-chen,https://w3id.org/scholarlydata/person/qi-huang,https://w3id.org/scholarlydata/person/weijia-chen,https://w3id.org/scholarlydata/person/yan-wan,https://w3id.org/scholarlydata/person/yikun-guo', 'Explainable Question Answering,Event Graph,Knowledge Reasoning', '0.2771727115110766']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/dh2010/abstracts/paper/ab-605', '34', 'Entropy and Divergence in a Modern Fiction Corpus', ' ', 'https://w3id.org/scholarlydata/person/hugh-craig', '', '0.32900903188701586'], ['https://w3id.org/scholarlydata/inproceedings/eswc-2017-demo-pd_34', '5080', 'YAM++ online: a Multi-task Platform for Ontology and Thesaurus Matching', 'We present the multi-task web platform YAM++ online for ontology↵and thesaurus matching, featuring a mapping validation and enrichment interface.↵The online matcher is based on the highly performant YAM++ system, while the↵validator allows to visualize an alignment, edit the relation type and add new↵mappings discovered through a keyword-based search by a domain expert.', 'https://w3id.org/scholarlydata/person/vincent-emonet,https://w3id.org/scholarlydata/person/duyhoa-ngo,https://w3id.org/scholarlydata/person/zohra-bellahsene,https://w3id.org/scholarlydata/person/konstantin-todorov', 'Ontology Alignment,Mapping Validation,Web Interface', '0.26353645128730097']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/eswc2013/paper/eswc-2013/52', '590', 'Towards Evaluating Interactive Ontology Matching Tools', 'With a growing number of ontologies used on the semantic web, agents can fully make sense of different datasets only if correspondences between those ontologies are known. Ontology matching tools have been proposed to find such correspondences. While the current research focus is mainly on fully automatic matching tools, some approaches have been proposed that involve the user in the matching process. However, there are currently no benchmarks and test methods to compare such tools. In this paper, we introduce a number of quality measures for interactive ontology matching tools, and we discuss means to automatically run benchmark tests for such tools. To demonstrate those evaluations, we show examples on assessing the quality of interactive matching tools which involve the user in matcher selection and matcher parametrization.', 'https://w3id.org/scholarlydata/person/heiko-paulheim,https://w3id.org/scholarlydata/person/dominique-ritze,https://w3id.org/scholarlydata/person/sven-hertling', 'User Interaction,Ontology Alignment,Ontology Matching,Evaluation Measures', '0.33189473328959596'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2018-industry-264', '5433', 'Maintaining the WarSampo Linked Data Cloud', ' ', '', '', '0.32229043362469945'], ['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/219', '2787', 'Towards a Glossary of Activities in the Ontology Engineering Field', 'The Semantic Web of the future will be characterized by using a very large number of ontologies embedded in ontology networks. It is important to provide strong methodological support for collaborative and context-sensitive development of networks of ontologies. This methodological support includes the identification and definition of which activities should be carried out when ontology networks are collaboratively built. In this paper we present the consensus reaching process followed within the NeOn consortium for the identification and definition of the activities involved in the ontology network development process. The consensus reaching process here presented produces as a result the NeOn Glossary of Activities. This work was conceived due to the lack of standardization in the Ontology Engineering terminology, which clearly contrasts with the Software Engineering field. Our future aim is to standardize the NeOn Glossary of Activities.', 'https://w3id.org/scholarlydata/person/asuncion-gomez-perez,https://w3id.org/scholarlydata/person/mari-carmen-suarez-figueroa', 'Semantic Web,Ontologies,Other', '0.312434037433955']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2002/proceedings/paper-35', '1256', 'The Usable Ontology: An Environment for Building and Assessing a Domain Ontology', ' ', '', '', '0.3252728217238334'], ['https://w3id.org/scholarlydata/inproceedings/swws2001/position-proceedings/paper-18', '1103', 'Semantics for Scientific Data: Smart Dictionaries as Ontologies', ' ', '', '', '0.32070159224041694'], ['https://w3id.org/scholarlydata/inproceedings/iswc2016/paper/research/research-68', '5378', 'Updating DL-Lite Ontologies through First-Order Queries', ' ', '', '', '0.3158665323011876']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 2\n",
      "[['https://w3id.org/scholarlydata/inproceedings/swws2001/proceedings/paper-07', '1137', 'Brokerage of Intellectual Property Rights in the Semantic Web', ' ', '', '', '0.3437439157758214'], ['https://w3id.org/scholarlydata/inproceedings/eswc-2017-workshops-ldq-2', '5170', 'Towards Ontology Quality Assessment', 'The success of systems making use of ontology schemas de- pend mainly on the quality of their underlying ontologies. This has been acknowledged by researchers who responded by suggesting metrics to measure different aspects of quality. Tools have also been designed, but determining the set of quality metrics to use may not be a straightfor- ward task. Research on ontology quality shows that detection of problems at an early stage of the ontology development cycle is necessary to re- duce costs and maintenance at later stages, which is more difficult to achieve and requires more effort. Assessment using the right metrics is therefore crucial to identify key quality problems. This ensures that the data and instances of the ontology schema are sound and fit for purpose. Our contribution is a systematic survey on quality metrics applicable to ontologies in the Semantic Web, and preliminary investigation towards methods to visualise quality problems in ontologies.', 'https://w3id.org/scholarlydata/person/silvio-mc-gurk,https://w3id.org/scholarlydata/person/charlie-abela,https://w3id.org/scholarlydata/person/jeremy-debattista', 'Linked Data Quality', '0.3423090043578196'], ['https://w3id.org/scholarlydata/inproceedings/iswc2016/paper/posterdemo/poster-18', '5319', 'pSPARQL: A Querying Language for Probabilistic RDF (Extended Abstract)', ' ', '', '', '0.3122892916335551']] indice 2\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2007+aswc2007/tracks/research/papers/295', '1832', 'The Fundamentals of iSPARQL - A Virtual Triple Approach For Similarity-Based Semantic Web Tasks', 'This research explores three SPARQL-based techniques to solve Semantic Web tasks that often require similarity measures, such as semantic data integration, ontology mapping, and Semantic Web service matchmaking. Our aim is to see how far it is possible to integrate customized similarity functions (CSF) into SPARQL to achieve good results for these tasks. Our first approach exploits virtual triples calling property functions to establish virtual relations among resources under comparison; the second approach uses extension functions to filter out resources that do not meet the requested similarity criteria; finally, our third technique applies new solution modifiers to post-process a SPARQL solution sequence. The semantics of the three approaches are formally elaborated and discussed. We close the paper with a demonstration of the usefulness of our iSPARQL framework in the context of a data integration and an ontology mapping experiment.', 'https://w3id.org/scholarlydata/person/abraham-bernstein,https://w3id.org/scholarlydata/person/christoph-kiefer,https://w3id.org/scholarlydata/person/markus-stocker', '', '0.4651717481419136'], ['https://w3id.org/scholarlydata/inproceedings/iswc2012/industry-proceedings/paper-05', '2284', 'Fortifying a SPARQL Endpoint for Enterprise Usage Scenarios', ' ', '', '', '0.4396884233911203'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2018-resources-199', '5566', 'Comunica: a Modular SPARQL Query Engine for the Web', ' ', '', '', '0.4144952752296783']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2010/paper/456', '2210', 'MoKi: aWiki-Based Conceptual Modeling Tool', 'The success of wikis for collaborative knowledge construction is triggering the development of a number of tools for collaborative conceptual modeling based on them. In this paper we present a completely revised version of MoKi, a tool for modelling ontologies and business process models in an integrated way.', 'https://w3id.org/scholarlydata/person/chiara-ghidini,https://w3id.org/scholarlydata/person/luciano-serafini,https://w3id.org/scholarlydata/person/marco-rospocher', 'Semantic Web', '0.2868894218729516'], ['https://w3id.org/scholarlydata/inproceedings/icpw2009/paper/main/3', '1031', 'An Interaction Design Perspective on the Pragmatic Web: Preliminary Thoughts', ' ', 'https://w3id.org/scholarlydata/person/c-baranauskas,https://w3id.org/scholarlydata/person/heiko-hornung,https://w3id.org/scholarlydata/person/m-cecilia', '', '0.27815142743197685'], ['https://w3id.org/scholarlydata/inproceedings/www2011/phd/application-of-semantic-web-technologies-for-multi', '4196', 'Application of Semantic Web Technologies for Multimedia Interpretation', 'Despite numerous outstanding results, highly complex and specialized multimedia algorithms have not been able to fulfill the promise of fully automated multimedia interpretation. An essential problem is that they are insufficiently aware of the context they operate in. Algorithms that do take a form of context in consideration, often function in a domain-specific environment. The generic framework proposed in this paper stimulates algorithm collaboration on an interpretation task by continuously actualizing the context of the multimedia item under interpretation. Semantic Web knowledge, combined with reasoning methods, forms the corner stone of the integration of these various interacting agents. We believe that this framework will enable an advanced interpretation of multimedia data that goes beyond the capabilities of individual algorithms. A basic platform implementation already indicates the potential of the concept, clearing the path for even more complex interpretation scenarios.', 'https://w3id.org/scholarlydata/person/ruben-verborgh', '', '0.26526163608983877']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/www2012/eu/26', '3525', 'The Multilingual Web', 'In this paper we report in the Multilingual Web initiative which is a collaboration between the Internationalization Activity of the W3C and the European Commission (EC), realized as a series of EC-funded projects. We review the outcomes of the first of these projects, “Multilingual Web”, which conducted a successful workshop series aimed at analyzing borders or “gaps” within Web technology standardization that currently hinder multilinguality on the Web. The resulting insight into the scientific, industrial and user stakeholders involved led to a further project “MultilingualWeb-LT”. This project has established a cross-sector W3C Working Group that will address some of the gaps that have been identified through standardization of meta-data.', 'https://w3id.org/scholarlydata/person/david-lewis,https://w3id.org/scholarlydata/person/felix-sasaki,https://w3id.org/scholarlydata/person/david-filip', 'Internationalization,Language Technologies,Localization,Standardization,Web Technologies', '0.31093002448386176'], ['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/913', '3307', 'From Research to Application in Multilingual Information Access: the Contribution of Evaluation', 'The importance of evaluation in promoting research and development in the information retrieval and natural language processing domains has long been recognised but is this sufficient? In many areas there is still a considerable gap between the results achieved by the research community and their implementation in commercial applications. This is particularly true for the cross-language or multilingual retrieval areas. Despite the strong demand for and interest in multilingual IR functionality, there are still very few operational systems on offer. The Cross Language Evaluation Forum (CLEF) is now taking steps aimed at changing this situation. The paper provides a critical assessment of the main results achieved by CLEF so far and discusses plans now underway to extend its activities in order to have a more direct impact on the application sector.', 'https://w3id.org/scholarlydata/person/julio-gonzalo,https://w3id.org/scholarlydata/person/giorgio-di-nunzio,https://w3id.org/scholarlydata/person/martin-braschler,https://w3id.org/scholarlydata/person/nicola-ferro,https://w3id.org/scholarlydata/person/mark-sanderson,https://w3id.org/scholarlydata/person/carol-peters', 'Question Answering,Multilinguality,Information Extraction, Information Retrieval', '0.3001733941614163'], ['https://w3id.org/scholarlydata/inproceedings/www2012/paper/1290', '4367', 'SPARQL Template Based Question Answering', 'Question answering (QA) systems provide a user-friendly way to obtain information from data sources. In particular, the generation of SPARQL queries on a particular RDF knowledge base from natural language queries has gained momentum, since they only require minimal user effort. However, none of the currently existing QA system can reliably generate such queries over large and heterogeneous knowledge base, which we can commonly find in the web of data. In this paper, we propose an approach, which is based on generating templates from natural language questions. Those templates are instantiated to form a set of SPARQL queries, which are then tested using several optimisations. We show the feasibility of the approach by successfully applying it on an existing QA benchmark.', 'https://w3id.org/scholarlydata/person/axel-cyrille-ngonga-ngomo,https://w3id.org/scholarlydata/person/daniel-gerber,https://w3id.org/scholarlydata/person/jens-lehmann,https://w3id.org/scholarlydata/person/lorenz-buehmann,https://w3id.org/scholarlydata/person/philipp-cimiano,https://w3id.org/scholarlydata/person/christina-unger', 'SPARQL,Semantic Web,Question Answering,Natural-Language Patterns', '0.2989728757526988']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2013/doctoral-consortium-proceedings/paper-02', '2535', 'Semantic Interpretation of Mobile Phone Records Exploiting Background Knowledge', ' ', '', '', '0.30863369178224426'], ['https://w3id.org/scholarlydata/inproceedings/iswc2005/poster-demo-proceedings/paper-70', '1593', \"Toward Task Ontology-based Modeling for Mobile Phone Users' Activity\", ' ', '', '', '0.3044584990516327'], ['https://w3id.org/scholarlydata/inproceedings/eswc2014/paper/demo/114', '606', 'Modelling OWL ontologies with Graffoo', ' ', 'https://w3id.org/scholarlydata/person/aldo-gangemi,https://w3id.org/scholarlydata/person/david-shotton,https://w3id.org/scholarlydata/person/fabio-vitali,https://w3id.org/scholarlydata/person/silvio-peroni,https://w3id.org/scholarlydata/person/riccardo-falco', '', '0.30385566788564355']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2008/paper/poster_demo/60', '1939', 'Collaborative Protege: Enabling Community-based Authoring of Ontologies', 'Ontologies are becoming so large in their coverage that no single person or a small group of people can develop them effectively and ontology development becomes a community-based enterprise. We present Collaborative Protege - an extension of the Protege ontology editor that we have designed specifically to support the collaboration process for a community of users. During the ontology-development process, Collaborative Protege allows users to hold discussions about the ontology components and changes using typed annotations; it tracks the change history of the ontology entities; it provides a chat and search functionality. Users edit simultaneously an ontology stored in a common repository. All changes made by a user are seen immediately by other users. Collaborative Protege is open source and distributed with the full installation of Protege.', 'https://w3id.org/scholarlydata/person/mark-a-musen,https://w3id.org/scholarlydata/person/natalya-noy,https://w3id.org/scholarlydata/person/tania-tudorache', 'Semantic Web,Protege,collaboration,ontology development,ontology editing', '0.3299437759359969'], ['https://w3id.org/scholarlydata/inproceedings/iswc2010/paper/146', '2144', 'Optimize First, Buy Later: Analyzing Metrics to Ramp-up Very Large Knowledge Bases', 'As knowledge bases move into the landscape of larger ontologies and have terabytes of related data, we must work on optimizing the performance of our tools. We are easily tempted to buy bigger machines or to fill rooms with armies of little ones to address the scalability problem. Yet, careful analysis and evaluation of the characteristics of our data-using metrics-often leads to dramatic improvements in performance. Firstly, are current scalable systems scalable enough? We found that for large or deep ontologies (some as large as 500,000 classes) it is hard to say because benchmarks obscure the load-time costs for materialization. Therefore, to expose those costs, we have synthesized a set of more representative ontologies. Secondly, in designing for scalability, how do we manage knowledge over time? By optimizing for data distribution and ontology evolution, we have reduced the population time, including materialization, for the NCBO Resource Index, a knowledge base of 16.4 billion annotations linking 2.4 million terms from 200 ontologies to 3.5 million data elements, from one week to less than one hour for one of the large datasets on the same machine.', 'https://w3id.org/scholarlydata/person/mark-a-musen,https://w3id.org/scholarlydata/person/natalya-noy,https://w3id.org/scholarlydata/person/nigam-shah,https://w3id.org/scholarlydata/person/clement-jonquet,https://w3id.org/scholarlydata/person/paea-lependu,https://w3id.org/scholarlydata/person/paul-r-alexander', 'semantic web', '0.3293349782107359'], ['https://w3id.org/scholarlydata/inproceedings/iswc2012/poster-demo-proceedings/paper-28', '2319', 'SPARQLoid - a Querying System using Own Ontology and Ontology Mappings with Reliability', 'Heterogeneity of ontologies on the web of data is very important problem. To solve this problem, there are a lot of researches about ontology mapping/alignment/matching. This paper shows an application called \\x93SPARQLoid\\x94 that is using a query rewriting method to enable the users to query any SPARQL endpoint with the user\\x92s own ontology even when their mappings are not reliable enough. Often ontology matching is very difficult problem and it sometimes produces mappings under a certain reliability. Based on the given reliability degrees on those mappings, SPARQLoid allows users to query data in the target SPARQL endpoints by using their own (or a specified certain) ontology under a control of sorting order based on their mapping reliability.', '', '', '0.30228712251945766']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2012/industry-proceedings/paper-09', '2288', 'Semantic Web Success Story: Practical Integration of Semantic Web Technology and Linked Data Principles in the Architecture and Implementation of an Enterprise Product', ' ', '', '', '0.3607335880098168'], ['https://w3id.org/scholarlydata/inproceedings/iswc2016/paper/doctoral/doctoralconsortium-12', '5221', 'Querying Distributed Heterogeneous Linked Data Interfaces through Reasoning', ' ', '', '', '0.32588853637862447'], ['https://w3id.org/scholarlydata/inproceedings/iswc2013/data.semanticweb.org/workshop/sml2od/2013/proceedings/paper-06', '2494', 'The role of vocabularies for estimating carbon footprint for food recipies using Linked Open Data', ' ', '', '', '0.3241727459977818']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/www2009/paper/63', '3955', 'Triplify \\x97 Light-Weight Linked Data Publication from Relational Databases', 'In this paper we present Triplify – a simplistic but effective approach to publish Linked Data from relational databases. Triplify is based on mapping HTTP-URI requests onto relational database queries. Triplify transforms the resulting relations into RDF statements and publishes the data on the Web in various RDF serializations, in particular as Linked Data. The rationale for developing Triplify is that the largest part of information on the Web is already stored in structured form, often as data contained in relational databases, but usually published by Web applications only as HTML mixing structure, layout and content. In order to reveal the pure structured information behind the current Web, we have implemented Triplify as a light-weight software component, which can be easily integrated into and deployed by the numerous, widely installed Web applications. Our approach includes a method for publishing update logs to enable incremental crawling of linked data sources. Triplify is complemented by a library of configurations for common relational schemata and a REST-enabled data source registry. Triplify configurations containing mappings are provided for many popular Web applications, including osCommerce, WordPress, Drupal, Gallery, and phpBB. We will show that despite its light-weight architecture Triplify is usable to publish very large datasets, such as 160GB of geo data from the OpenStreetMap project.', '', 'Semantic/Data Web', '0.26948183550219224'], ['https://w3id.org/scholarlydata/inproceedings/www2010/paper/main/542', '4042', 'How useful are your comments? - Analyzing and Predicting YouTube Comments and Comment Ratings', 'An analysis of the leading social video sharing platform YouTube reveals a high amount of community feedback through comments for published videos as well as through meta ratings for these comments. In this paper, we present an in-depth study of commenting and comment rating behavior on a sample of more than 6 million comments from about 67,000 YouTube videos for which we analyzed various dependencies between comments, views, comment ratings and topic categories. Furthermore, we also study the influence of sentiment expressed in comments on the ratings for these comments using the SentiWordNet thesaurus, a lexical WordNet-based resource containing sentiment annotations. Finally, to predict community acceptance for comments not yet rated, we build different classifiers for the prediction of ratings for these comments. The results of our large-scale evaluations are promising and indicate that community feedback on already rated comments can help to filter new unrated comments or suggest particularly useful but still unrated comments.', 'https://w3id.org/scholarlydata/person/wolfgang-nejdl,https://w3id.org/scholarlydata/person/sergiu-chelaru,https://w3id.org/scholarlydata/person/jose-san-pedro,https://w3id.org/scholarlydata/person/stefan-siersdorfer', 'analytics,Social data analysis', '0.2515680817085059']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2005/poster-demo-proceedings/paper-71', '1594', 'Towards Browsing Distant Metadata with Semantic Signature', ' ', '', '', '0.3846858878089152'], ['https://w3id.org/scholarlydata/inproceedings/www2008/paper/373', '3757', 'Diverse and Representative Image Search Results for Landmarks', \"Can we automatically generate representative and diverse views of the world's landmarks from community-contributed collections on the web? Community-contributed collections of media on the web are a becoming a vast, rich resource for image and video on a long-tailed array of topics.  We use a combination of context- and content-based tools to generate representative sets of images for location-driven features and landmarks, a common search task. To do that, we using location and other metadata, as well as tags associated with images, and the images' visual features. We present an approach to extracting tags that represent landmarks. We show how to use unsupervised methods to extract representative views and images for each landmark. This approach can potentially scale to provide better search and representation for every landmark, worldwide.  We evaluate the system in the context of web image search using a real-life dataset of 110,000 images from the San Francisco area.\", 'https://w3id.org/scholarlydata/person/lyndon-kennedy,https://w3id.org/scholarlydata/person/mor-naaman', 'tagging,social media,photo collections,image analysis,image search', '0.3692065165318993'], ['https://w3id.org/scholarlydata/inproceedings/iswc2005/poster-demo-proceedings/paper-53', '1576', 'Semantic Wiki as an Integrated Content and Metadata Management System', ' ', '', '', '0.3207377509027934']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/www2009/paper/105', '3825', 'A Geographical Analysis of Knowledge Production in Computer Science', 'We analyze knowledge production in Computer Science by means of coauthorship networks. For this, we consider 30 graduate programs of different regions of the world, being 8 programs in Brazil, 16 in North America (3 in Canada and 13 in the United States), and 6 in Europe (2 in France, 1 in Switzerland and 3 in the United Kingdom). We use a dataset that consists of 176,537 authors and 352,766 publication entries distributed among 2,176 publication venues. The results obtained for different metrics of collaboration social networks indicate the process of knowledge production has a ˘changed differently for each region. Research is increasingly done in teams across different fields of Computer Science. The size of the giant component indicates the existence of isolated collaboration groups in the European network, contrasting to the degree of connectivity found in the Brazilian and North-American counterparts. We also analyzed the temporal evolution of the social networks representing the three regions. The number of authors per paper experienced an increase in a time span of 12 years. We observe that the number of collaborations between authors grows faster than the number of authors, benefiting from the existing network structure. The temporal evolution shows differences between well-established fields, such as Databases and Computer Architecture, and emerging fields, like Bioinformatics and Geoinformatics. The patterns of collaboration analyzed in this paper contribute to an overall understanding of Computer Science research in different geographical regions that could not be achieved without the use of complex networks and a large publication database.', '', 'WWW in lbero-America', '0.35516015437841664'], ['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/358', '2891', 'LC-STAR II: Starring more Lexica', 'LC-STAR II is a follow-up project of the EU funded project LC-STAR (Lexica and Corpora for Speech-to-Speech Translation Components, IST-2001-32216). LC-STAR II develops large lexica containing information for speech processing in ten languages targeting especially automatic speech recognition and text to speech synthesis but also other applications like speech-to-speech translation and tagging. The project follows by large the specifications developed within the scope of LC-STAR covering thirteen languages: Catalan, Finnish, German, Greek, Hebrew, Italian, Mandarin Chinese, Russian, Turkish, Slovenian, Spanish, Standard Arabic and US-English. The ten new LC-STAR II languages are: Brazilian-Portuguese, Cantonese, Czech, English-UK, French, Hindi, Polish, Portuguese, Slovak, and Urdu. The project started in 2006 with a lifetime of two years. The project is funded by a consortium, which includes Microsoft (USA), Nokia (Finland), NSC (Israel), Siemens (Germany) and Harmann/Becker (Germany). The project is coordinated by UPC (Spain) and validation is performed by SPEX (The Netherlands), and CST (Denmark). The developed language resources will be shared among partners.This paper presents a summary of the creation of word lists and lexica and an overview of adaptations of the specifications and conceptual representation model from LC-STAR to the new languages. The validation procedure will be presented too.', 'https://w3id.org/scholarlydata/person/hanne-fersoe,https://w3id.org/scholarlydata/person/henk-van-den-heuvel,https://w3id.org/scholarlydata/person/ute-ziegenhain,https://w3id.org/scholarlydata/person/asuncion-moreno', 'Lexicon, lexical database,LR national/international projects, organizational/policy issues,Standards for LRs', '0.3311115588370449'], ['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/574', '3055', 'F0 of Adolescent Speakers   First Results for the German Ph@ttSessionz Database', 'The first release of the German Ph@ttSessionz speech database contains read and spontaneous speech from 864 adolescent speakers and is the largest database of its kind for German. It was recorded via the WWW in over 40 public schools in all dialect regions of Germany. In this paper, we present a cross-sectional study of f0 measurements on this database. The study documents the profound changes in male voices at the age 13-15. Furthermore, it shows that on a perceptive mel-scale, there is little difference in the relative f0 variability for male and female speakers. A closer analysis reveals that f0 variability is dependent on the speech style and both the length and the type of the utterance. The study provides statistically reliable voice parameters of adolescent speakers for German. The results may contribute to making spoken dialog systems more robust by restricting user input to utterances with low f0 variability.', 'https://w3id.org/scholarlydata/person/florian-schiel,https://w3id.org/scholarlydata/person/tania-ellbogen,https://w3id.org/scholarlydata/person/christoph-draxler', 'Speech recognition and understanding,Speech resource/database,Phonetic Databases, Phonology', '0.3132205233422323']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2005/poster-demo-proceedings/paper-57', '1580', 'Semantics-based Publication Management using RSS and FOAF', ' ', '', '', '0.2968484513075344'], ['https://w3id.org/scholarlydata/inproceedings/ekaw-2016-phdsym-239', '4548', 'Automatic maintenance of semantic annotations', 'Biomedical Knowledge Organization Systems (KOS) play a key role in enriching information in order to make them machine understandable. This is done through semantic annotation which consists in the association of concept labels taken from KOS with pieces of digital information taken from the source to annotate. However, the dynamic nature of these KOS directly impacts on the annotations, creating a mismatch between the enriched data and the concept labels. This PhD study addresses the evolution of semantic annotations due to the evolution of KOS and aims at proposing an approach to automatize the maintenance of semantic annotations.', 'https://w3id.org/scholarlydata/person/silvio-cardoso', 'Semantic annotations,annotation maintenance,knowledge organization systems', '0.28429253124857384'], ['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/862', '3268', 'Authorship Identification of Romanian Texts with Controversial Paternity', 'In this work we propose a new strategy for the authorship identification problem and we test it on an example from Romanian literature: did Radu Albala found the continuation of Mateiu Caragiale s novel Sub pecetea tainei, or did he write himself the respective continuation? The proposed strategy is based on the similarity of rankings of function words; we compare the obtained results with the results obtained by a learning method (namely Support Vector Machines -SVM- with a string kernel).', 'https://w3id.org/scholarlydata/person/anca-dinu,https://w3id.org/scholarlydata/person/liviu-dinu,https://w3id.org/scholarlydata/person/marius-popescu', 'LR national/international projects, organizational/policy issues,Acquisition, Machine Learning,Authoring tools, proofing', '0.258208509929194']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/lrec2008/papers/547', '3031', 'Building a Bio-Event Annotated Corpus for the Acquisition of Semantic Frames from Biomedical Corpora', 'This paper reports on the design and construction of a bio-event annotated corpus which was developed with a specific view to the acquisition of semantic frames from biomedical corpora. We describe the adopted annotation scheme and the annotation process, which is supported by a dedicated annotation tool. The annotated corpus contains 677 abstracts of biomedical research articles.', 'https://w3id.org/scholarlydata/person/paul-thompson,https://w3id.org/scholarlydata/person/philip-cotter,https://w3id.org/scholarlydata/person/giulia-venturi,https://w3id.org/scholarlydata/person/john-mcnaught,https://w3id.org/scholarlydata/person/simonetta-montemagni,https://w3id.org/scholarlydata/person/sophia-ananiadou,https://w3id.org/scholarlydata/person/andrea-trabucco', 'Semantics,Corpus (creation, annotation, etc.),Text mining', '0.3434545333450218'], ['https://w3id.org/scholarlydata/inproceedings/iswc2006/paper-8', '1731', 'The Summary Abox: Cutting Ontologies Down to Size', 'Reasoning on OWL ontologies is known to be intractable in the worst-case, which is a serious problem because in practice, most OWL ontologies have large Aboxes, i.e., numerous assertions about individuals and their relations. We propose a technique that uses a summary of the ontology (summary Abox ) to reduce reasoning to a small subset of the original Abox, and prove that our techniques are sound and complete.We demonstrate the scalability of this technique for consistency detection in 4 ontologies, the largest of which has 6.5 million role assertions.', '', '', '0.32922976113982694'], ['https://w3id.org/scholarlydata/inproceedings/iswc2004/poster-proceedings/paper-08', '1419', 'Description of an Instructional Ontology and its Application in Web Services for Education', ' ', '', '', '0.31511650600672647']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc-2019-outrageous-494', '4934', 'Co-LOD: Continuous Space Linked Open Data', 'The Linked Open Data (LOD) initiative has been one of the successful manifestations of Semantic Web efforts over the last two decades, with near-exponential growth of LOD datasets in the initial years. Entities and datasets on LOD are naturally discrete, making them amenable to both well-defined reasoning and retrieval procedures that ultimately return lists or sets of resource identifiers fulfilling some criteria (whether stating user intent or using pattern-matching query languages like SPARQL). In recent years, representation learning algorithms have witnessed a powerful ascent in mainstream Artificial Intelligence, fueled in part by the adoption and refinement of neural network architectures like Recurrent Neural Nets and skip-grams, and by empirical successes such as achieved in the natural language processing and knowledge discovery communities by word and graph embeddings. Large datasets, which are almost always required by such algorithms, make it possible to train and release models openly. In some cases, open models can even be released based on proprietary datasets like Twitter corpora. In this paper, we propose that the Semantic Web community position it- self as a pre-eminent research leader in this space by leveraging the vast and diverse collection of structured datasets that are currently available on Linked Open Data, to build out a corresponding continuous-space equivalent.', 'https://w3id.org/scholarlydata/person/pedro-szekely,https://w3id.org/scholarlydata/person/mayank-kejriwal', 'Linked Open Data,Knowledge Graphs,Embeddings,Continuous Space,Representation Learning', '0.28995015436370886'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2017-iswc-2017-in-use-139', '4630', 'A Controlled Crowdsourcing Platform for High-Quality Ontology Development and Data Annotation', 'Traditional approaches to ontology development have a large lapse between the time when a user using the ontology has found a need to extend it and the time when it does get extended, which can be weeks or months. We propose a new approach to ontology development and data annotation where users can extend the ontology on the fly as they add metadata, creating terms that can be immediately adopted by others and eventually become standardized. This approach combines traditional ontology development where terms are discussed and where agreement must be reached before they become part of the core standard, and a crowdsourced approach where expert users (the crowd) can add terms as needed to support their work. We have implemented this approach as a socio-technical system that includes: 1) a crowdsourcing platform to support metadata creation and addition of new terms, 2) a range of social editorial processes to make standardization decisions for those new terms, and 3) a framework for ontology revision and updates to the metadata created with the previous version of the ontology. We present a prototype implementation of this approach for the paleoclimate community, the Linked Earth Platform, containing over 600 datasets and with over 50 active contributors. Users are doing science with their data while extending the ontology, therefore producing useful high-quality metadata.', 'https://w3id.org/scholarlydata/person/yolanda-gil,https://w3id.org/scholarlydata/person/deborah-khider,https://w3id.org/scholarlydata/person/julien-emile-geay,https://w3id.org/scholarlydata/person/nicholas-mckay,https://w3id.org/scholarlydata/person/daniel-garijo,https://w3id.org/scholarlydata/person/varun-ratnakar', 'crowdsourcing,Metadata,software engineering,semantic wiki,collaborative ontology engineering,incremental vocabulary development', '0.26161075736027006']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/dh2010/abstracts/paper/ab-605', '34', 'Entropy and Divergence in a Modern Fiction Corpus', ' ', 'https://w3id.org/scholarlydata/person/hugh-craig', '', '0.3388060865604517'], ['https://w3id.org/scholarlydata/inproceedings/www2007/paper/main/495', '3669', 'A Large-scale Evaluation and Analysis of Personalized Search Strategies', 'Although personalized search has been proposed for many years and many personalization strategies have been investigated, it is still unclear whether personalization is consistently effective on different queries for different users, and under different search contexts. In this paper, we study the problem and get some preliminary conclusions. We present a large-scale personalized search evaluation framework based on search logs and then evaluate five personalized search strategies (including two click-based and three profile-based ones) using 12-day MSN search logs. By analyzing the results, we reveal that personalized search has significant improvement over common web search on some queries but it also has little effect on other queries (e.g., queries with small click entropy) and even harms search accuracy under some situations. Furthermore, we show that click-based personalization strategies perform consistently and considerablely well while profile-based ones are unstable in our experiments. We also reveal that both long-term and short-term contexts are very important in improving search performance for profile-based personalized search strategies.', 'https://w3id.org/scholarlydata/person/zhicheng-dou,https://w3id.org/scholarlydata/person/ruihua-song,https://w3id.org/scholarlydata/person/ji-rong-wen', '', '0.29083371236285466'], ['https://w3id.org/scholarlydata/inproceedings/iswc2002/poster-proceedings/paper-12', '1177', 'Melita: Active Document Enrichment using Adaptive Information Extraction from Text', ' ', '', '', '0.2777694307108975']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/eswc2015/paper/demo/17', '831', 'Distributed Linked Data Business Communication Networks: The LUCID Endpoint', 'With the LUCID Endpoint, we demonstrate how companies can utilize Linked Data technology to provide major data items for their business partners in a timely manner, machine readable and with open and extensible schemata. The main idea is to provide a Linked Data infrastructure which enables all partners to fetch, as well as to clone and to synchronize datasets from other partners over the network. This concept allows for building of networks of business partners much like as social network but in a distributed manner. It furthermore provides a technical infrastructure for business communication acts such as supply chain communication or master data management.', 'https://w3id.org/scholarlydata/person/sebastian-tramp,https://w3id.org/scholarlydata/person/timofey-ermilov,https://w3id.org/scholarlydata/person/marvin-frommhold,https://w3id.org/scholarlydata/person/niklas-petersen,https://w3id.org/scholarlydata/person/ruben-navarro-piris,https://w3id.org/scholarlydata/person/soren-auer', '', '0.3416640905236436'], ['https://w3id.org/scholarlydata/inproceedings/iswc-2017-iswc-2017-posters-and-demos-505', '4681', 'Using Word Embeddings for Search in Linked Data with Ontodia', 'Ontodia is an open-source diagramming and visual exploration tool for linked data and ontologies. Here, we present an extension of the Ontodia data query functionalities. We evaluate different types and configurations of word embeddings for improving recall and flexibility of the Ontodia natural language interface. The demonstration will focus especially on the new query functionalities, where Ontodia will be applied to Wikidata as underlying dataset.', 'https://w3id.org/scholarlydata/person/nikolay-klimov,https://w3id.org/scholarlydata/person/dmitry-mouromtsev,https://w3id.org/scholarlydata/person/dmitry-pavlov,https://w3id.org/scholarlydata/person/gerhard-wohlgenannt,https://w3id.org/scholarlydata/person/yury-emelyanov,https://w3id.org/scholarlydata/person/daniil-razdyakonov', 'Linked Data,Wikidata,word embeddings,Ontodia,visual interface', '0.31813129082421054'], ['https://w3id.org/scholarlydata/inproceedings/iswc2010/paper/903', '2278', 'What does It Look Like, Really? Imagining how Citizens might Effectively, Usefully and Easily Find, Explore, Query and Re-present Open/Linked Data', \"Are we in the semantic web/linked data community effectively attempting to make possible a new literacy - one of data rather than document analysis? By opening up data beyond the now familiar hand crafted Web 2 mash up of data about X plus geography, what are we trying to do, really? Is the goal at least in part to enable net citizens rather than only geeks the ability to pick up, explore, blend, interogate and represent data sources so that we may draw our own statistically informed conclusions about information, and thereby build new knowledge in ways not readily possible before without access to these data seas? If we want citizens rather than just scientists or statisticians or journalists for that matter to be able to pour over data and ask statistically sophisticated questions of comparison and contrast betewen times, places and people, does that mission re-order our research priorities at all? If the goal is to enpower citizens to be able to make use of data, what do we need to make this vision real beyond attending to Tim Berners-Lee's call to 'free your data'? The purpose of this talk therefore will be to look at key interaction issues around defining and delivering a useful, usable *data explorotron* for citizens. In particular, we'll consider who is a 'citizen user' and what access to and tools for linked data sense making means in this case. From that perspective, we'll consider research issues around discovery, exploration, interrogation and representation of data for not only a single wild data source but especially for multiple wild heterogeneous data sources. I hope this talk may help frame some stepping stones towards useful and usable interaction with linked data, and look forward to input from the community to refine such a new literacy agenda further.\", 'https://w3id.org/scholarlydata/person/mc-schraefel', 'semantic web', '0.31063834283926733']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2013/data.semanticweb.org/workshop/ssn/2013/proceedings/paper-05', '2503', 'An Ontology Framework for Water Quality Management', ' ', '', '', '0.33912024937421703'], ['https://w3id.org/scholarlydata/inproceedings/iswc2013/proceedings-1/paper-06', '2627', 'Empirical Study of Logic-Based Modules: Cheap Is Cheerful', 'For ontology reuse and integration, a number of approaches have been devised that aim at identifying modules, i.e., suitably small sets of “relevant” axioms from ontologies. Here we consider three logically sound notions of modules: MEX modules, only applicable to inexpressive ontologies; modules based on semantic locality, a sound approximation of the ﬁrst; and modules based on syntactic locality, a sound approximation of the second (and thus the ﬁrst), widely used since these modules can be extracted from OWL DL ontologies in time polynomial in the size of the ontology. In this paper we investigate the quality of both approximations over a large corpus of ontologies, using our own implementation of semantic locality, which is the ﬁrst to our knowledge. In particular, we show with statistical signiﬁcance that, in most cases, there is no diﬀerence between the two module notions based on locality; where they diﬀer, the additional axioms can either be easily ruled out or their number is relatively small. We classify the axioms that explain the rare diﬀerences into four kinds of “culprits” and discuss which of those can be avoided by extending the deﬁnition of syntactic locality. Finally, we show that diﬀerences between MEX and locality-based modules occur for a minority of ontologies from our corpus and largely aﬀect (approximations of ) expressive ontologies – this conclusion relies on a much larger and more diverse sample than existing comparisons between MEX and syntactic locality-based modules.', '', '', '0.33600361619704316'], ['https://w3id.org/scholarlydata/inproceedings/iswc2016/paper/posterdemo/demo-108', '5246', 'The ESSOT System Goes Wild: an Easy Way For Translating Ontologies', ' ', '', '', '0.3317500322100219']] indice 3\n",
      "\n",
      "\n",
      "  ------------------------------------------------------------ Índex 3\n",
      "[['https://w3id.org/scholarlydata/inproceedings/iswc2009/paper/poster_demo/162', '2070', 'Finding Semantic Web Ontology Terms from Words', 'The Semantic Web was designed to unambiguously define and use ontologies to encode data and knowledge on the Web. Many people find it difficult, however, to write complex RDF statements and queries because it requires familiarity with the appropriate ontologies and the terms they define. We describe a framework that eases the experiences in authoring and querying RDF data, in which we focus on automatically finding a set of appropriate Semantic Web ontology terms from a set of words used as the labels of nodes and edges in an incoming semantic graph.', 'https://w3id.org/scholarlydata/person/tim-finin,https://w3id.org/scholarlydata/person/lushan-han,https://w3id.org/scholarlydata/person/yelena-yesha', '', '0.3349845052858182'], ['https://w3id.org/scholarlydata/inproceedings/iswc2004/demo-proceedings/paper-11', '1381', 'OntoLT Version 1.0: Middleware for Ontology Extraction from Text', ' ', '', '', '0.32204329137149407'], ['https://w3id.org/scholarlydata/inproceedings/iswc2006/paper-23', '1696', 'Ontology Query Answering on Databases', 'With the fast development of Semantic Web, more and more RDF and OWL ontologies are created and shared. The effective management, such as storage, inference and query, of these ontologies on databases gains increasing attentions. This paper addresses ontology query answering by means of a Datalog program, specifically tailored to bridge ontologies and databases. Introducing meta integrity constraints inspired by epistemic interpretations, we believe such a Datalog program suitable to capture ontologies in the DB favor, while keeping reasoning tractable -- Here, we present a logical equivalent knowledge base whose (sound and complete) inference system appears to a Datalog program. As such, a deductive RDF database is responsible for SPARQL query answering involved in OWL and SWRL. Bi-directional strategies, taking advantage of both forward and backward chaining, are then studied to support for this kind of customized Datalog programs, returning exactly answers to the query with respect to its logical framework.', '', '', '0.2931020324461299']] indice 3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import openpyxl \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "resultListIndex1 = []\n",
    "resultListIndex2 = []\n",
    "resultListIndex3 = []\n",
    "baseId1 = []\n",
    "baseId2 = []\n",
    "baseId3 = []\n",
    "\n",
    "contador1 = 0\n",
    "contador2 = 0\n",
    "contador3 = 0\n",
    "\n",
    "papersBase1 =  pd.DataFrame()\n",
    "papersReco1 =  pd.DataFrame()\n",
    "papersBase2 =  pd.DataFrame()\n",
    "papersReco2 =  pd.DataFrame()\n",
    "papersBase3 =  pd.DataFrame()\n",
    "papersReco3 =  pd.DataFrame()\n",
    "index1DF = pd.DataFrame()\n",
    "index2DF = pd.DataFrame()\n",
    "index3DF = pd.DataFrame()\n",
    "varVacia = ''\n",
    "#se trae todos los papers en la variable result \n",
    "result = exPapers()\n",
    "resultsList = []\n",
    "#se guarda en una lista resultsList, los papers\n",
    "for i in result:\n",
    "    resultsList.append(i)    \n",
    "#se guarda en una lista los papers generados aleatoriamente\n",
    "num = 20\n",
    "aleatorios = np.random.choice(resultsList, num, False)\n",
    "print(aleatorios, \"papers Aleatorios\")\n",
    "#De los papers generados aletoriamente, se los recorre para sacar los similares mediante la utilizacion del indice 1\n",
    "for uriPaper1 in aleatorios:\n",
    "    print('\\n\\n ', 60*'-', 'Índex 1')\n",
    "    resultsIdx1 = getData(uriPaper1, queryIdx1)\n",
    "    resultListIndex1 = resultsIdx1[1:4]\n",
    "    print(resultListIndex1, \"indice 1\")\n",
    "    #Metadatos del paper aleatorio\n",
    "    baseId1 = resultsIdx1[0]\n",
    "    index1 = \"1 (basic metadata)\"\n",
    "    papersBase1 = papersBase1.append({'index':index1, 'uri bas':baseId1[0], 'title bas':baseId1[2], 'abstract bas':baseId1[3], 'keywords bas':baseId1[5]}, ignore_index=True)\n",
    "    #Metadatos del paper recomendado\n",
    "    for result1 in resultListIndex1:       \n",
    "        if (contador1 == 3):\n",
    "            contador1 = 0\n",
    "        contador1 = contador1 + 1\n",
    "        papersReco1 = papersReco1.append({'uri bas':baseId1[0], 'uri sim':result1[0], 'title sim':result1[2], 'abstract sim':result1[3], 'keywords sim':result1[5], 'score GraphDB':result1[6], 'Rank GraphDB':contador1, 'rating manually title':varVacia,'rating manually abstract':varVacia, 'rating manually keywords':varVacia,'average ratings':varVacia, 'Rank manually':varVacia, 'error':varVacia}, ignore_index=True)\n",
    "        index1DF = papersBase1.merge(papersReco1, left_on = \"uri bas\", right_on = \"uri bas\")\n",
    "    contador1 = 0   \n",
    "#De los papers generados aletoriamente, se los recorre para sacar los similares mediante la utilizacion del indice 2       \n",
    "for uriPaper2 in aleatorios:\n",
    "    print('\\n\\n ', 60*'-', 'Índex 2')\n",
    "    resultsIdx2 = getData(uriPaper2, queryIdx2)\n",
    "    resultListIndex2 = resultsIdx2[1:4]\n",
    "    print(resultListIndex2, \"indice 2\")\n",
    "    #Metadatos del paper aleatorio\n",
    "    baseId2 = resultsIdx2[0]\n",
    "    index2 = \"2 (basic metadata + dbpedia)\"\n",
    "    papersBase2 = papersBase2.append({'index':index2, 'uri bas':baseId2[0], 'title bas':baseId2[2], 'abstract bas':baseId2[3], 'keywords bas':baseId2[5]}, ignore_index=True)\n",
    "    #Metadatos del paper recomendado\n",
    "    for result2 in resultListIndex2:        \n",
    "        if (contador2 == 3):\n",
    "            contador2 = 0\n",
    "        contador2 = contador2 + 1\n",
    "        papersReco2 = papersReco2.append({'uri bas':baseId2[0], 'uri sim':result2[0], 'title sim':result2[2], 'abstract sim':result2[3], 'keywords sim':result2[5], 'score GraphDB':result2[6], 'Rank GraphDB':contador2,'rating manually title':varVacia,'rating manually abstract':varVacia, 'rating manually keywords':varVacia,'average ratings':varVacia, 'Rank manually':varVacia, 'error':varVacia}, ignore_index=True)\n",
    "        index2DF = papersBase2.merge(papersReco2, left_on = \"uri bas\", right_on = \"uri bas\")\n",
    "    contador2 = 0\n",
    "#De los papers generados aletoriamente, se los recorre para sacar los similares mediante la utilizacion del indice 2       \n",
    "for uriPaper3 in aleatorios:\n",
    "    print('\\n\\n ', 60*'-', 'Índex 3')\n",
    "    resultsIdx3 = getData(uriPaper3, queryIdx3)\n",
    "    resultListIndex3 = resultsIdx3[1:4]\n",
    "    print(resultListIndex3,\"indice 3\")\n",
    "    #Metadatos del paper aleatorio\n",
    "    baseId3 = resultsIdx3[0]\n",
    "    index3 = \"3 (basic metadata + dbpedia + spacy)\"\n",
    "    papersBase3 = papersBase3.append({'index':index3, 'uri bas':baseId3[0], 'title bas':baseId3[2], 'abstract bas':baseId3[3], 'keywords bas':baseId3[5]}, ignore_index=True)\n",
    "    #Metadatos del paper recomendado\n",
    "    for result3 in resultListIndex3:        \n",
    "        if (contador3 == 3):\n",
    "            contador3 = 0\n",
    "        contador3 = contador3 + 1\n",
    "        papersReco3 = papersReco3.append({'uri bas':baseId3[0], 'uri sim':result3[0], 'title sim':result3[2], 'abstract sim':result3[3], 'keywords sim':result3[5], 'score GraphDB':result3[6], 'Rank GraphDB':contador3,'rating manually title':varVacia,'rating manually abstract':varVacia, 'rating manually keywords':varVacia,'average ratings':varVacia, 'Rank manually':varVacia, 'error':varVacia}, ignore_index=True)\n",
    "        index3DF = papersBase3.merge(papersReco3, left_on = \"uri bas\", right_on = \"uri bas\")\n",
    "    contador3 = 0\n",
    "#union de los 3 dataframes mediante .concat\n",
    "pdList = [index1DF, index2DF, index3DF]\n",
    "vmDF = pd.concat(pdList)\n",
    "#creacion del archivo excel \"validacion_manual\"\n",
    "vmDF.to_excel('validacion_manual.xlsx')\n",
    "#simIndex1.to_excel('validacion_manual.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-airplane",
   "metadata": {},
   "source": [
    "# # Validacion Automatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "infrared-terror",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our purpose is to provide end-users a means to query ontology based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. Our contribution is materialized in a system named SWIP, standing for Semantic Web Interface Using Patterns. The demo will present use cases of this system. 1111111\n",
      "  2222222\n",
      "Our purpose is to provide end-users a means to query ontology based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. Our contribution is materialized in a system named SWIP, standing for Semantic Web Interface Using Patterns. The demo will present use cases of this system. 1111111\n",
      "  2222222\n",
      "Our purpose is to provide end-users a means to query ontology based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. Our contribution is materialized in a system named SWIP, standing for Semantic Web Interface Using Patterns. The demo will present use cases of this system. 1111111\n",
      "  2222222\n",
      "The modelling of realistic emotional behaviour is needed for various applications in multimodal human-machine interaction such as the design of emotional conversational agents (Martin et al., 2005) or of emotional detection systems (Devillers and Vidrascu, 2007). Yet, building such models requires appropriate definition of various levels for representing the emotions themselves but also some contextual information such as the events that elicit these emotions. This paper presents a coding scheme that has been defined following annotations of a corpus of TV interviews (EmoTV). Deciding which events triggered or may trigger which emotion is a challenge for building efficient emotion eliciting protocols. In this paper, we present the protocol that we defined for collecting another corpus of spontaneous human-human interactions recorded in laboratory conditions (EmoTaboo). We discuss the events that we designed for eliciting emotions. Part of this scheme for coding emotional event is being included in the specifications that are currently defined by a working group of the W3C (the W3C Emotion Incubator Working group). This group is investigating the feasibility of working towards a standard representation of emotions and related states in technological contexts. 1111111\n",
      "This paper reports on the creation of the multimodal NIMITEK corpus of affected behavior in human-machine interaction and its role in the development of the NIMITEK prototype system. The NIMITEK prototype system is a spoken dialogue system for supporting users while they solve problems in a graphics system. The central feature of the system is adaptive dialogue management. The system dynamically defines a dialogue strategy according to the current state of the interaction (including also the emotional state of the user). Particular emphasis is devoted to the level of naturalness of interaction. We discuss that a higher level of naturalness can be achieved by combining a habitable natural language interface and an appropriate dialogue strategy. The role of the NIMITEK multimodal corpus in achieving these requirements is twofold: (1) in developing the model of attentional state on the level of user s commands that facilitates processing of flexibly formulated commands, and (2) in defining the dialogue strategy that takes the emotional state of the user into account. Finally, we sketch the implemented prototype system and describe the incorporated dialogue management module. Whereas the prototype system itself is task-specific, the described underlying concepts are intended to be task-independent. 2222222\n",
      "This paper describes Babylon, a system that attempts to overcome the shortage of parallel texts in low-density languages by supplementing existing parallel texts with texts gathered automatically from the Web. In addition to the identification of entire Web pages, we also propose a new feature specifically designed to find parallel text chunks within a single document. Experiments carried out on the Quechua-Spanish language pair show that the system is successful in automatically identifying a significant amount of parallel texts on the Web. Evaluations of a machine translation system trained on this corpus indicate that the Web-gathered parallel texts can supplement manually compiled parallel texts and perform significantly better than the manually compiled texts when tested on other Web-gathered data. 1111111\n",
      "  2222222\n",
      "This paper describes Babylon, a system that attempts to overcome the shortage of parallel texts in low-density languages by supplementing existing parallel texts with texts gathered automatically from the Web. In addition to the identification of entire Web pages, we also propose a new feature specifically designed to find parallel text chunks within a single document. Experiments carried out on the Quechua-Spanish language pair show that the system is successful in automatically identifying a significant amount of parallel texts on the Web. Evaluations of a machine translation system trained on this corpus indicate that the Web-gathered parallel texts can supplement manually compiled parallel texts and perform significantly better than the manually compiled texts when tested on other Web-gathered data. 1111111\n",
      "Concepts and relations in ontologies and in other knowledge organisation systems are usually annotated with natural language labels. Most ontology matchers rely on such labels in element-level matching techniques. State-of-the-art approaches, however, tend to make implicit assumptions about the language used in labels (usually English) and are either domain-agnostic or are built for a specific domain. When faced with labels in different languages, most approaches resort to general-purpose machine translation services to reduce the problem to monolingual English-only matching. We investigate a thoroughly different and highly extensible solution based on semantic matching where labels are parsed by multilingual natural language processing and then matched using language-independent and domain aware background knowledge acting as an interlingua. The method is implemented in NuSM, the language and domain aware evolution of the SMATCH semantic matcher, and is evaluated against a translation-based approach. We also design and evaluate a fusion matcher that combines the outputs of the two techniques in order to boost precision or recall beyond the results produced by either technique alone. 2222222\n",
      "This paper describes Babylon, a system that attempts to overcome the shortage of parallel texts in low-density languages by supplementing existing parallel texts with texts gathered automatically from the Web. In addition to the identification of entire Web pages, we also propose a new feature specifically designed to find parallel text chunks within a single document. Experiments carried out on the Quechua-Spanish language pair show that the system is successful in automatically identifying a significant amount of parallel texts on the Web. Evaluations of a machine translation system trained on this corpus indicate that the Web-gathered parallel texts can supplement manually compiled parallel texts and perform significantly better than the manually compiled texts when tested on other Web-gathered data. 1111111\n",
      "The Spoken Language Communication and Translation System for Tactical Use (TRANSTAC) program is a Defense Advanced Research Agency (DARPA) program to create bidirectional speech-to-speech machine translation (MT) that will allow U.S. Soldiers and Marines, speaking only English, to communicate, in tactical situations, with civilian populations who speak only other languages (for example, Iraqi Arabic). A key metric for the program is the odds of successfully transferring low-level concepts, defined as the source-language content words. The National Institute of Standards and Technology (NIST) has now carried out two large-scale evaluations of TRANSTAC systems, using that metric. In this paper we discuss the merits of that metric. It has proven to be quite informative. We describe exactly how we defined this metric and how we obtained values for it from panels of bilingual judges allowing others to do what we have done. We compare results on this metric to results on Likert-type judgments of semantic adequacy, from the same panels of bilingual judges, as well as to a suite of typical automated MT metrics (BLEU, TER, METEOR). 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary challenge of machine perception is to define efficient computational methods to derive high-level knowledge from low-level sensor observation data. Emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing and perception, which enable advanced integration and interpretation of heterogeneous sensor data. The computational complexity of OWL, however, seriously limits its applicability and use within resource-constrained environments, such as mobile devices. To overcome this issue, we employ OWL to formally define the inference tasks needed for machine perception - explanation and discrimination - and then provide efficient algorithms for these tasks, using bit-vector encodings and operations. The applicability of our approach to machine perception is evaluated on a smart-phone mobile device, demonstrating dramatic improvements in both efficiency and scale. 1111111\n",
      "  2222222\n",
      "The primary challenge of machine perception is to define efficient computational methods to derive high-level knowledge from low-level sensor observation data. Emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing and perception, which enable advanced integration and interpretation of heterogeneous sensor data. The computational complexity of OWL, however, seriously limits its applicability and use within resource-constrained environments, such as mobile devices. To overcome this issue, we employ OWL to formally define the inference tasks needed for machine perception - explanation and discrimination - and then provide efficient algorithms for these tasks, using bit-vector encodings and operations. The applicability of our approach to machine perception is evaluated on a smart-phone mobile device, demonstrating dramatic improvements in both efficiency and scale. 1111111\n",
      "  2222222\n",
      "The primary challenge of machine perception is to define efficient computational methods to derive high-level knowledge from low-level sensor observation data. Emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing and perception, which enable advanced integration and interpretation of heterogeneous sensor data. The computational complexity of OWL, however, seriously limits its applicability and use within resource-constrained environments, such as mobile devices. To overcome this issue, we employ OWL to formally define the inference tasks needed for machine perception - explanation and discrimination - and then provide efficient algorithms for these tasks, using bit-vector encodings and operations. The applicability of our approach to machine perception is evaluated on a smart-phone mobile device, demonstrating dramatic improvements in both efficiency and scale. 1111111\n",
      "  2222222\n",
      "This paper describes a database of 11 dependency treebanks which were unified by means of a two-dimensional graph format. The format was evaluated with respect to storage-complexity on the one hand, and efficiency of data access on the other hand. An example of how the treebanks can be integrated within a unique interface is given by means of the DTDB interface. 1111111\n",
      "  2222222\n",
      "This paper describes a database of 11 dependency treebanks which were unified by means of a two-dimensional graph format. The format was evaluated with respect to storage-complexity on the one hand, and efficiency of data access on the other hand. An example of how the treebanks can be integrated within a unique interface is given by means of the DTDB interface. 1111111\n",
      "We describe a mapping language for converting data contained in spreadsheets into the Web Ontology Language (OWL). The developed language, called M2, overcomes shortcomings with existing mapping techniques, including their restriction to well-formed spreadsheets reminiscent of a single relational database table and verbose syntax for expressing mapping rules when transforming spreadsheet contents into OWL. The M2 language provides expressive, yet concise mechanisms to create both individual and class axioms when generating OWL ontologies. We additionally present an implementation of the mapping approach, Mapping Master, which is available as a plug-in for the Protege ontology editor. 2222222\n",
      "In the semantic web environment, where two or more independent ontologies can be used in order to describe knowledge and data, ontologies have to be aligned by defining mappings among the elements of one ontology and the elements of another ontology. Very often, mappings are not derived by the semantics of the ontologies that are compared, but, rather, by an evaluation of the similarity of the terminology used in the two ontologies or of their syntactic structure. Moreover, ontology mappings can be inaccurate, because ontology matching tools derive such mappings from inaccurate terminology or even because they are not specifically tailored for the domain at hand. In this paper, we propose a new mapping validation approach for interpreting similarity-based mappings as semantic relations, by coping also with inaccuracy situations. The idea is to see two independent ontologies as a unique distributed knowledge base and to assume a semantic interpretation of ontology mappings as probabilistic and hypothetical relations among ontology elements. We present and use a probabilistic reasoning tool in order to validate mappings and to possibly infer new relations among the ontologies. 1111111\n",
      "  2222222\n",
      "In the semantic web environment, where two or more independent ontologies can be used in order to describe knowledge and data, ontologies have to be aligned by defining mappings among the elements of one ontology and the elements of another ontology. Very often, mappings are not derived by the semantics of the ontologies that are compared, but, rather, by an evaluation of the similarity of the terminology used in the two ontologies or of their syntactic structure. Moreover, ontology mappings can be inaccurate, because ontology matching tools derive such mappings from inaccurate terminology or even because they are not specifically tailored for the domain at hand. In this paper, we propose a new mapping validation approach for interpreting similarity-based mappings as semantic relations, by coping also with inaccuracy situations. The idea is to see two independent ontologies as a unique distributed knowledge base and to assume a semantic interpretation of ontology mappings as probabilistic and hypothetical relations among ontology elements. We present and use a probabilistic reasoning tool in order to validate mappings and to possibly infer new relations among the ontologies. 1111111\n",
      "In this paper, we will show how we use ontologies to bootstrap a knowledge acquisition process that extracts product knowledge from tabular data on Web pages. Furthermore, we use logical rules to reason about product specific properties and to derive higher-order knowledge about product features. We will also explain the knowledge acquisition process, covering both ontological and procedural aspects. Finally, we will give an qualitative and quantitative evaluation of our results. 2222222\n",
      "In the semantic web environment, where two or more independent ontologies can be used in order to describe knowledge and data, ontologies have to be aligned by defining mappings among the elements of one ontology and the elements of another ontology. Very often, mappings are not derived by the semantics of the ontologies that are compared, but, rather, by an evaluation of the similarity of the terminology used in the two ontologies or of their syntactic structure. Moreover, ontology mappings can be inaccurate, because ontology matching tools derive such mappings from inaccurate terminology or even because they are not specifically tailored for the domain at hand. In this paper, we propose a new mapping validation approach for interpreting similarity-based mappings as semantic relations, by coping also with inaccuracy situations. The idea is to see two independent ontologies as a unique distributed knowledge base and to assume a semantic interpretation of ontology mappings as probabilistic and hypothetical relations among ontology elements. We present and use a probabilistic reasoning tool in order to validate mappings and to possibly infer new relations among the ontologies. 1111111\n",
      "  2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing opinion retrieval techniques do not provide context-dependent relevant results. Most of the approaches used by state-of-the-art techniques are based on frequency of query terms, such that all documents containing query terms are retrieved, regardless of contextual relevance to the intent of the human seeking the opinion. However, in a particular opinionated document, words could occur in different contexts, yet meet the frequency attached to a certain opinion threshold, thus explicitly creating a bias in overall opinion retrieved. In this paper we propose a sentence-level contextual model for opinion retrieval using grammatical tree derivations and approval voting mechanism. Model evaluation performed between our contextual model, BM25, and language model shows that the model can be effective for contextual opinion retrieval such as faceted opinion retrieval. 1111111\n",
      "  2222222\n",
      "Existing opinion retrieval techniques do not provide context-dependent relevant results. Most of the approaches used by state-of-the-art techniques are based on frequency of query terms, such that all documents containing query terms are retrieved, regardless of contextual relevance to the intent of the human seeking the opinion. However, in a particular opinionated document, words could occur in different contexts, yet meet the frequency attached to a certain opinion threshold, thus explicitly creating a bias in overall opinion retrieved. In this paper we propose a sentence-level contextual model for opinion retrieval using grammatical tree derivations and approval voting mechanism. Model evaluation performed between our contextual model, BM25, and language model shows that the model can be effective for contextual opinion retrieval such as faceted opinion retrieval. 1111111\n",
      "In this paper, we propose a novel unsupervised approach to query segmentation, which is an important task in Web search. We use a generative query model to recover a query’s underlying concepts that compose its original segmented form. The model’s parameters are estimated using an expectation-maximization (EM) algorithm, optimizing the minimum description length objective function on a partial corpus that is specific to the query. To augment this unsupervised learning, we incorporate evidence from Wikipedia. Experiments show that our approach dramatically improves performance over the traditional approach based on mutual information, and produces comparable results with a supervised method. In particular, the basic generative language model contributes a 7.4% improvement over the mutual information based method (measured by segment F1 on the Intersection test set). EM optimization further improves the performance by 14.3%. Additional knowledge from Wikipedia provides another improvement of 24.3%, adding up to a total of 46% improvement (from 0.530 to 0.774). 2222222\n",
      "Existing opinion retrieval techniques do not provide context-dependent relevant results. Most of the approaches used by state-of-the-art techniques are based on frequency of query terms, such that all documents containing query terms are retrieved, regardless of contextual relevance to the intent of the human seeking the opinion. However, in a particular opinionated document, words could occur in different contexts, yet meet the frequency attached to a certain opinion threshold, thus explicitly creating a bias in overall opinion retrieved. In this paper we propose a sentence-level contextual model for opinion retrieval using grammatical tree derivations and approval voting mechanism. Model evaluation performed between our contextual model, BM25, and language model shows that the model can be effective for contextual opinion retrieval such as faceted opinion retrieval. 1111111\n",
      "Discovering users' specific and implicit geographic intention in web search can greatly help satisfy users' information needs. We build a geo intent analysis system that uses minimal supervision to learn a model from large amounts of web-search logs for this discovery. We build a city language model, which is a probabilistic representation of the language surrounding the mention of a city in web queries. We use several features derived from these language models to: (1) identify users' implicit geo intent and pinpoint the city corresponding to this intent, (2) determine whether the geo-intent is localized around the users' current geographic location, (3) predict cities for queries that have a mention of an entity that is located in a specific place. Experimental results demonstrate the effectiveness of using features derived from the city language model. We find that (1) the system has over 90% precision and more than 74% accuracy for the task of detecting users' implicit city level geo intent (2) the system achieves more than 96% accuracy in determining whether implicit geo queries are local geo queries, neighbor region geo queries or none-of these (3) the city language model can effectively retrieve cities in locationspecific queries with high precision (88%) and recall (74%); human evaluation shows that the language model predicts city labels for location-specific queries with high accuracy (84.5%). 2222222\n",
      "Identity relations are at the foundation of the Semantic Web and the Linked Data initiative.  In many instances the classical interpretation of identity is too strong for practical purposes.  This is particularly the case when two entities are considered the same in some but not all contexts.  Unfortunately, modeling the specific contexts in which an identity relation holds is cumbersome and, due to arbitrary reuse and the Open World Assumption, it is impossible to anticipate all contexts in which an entity will be used.  We propose an alternative semantics for owl:sameAs that partitions the original relation into a hierarchy of subrelations.  The subrelation to which an identity statement belongs depends on the dataset in which the statement occurs.  Adding future assertions may change the subrelation to which an identity statement belongs, resulting in a context-dependent and non-monotone semantics.  We show that this more complicated semantics is better able to characterize the pragmatic use of owl:sameAs as observed in Linked Open Datasets. 1111111\n",
      "The potential of the semantic data available in the Web is enormous but in most cases it is very difficult for users to explore and use this data. Applying information visualization techniques to the Semantic Web helps users to easily explore large amounts of data and interact with them. We devise a formal Linked Data Visualization model (LDVM), which allows to dynamically connect data with visualizations. 2222222\n",
      "Identity relations are at the foundation of the Semantic Web and the Linked Data initiative.  In many instances the classical interpretation of identity is too strong for practical purposes.  This is particularly the case when two entities are considered the same in some but not all contexts.  Unfortunately, modeling the specific contexts in which an identity relation holds is cumbersome and, due to arbitrary reuse and the Open World Assumption, it is impossible to anticipate all contexts in which an entity will be used.  We propose an alternative semantics for owl:sameAs that partitions the original relation into a hierarchy of subrelations.  The subrelation to which an identity statement belongs depends on the dataset in which the statement occurs.  Adding future assertions may change the subrelation to which an identity statement belongs, resulting in a context-dependent and non-monotone semantics.  We show that this more complicated semantics is better able to characterize the pragmatic use of owl:sameAs as observed in Linked Open Datasets. 1111111\n",
      "Entities have been deserved special attention in the latest years; however their identification is still troublesome. Existing approaches exploit ad hoc services or centralized architectures. In this paper we present a novel distributed approach natively built on top of Linked Data concepts and protocols. 2222222\n",
      "Identity relations are at the foundation of the Semantic Web and the Linked Data initiative.  In many instances the classical interpretation of identity is too strong for practical purposes.  This is particularly the case when two entities are considered the same in some but not all contexts.  Unfortunately, modeling the specific contexts in which an identity relation holds is cumbersome and, due to arbitrary reuse and the Open World Assumption, it is impossible to anticipate all contexts in which an entity will be used.  We propose an alternative semantics for owl:sameAs that partitions the original relation into a hierarchy of subrelations.  The subrelation to which an identity statement belongs depends on the dataset in which the statement occurs.  Adding future assertions may change the subrelation to which an identity statement belongs, resulting in a context-dependent and non-monotone semantics.  We show that this more complicated semantics is better able to characterize the pragmatic use of owl:sameAs as observed in Linked Open Datasets. 1111111\n",
      "The Music Encoding Initiative (MEI) XML schema expresses musical structure addressing score elements at musically meaningful levels of granularity (e.g., individual systems, measures, or notes). While this provides a comprehensive representation of music content, only concepts and relationships provided by the MEI schema can be encoded. Here, we present our Music Encoding and Linked Data (MELD) framework which applies RDF Web Annotations to targetted portions of the MEI structure. Concepts and relationships from the Semantic Web can be included alongside MEI in an expanded musical knowledge graph. We have implemented a music performance scenario which collects, distributes, and displays semantic annotations, enhancing a digital musical score used by performers in a live music jam session. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms. First, the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks. Second, automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts, which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms. The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks. Finally, a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list. Our experiments on a large-scale database of web pages have provided very positive results. 1111111\n",
      "Comparing and contrasting is an important strategy people adopt to understand new situations and create solutions to for new problems. Similar events can provide hints for problem solving, as well as larger contexts for understanding the specific circumstances of an event. Lessons from past experience can be applied, insights can be gained about the new situation from familiar examples, and trends can be discovered among similar events. As the largest knowledge base for human beings, the Web provides both an opportunity and a challenge to discover comparable cases in order to facilitate situation analysis and problem solving. In this paper, we present Compare&Contrast, a system that uses the Web to discover comparable cases for news stories, documents about similar situations but involving distinct entities. The system analyzes a news story given by the user and builds a model of the situation. With the situation model, the system dynamically discovers entities comparable to the main entity in the original story and uses these comparable entities as seeds to retrieve web pages about comparable cases. The system is domain independent, does not require any knowledge engineering efforts, and deals with the complexity of unstructured text and noise on the web in a robust way. We evaluated the system with both a systematic experiment on a collection of news articles and a user study. 2222222\n",
      "This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms. First, the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks. Second, automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts, which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms. The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks. Finally, a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list. Our experiments on a large-scale database of web pages have provided very positive results. 1111111\n",
      "  2222222\n",
      "This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms. First, the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks. Second, automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts, which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms. The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks. Finally, a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list. Our experiments on a large-scale database of web pages have provided very positive results. 1111111\n",
      "An approach to improve an RCC-derived geospatial approximation is presented which makes use of OWL class inclusion axioms. The algorithm used to control the approximation combines hypothesis testing with consistency checking provided by a knowledge representation system based on description logics. Propositions about the consistency of the refined ABox w.r.t. the associated TBox when compared to baseline ABox and TBox are made. Formal proves of the divergent consistency results when checking either of both are provided. The application of the approach to a geospatial setting results in a roughly tenfold improved approximation when using the refined ABox and TBox. Ways to further improve the approximation and to automate the detection of falsely calculated relations are discussed. 2222222\n",
      "This paper describes the development of a written corpus of argumentative reasoning. Arguments in the corpus have been analysed using state of the art techniques from argumentation theory and have been marked up using an open, reusable markup language. A number of the key challenges enountered during the process are explored, and preliminary observations about features such as inter-coder reliability and corpus statistics are discussed. In addition, several examples are offered of how this kind of language resource can be used in linguistic, computational and philosophical research, and in particular, how the corpus has been used to initiate a programme investigating the automatic detection of argumentative structure. 1111111\n",
      "We present a novel merging algorithm for light-weight ontologies using answer set programming and linguistic background knowledge. The semi-automatic method provides a number of solutions for the user to choose from, by straightforwardly applying intuitive merging rules in a declarative programming environment. 2222222\n",
      "This paper describes the development of a written corpus of argumentative reasoning. Arguments in the corpus have been analysed using state of the art techniques from argumentation theory and have been marked up using an open, reusable markup language. A number of the key challenges enountered during the process are explored, and preliminary observations about features such as inter-coder reliability and corpus statistics are discussed. In addition, several examples are offered of how this kind of language resource can be used in linguistic, computational and philosophical research, and in particular, how the corpus has been used to initiate a programme investigating the automatic detection of argumentative structure. 1111111\n",
      "We introduce a technique to determine implicit information in an RDF graph. In addition to taxonomic knowledge about concepts and properties typically expressible in languages such as RDFS and OWL, we focus on knowledge determined by arithmetic equations. The main use case is exploiting knowledge about functional dependencies among numerical properties expressible by means of such arithmetic equations. While some of this knowledge is expressible for instance in rule extensions to  ontology languages, we provide a more flexible framework that treats property equations as first class citizens in the ontology language. The combination of ontological reasoning and property equations is realized by extending query rewriting techniques already successfully applied for ontology languages such as (the DL-fragment of) RDFS, or also OWL QL, respectively. We deploy this technique for rewriting SPARQL queries and discuss the feasibility of alternative implementations, such as rule-based approaches. 2222222\n",
      "This paper describes the development of a written corpus of argumentative reasoning. Arguments in the corpus have been analysed using state of the art techniques from argumentation theory and have been marked up using an open, reusable markup language. A number of the key challenges enountered during the process are explored, and preliminary observations about features such as inter-coder reliability and corpus statistics are discussed. In addition, several examples are offered of how this kind of language resource can be used in linguistic, computational and philosophical research, and in particular, how the corpus has been used to initiate a programme investigating the automatic detection of argumentative structure. 1111111\n",
      "This paper discusses the problem of utilising multiply annotated data in training biomedical information extraction systems. Two corpora, annotated with entities and relations, and containing a number of multiply annotated documents, are used to train named entity recognition and relation extraction systems. Several methods of automatically combining the multiple annotations to produce a single annotation are compared, but none produces better results than simply picking one of the annotated versions at random. It is also shown that adding extra singly annotated documents produces faster performance gains than adding extra multiply annotated documents. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The huge amount of the available information in the Web creates the need of effective information extraction systems that are able to produce metadata that satisfy user s information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn extraction models. The production of such corpora can be significantly facilitated by annotation tools that are able to annotate, according to a defined ontology, not only named entities but most importantly relations between them. This paper describes the BOEMIE ontology-based annotation tool which is able to locate blocks of text that correspond to specific types of named entities, fill tables corresponding to ontology concepts with those named entities and link the filled tables based on relations defined in the domain ontology. Additionally, it can perform annotation of blocks of text that refer to the same topic. The tool has a user-friendly interface, supports automatic pre-annotation, annotation comparison as well as customization to other annotation schemata. The annotation tool has been used in a large scale annotation task involving 3,000 web pages regarding athletics. It has also been used in another annotation task involving 503 web pages with medical information, in different languages. 1111111\n",
      "As data collections become established in key disciplines, some of the longstanding barriers to data sharing become to dissolve; yet others remain. While metadata and ontologies help overcome the problems of finding and interpreting data, the lack of clarity over licensing remains a real impediment to data reuse. Freedom from legal restriction and uncertainty is essential for the effective sharing, combining and deriving of data from these distributed collections. Reuse and recombination of data will be greatly facilitated by expanding the definition of the semantic web to include the semantics of data licensing. We aim to express licensing terms in a computable manner, within the context of research practice, enabling us to infer the resulting state of rights, obligations and conditions that are inherited by derived and recombined datasets, using a mixed bag of licenses. Building off this we aim to simulate the effects of varying licensing practices within communities, proposing a measure of health of our scholarly record based on compatibility and restrictiveness of the licenses contained therein. 2222222\n",
      "The huge amount of the available information in the Web creates the need of effective information extraction systems that are able to produce metadata that satisfy user s information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn extraction models. The production of such corpora can be significantly facilitated by annotation tools that are able to annotate, according to a defined ontology, not only named entities but most importantly relations between them. This paper describes the BOEMIE ontology-based annotation tool which is able to locate blocks of text that correspond to specific types of named entities, fill tables corresponding to ontology concepts with those named entities and link the filled tables based on relations defined in the domain ontology. Additionally, it can perform annotation of blocks of text that refer to the same topic. The tool has a user-friendly interface, supports automatic pre-annotation, annotation comparison as well as customization to other annotation schemata. The annotation tool has been used in a large scale annotation task involving 3,000 web pages regarding athletics. It has also been used in another annotation task involving 503 web pages with medical information, in different languages. 1111111\n",
      "Recently more and more structured data in form of RDF triples have been published and integrated into Linked Open Data (LOD). While the current LOD contains hundreds of data sources with billions of triples, it has a small number of distinct relations compared with the large number of entities. On the other hand, Web pages are growing rapidly, which results in much larger number of textual contents to be exploited. With the popularity and wide adoption of open information extraction technology, extracting entities and relations among them from text at the Web scale is possible. In this paper, we present an approach to extract the subject individuals and the object counterparts for the relations from text and determine the most appropriate domain and range as well as the most confident dependency path patterns for the given relation based on the EM algorithm. As a preliminary results, we built a knowledge base for relations extracted from Chinese encyclopedias. The experimental results show the effectiveness of our approach to extract relations with reasonable domain, range and path pattern restrictions as well as high-quality triples. 2222222\n",
      "The huge amount of the available information in the Web creates the need of effective information extraction systems that are able to produce metadata that satisfy user s information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn extraction models. The production of such corpora can be significantly facilitated by annotation tools that are able to annotate, according to a defined ontology, not only named entities but most importantly relations between them. This paper describes the BOEMIE ontology-based annotation tool which is able to locate blocks of text that correspond to specific types of named entities, fill tables corresponding to ontology concepts with those named entities and link the filled tables based on relations defined in the domain ontology. Additionally, it can perform annotation of blocks of text that refer to the same topic. The tool has a user-friendly interface, supports automatic pre-annotation, annotation comparison as well as customization to other annotation schemata. The annotation tool has been used in a large scale annotation task involving 3,000 web pages regarding athletics. It has also been used in another annotation task involving 503 web pages with medical information, in different languages. 1111111\n",
      "  2222222\n",
      "The paper gives a comprehensive overview over the results, the concepts and the methods which were developed and used to create the Pronouncing Dictionary of Austrian German (ÖAWB) and the Austrian Pronouncing Database ADABA. The ÖAWB contains 42,000 entries which are based on a large audio corpus of 75,964 realisations of two model speakers each from Austria, Germany and Switzerland. The ADABA database provides 9 different ways to search the data. It also contains 24 model texts and another 30 texts showing linguistic and phonetic variation in Austria and in the other German speaking countries. The codification of Austrian standard pronunciation was based on the concept of German as a pluricentric language and on the concept of  media presentation language . Austrian pronunciation forms are presented in parallel with those of Germany and Switzerland to allow the comparison of differences between linguistically close national varieties of a language. The paper also gives a detailed characterisation of the software (transcriber, database) which was developed during the project that was supported by the Austrian national broadcasting corporation ORF and the University for Music and Dramatic Arts in Graz. Some of the software and the data can be obtained from the web site www.adaba.at. 1111111\n",
      "The first release of the German Ph@ttSessionz speech database contains read and spontaneous speech from 864 adolescent speakers and is the largest database of its kind for German. It was recorded via the WWW in over 40 public schools in all dialect regions of Germany. In this paper, we present a cross-sectional study of f0 measurements on this database. The study documents the profound changes in male voices at the age 13-15. Furthermore, it shows that on a perceptive mel-scale, there is little difference in the relative f0 variability for male and female speakers. A closer analysis reveals that f0 variability is dependent on the speech style and both the length and the type of the utterance. The study provides statistically reliable voice parameters of adolescent speakers for German. The results may contribute to making spoken dialog systems more robust by restricting user input to utterances with low f0 variability. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper gives a comprehensive overview over the results, the concepts and the methods which were developed and used to create the Pronouncing Dictionary of Austrian German (ÖAWB) and the Austrian Pronouncing Database ADABA. The ÖAWB contains 42,000 entries which are based on a large audio corpus of 75,964 realisations of two model speakers each from Austria, Germany and Switzerland. The ADABA database provides 9 different ways to search the data. It also contains 24 model texts and another 30 texts showing linguistic and phonetic variation in Austria and in the other German speaking countries. The codification of Austrian standard pronunciation was based on the concept of German as a pluricentric language and on the concept of  media presentation language . Austrian pronunciation forms are presented in parallel with those of Germany and Switzerland to allow the comparison of differences between linguistically close national varieties of a language. The paper also gives a detailed characterisation of the software (transcriber, database) which was developed during the project that was supported by the Austrian national broadcasting corporation ORF and the University for Music and Dramatic Arts in Graz. Some of the software and the data can be obtained from the web site www.adaba.at. 1111111\n",
      "Producing a robust and comprehensive representation of the research topics covered by a scientific publication is a crucial task that has a major impact on its retrievability and consequently on the diffusion of the relevant scientific ideas. Springer Nature, the world's largest academic book publisher, has typically entrusted this task to the most expert editors, which had to manually analyse new books and produce a list of the most relevant topics. To support Springer Nature in this task, we developed Smart Topic Miner, an application which assists the editorial team in annotating proceedings books according to a large-scale ontology of research areas. Over the past three years we evolved this application according to the editors feedback and developed a new engine, a new interface, and several other functionalities. In this demo paper, we present Smart Topic Miner 2, the most recent version of the tool, which is being regularly utilized by editors in Germany, China, Brazil, and Japan to annotate all book series covering conference proceedings in Computer Science, for a total of about 800 volumes per year. 2222222\n",
      "The paper gives a comprehensive overview over the results, the concepts and the methods which were developed and used to create the Pronouncing Dictionary of Austrian German (ÖAWB) and the Austrian Pronouncing Database ADABA. The ÖAWB contains 42,000 entries which are based on a large audio corpus of 75,964 realisations of two model speakers each from Austria, Germany and Switzerland. The ADABA database provides 9 different ways to search the data. It also contains 24 model texts and another 30 texts showing linguistic and phonetic variation in Austria and in the other German speaking countries. The codification of Austrian standard pronunciation was based on the concept of German as a pluricentric language and on the concept of  media presentation language . Austrian pronunciation forms are presented in parallel with those of Germany and Switzerland to allow the comparison of differences between linguistically close national varieties of a language. The paper also gives a detailed characterisation of the software (transcriber, database) which was developed during the project that was supported by the Austrian national broadcasting corporation ORF and the University for Music and Dramatic Arts in Graz. Some of the software and the data can be obtained from the web site www.adaba.at. 1111111\n",
      "The publishing industry is undergoing major changes. ↵These changes are mainly based on technical developments and related habits of information consumption. ↵Wolters Kluwer already engaged in new solutions to meet these challenges and to improve all processes of generating good quality content in the backend on the one hand and to deliver information and software in the frontend that facilitates the customer's life on the other hand.↵JURION is an innovative legal information platform developed by Wolters Kluwer Germany (WKD) that merges and interlinks over one million documents of content and data from diverse sources such as national and European legislation and court judgments, extensive internally authored content and local customer data, as well as social media and web data (e.g. DBpedia). ↵In collecting and managing this data, all stages of the Data Lifecycle are present – extraction, storage, authoring, interlinking, enrichment, quality analysis, repair and publication. ↵Ensuring data quality is a key step in the JURION data lifecycle.↵In this industry paper we present two use cases for verifying quality: ↵1) integrating quality tools in the existing software infrastructure and ↵2) improving the data enrichment step by checking the external sources before importing them in JURION.↵We open-source part of our extensions and provide a screencast with our prototype in action. 2222222\n",
      "Opinionated social media like product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or demote some target products. For reviews to reflect genuine user experiences and opinions, such spam reviews should be detected. Prior works on opinion spam focused on detecting fake reviews and individual spammers. However, a spammer group (a group of reviewers who work collaboratively to write fake reviews) is even more damaging and can take total control of the sentiment on the target product due to its size. This paper studies spam detection in the collaborative setting, i.e., to discover review spammer groups. The proposed method first uses a frequent itemset mining method to find a set of candidate groups. It then uses several behavioral models derived from the collusion phenomenon among spammers and relation models based on inter-relationships among products, groups, and individual reviewers to detect spammer groups. Additionally, we also built a labeled group spam dataset. To the best of our knowledge, this is the first labeled dataset on group opinion spam. Experimental results show that the proposed method is highly effective and it outperforms multiple strong baselines including both the state-of-the-art learning to rank and supervised classification algorithms. 1111111\n",
      "In this demo paper, we present the first ontology-based Vietnamese question answeringsystem KbQAS in which a knowledge acquisition approach for question analysis is integrated. 2222222\n",
      "Opinionated social media like product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or demote some target products. For reviews to reflect genuine user experiences and opinions, such spam reviews should be detected. Prior works on opinion spam focused on detecting fake reviews and individual spammers. However, a spammer group (a group of reviewers who work collaboratively to write fake reviews) is even more damaging and can take total control of the sentiment on the target product due to its size. This paper studies spam detection in the collaborative setting, i.e., to discover review spammer groups. The proposed method first uses a frequent itemset mining method to find a set of candidate groups. It then uses several behavioral models derived from the collusion phenomenon among spammers and relation models based on inter-relationships among products, groups, and individual reviewers to detect spammer groups. Additionally, we also built a labeled group spam dataset. To the best of our knowledge, this is the first labeled dataset on group opinion spam. Experimental results show that the proposed method is highly effective and it outperforms multiple strong baselines including both the state-of-the-art learning to rank and supervised classification algorithms. 1111111\n",
      "  2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a challenging task to match similar or related terms/expressions in NLP and Text Mining applications. Two typical areas in need for such work are terminology and ontology constructions, where terms and concepts are extracted and organized into certain structures with various semantic relations. In the EU BOOTSTrep Project we test various techniques for matching terms that can assist human domain experts in building and enriching ontologies. This paper reports on a work in which we evaluated a text comparing and clustering tool for this task. Particularly, we explore the feasibility of matching related terms with their definitions. Ontology terms, such as Gene Ontology terms, are often assigned with detailed definitions, which provide a fundamental information source for detecting relations between terms. Here we focus on the exploitation of term definitions for the term matching task. Our experiment shows that the tool is capable of grouping many related terms using their definitions. 1111111\n",
      "We explore a diagrammatic logic suitable for specifying ontologies using a case study. Diagrammatic reasoning is used to establish consequences of the ontology. 2222222\n",
      "It is a challenging task to match similar or related terms/expressions in NLP and Text Mining applications. Two typical areas in need for such work are terminology and ontology constructions, where terms and concepts are extracted and organized into certain structures with various semantic relations. In the EU BOOTSTrep Project we test various techniques for matching terms that can assist human domain experts in building and enriching ontologies. This paper reports on a work in which we evaluated a text comparing and clustering tool for this task. Particularly, we explore the feasibility of matching related terms with their definitions. Ontology terms, such as Gene Ontology terms, are often assigned with detailed definitions, which provide a fundamental information source for detecting relations between terms. Here we focus on the exploitation of term definitions for the term matching task. Our experiment shows that the tool is capable of grouping many related terms using their definitions. 1111111\n",
      "We describe an on-line environment in which the ontology development process can be performed collaboratively in a Wiki-like fashion. To start the construction (or the extension) of an ontology, the user can exploit a domain corpus, from which the terminological component of the system automatically extracts a set of domain-specific key-concepts. These key-concepts are further disambiguated in order to be linked to existing external resources and obtain additional information such as the concept definition, the synonyms and the hypernyms. Finally, the user can easily select through the interface which concepts should be imported into the ontology. The system support several ontology engineering tasks, including (i) boosting the construction or extension of ontologies, (ii) terminologically evaluating and ranking ontologies, and (iii) ranking the concepts defined in an ontology according to their relevance with respect to the domain described by the corpus. 2222222\n",
      "It is a challenging task to match similar or related terms/expressions in NLP and Text Mining applications. Two typical areas in need for such work are terminology and ontology constructions, where terms and concepts are extracted and organized into certain structures with various semantic relations. In the EU BOOTSTrep Project we test various techniques for matching terms that can assist human domain experts in building and enriching ontologies. This paper reports on a work in which we evaluated a text comparing and clustering tool for this task. Particularly, we explore the feasibility of matching related terms with their definitions. Ontology terms, such as Gene Ontology terms, are often assigned with detailed definitions, which provide a fundamental information source for detecting relations between terms. Here we focus on the exploitation of term definitions for the term matching task. Our experiment shows that the tool is capable of grouping many related terms using their definitions. 1111111\n",
      "We propose an automatic system for annotating accurately data tables extracted from the web. This system is designed to provide additional data to an existing querying system called MIEL, which relies on a common vocabulary used to query local relational databases. We will use the same vocabulary, translated into an OWL ontology, to annotate the tables. Our annotation system is unsupervised. It uses only the knowledge defined in the ontology to automatically annotate the entire content of tables, using an aggregation approach: first annotate cells, then columns, then relations between those columns. The annotations are fuzzy: instead of linking an element of the table with a precise concept of the ontology, the elements of the table are annotated with several concepts, associated with their relevance degree. Our annotation process has been validated experimentally on scientific domains (microbial risk in food, chemical risk in food) and a technical domain (aeronautics). 2222222\n",
      "With the proliferation of knowledge intensive applications, there is a vivid research in the domain of knowledge representation. Description Logics are designed to be a convenient means for such representation task. One of their main advantages over other formalisms is a clearly defined semantics which opens the possibility to provide reasoning services with mathematical rigorousness.  My PhD work is concerned with Description Logic reasoning. I am particularly interested in ABox reasoning when the available data is really large. This domain is much less explored than TBox reasoning. Nevertheless, reasoning over large ABoxes is useful for problems like web-based reasoning.  I am one of the developers of the DLog data reasoner which implements a two phase reasoning: the first phase uses complex reasoning to turn the TBox into a set of simple rules, while the second phase is geared towards very fast query answering over large ABoxes. DLog currently supports the SHIQ DL language. We are trying to extend the reasoner to more expressive languages, hopefully until SROIQ, the logic behind OWL 2. 1111111\n",
      "Ontology alignment is the task of matching concepts and terminology from multiple ontologies. Ontology alignment is especially relevant in the semantic web domain as RDF documents and OWL ontologies are quite heterogeneous, yet often describe related concepts. The end goal for ontology matching is to allow the knowledge sets to interoperate. To achieve this goal, it is necessary for queries to return results that include knowledge, and inferred knowledge, from multiple datasets and terminologies, using the alignment information. Furthermore, ontology alignment is not an exact science, and concept matchings often involve uncertainty. The goal of this paper is to provide a semantic web repository that supports applying alignments to the dataset and reasoning with alignments. Our goal is to provide high performance queries that return results that include inference across alignment matchings, and rank results using certainty information. Our semantic web repository uses distributed inference and probabilistic reasoning to allow datasets to be efficiently updated with ontology alignments. We materialize the inferred, aligned data and make it available in efficient queries. 2222222\n",
      "With the proliferation of knowledge intensive applications, there is a vivid research in the domain of knowledge representation. Description Logics are designed to be a convenient means for such representation task. One of their main advantages over other formalisms is a clearly defined semantics which opens the possibility to provide reasoning services with mathematical rigorousness.  My PhD work is concerned with Description Logic reasoning. I am particularly interested in ABox reasoning when the available data is really large. This domain is much less explored than TBox reasoning. Nevertheless, reasoning over large ABoxes is useful for problems like web-based reasoning.  I am one of the developers of the DLog data reasoner which implements a two phase reasoning: the first phase uses complex reasoning to turn the TBox into a set of simple rules, while the second phase is geared towards very fast query answering over large ABoxes. DLog currently supports the SHIQ DL language. We are trying to extend the reasoner to more expressive languages, hopefully until SROIQ, the logic behind OWL 2. 1111111\n",
      "This thesis focuses on developing an efficient framework for contextualized knowledge representation on Semantic Web. We bring about the drawbacks of existing context formalisms which hinders an efficient implementation and propose a formalism for contexts that enables for the development of framework with desired properties. Some of the future milestones to be achieved for this thesis work are (i) develop a proof theory for the logical framework based on Description Logics (DL) (ii) developing reasoning algorithms (iii) verify and compare the performance of these algorithms to existing distributed reasoning formalisms and (iv) system implementation 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the proliferation of knowledge intensive applications, there is a vivid research in the domain of knowledge representation. Description Logics are designed to be a convenient means for such representation task. One of their main advantages over other formalisms is a clearly defined semantics which opens the possibility to provide reasoning services with mathematical rigorousness.  My PhD work is concerned with Description Logic reasoning. I am particularly interested in ABox reasoning when the available data is really large. This domain is much less explored than TBox reasoning. Nevertheless, reasoning over large ABoxes is useful for problems like web-based reasoning.  I am one of the developers of the DLog data reasoner which implements a two phase reasoning: the first phase uses complex reasoning to turn the TBox into a set of simple rules, while the second phase is geared towards very fast query answering over large ABoxes. DLog currently supports the SHIQ DL language. We are trying to extend the reasoner to more expressive languages, hopefully until SROIQ, the logic behind OWL 2. 1111111\n",
      "An important open question in the semantic Web is the precise relationship between the RDF(S) semantics and the semantics of standard knowledge representation formalisms such as logic programming and description logics. In this paper we address this issue by considering embeddings of RDF and RDFS in logic. Using these embeddings, combined with existing results about various fragments of logic, we establish several novel complexity results. The embeddings we consider show how techniques from deductive databases and description logics can be used for reasoning with RDF(S). Finally, we consider querying RDF graphs and establish the data complexity of conjunctive querying for the various RDF entailment regimes. 2222222\n",
      "Requirement engineering is emerging as an increasingly important discipline for supporting Web application development, as these are designed to satisfy diverse stakeholder needs, additional functional, information, multimedia and usability requirements as compared to traditional software applications. Moreover, when considering innovative e-commerce applications, value-based requirements engineering is an extremely relevant methodology which exploits the concept of economic value during the requirements engineering activity. In contrast, most of the methodologies proposed for the development of Web applications, primarily focus on the system design, and paying less attention to the requirements engineering, and specifically to value-based requirement engineering. Focusing this aspect, the paper presents integration of value-based requirement engineering models to WebML models using our recently proposed VIP Business Modeling Framework. The integration process is demonstrated using a well-known e-commerce application example by first presenting example VIP business models and then deriving WebML process, structural and other models from these business models. 1111111\n",
      "  2222222\n",
      "Requirement engineering is emerging as an increasingly important discipline for supporting Web application development, as these are designed to satisfy diverse stakeholder needs, additional functional, information, multimedia and usability requirements as compared to traditional software applications. Moreover, when considering innovative e-commerce applications, value-based requirements engineering is an extremely relevant methodology which exploits the concept of economic value during the requirements engineering activity. In contrast, most of the methodologies proposed for the development of Web applications, primarily focus on the system design, and paying less attention to the requirements engineering, and specifically to value-based requirement engineering. Focusing this aspect, the paper presents integration of value-based requirement engineering models to WebML models using our recently proposed VIP Business Modeling Framework. The integration process is demonstrated using a well-known e-commerce application example by first presenting example VIP business models and then deriving WebML process, structural and other models from these business models. 1111111\n",
      "Data integration is complex often requiring much technical knowledge and expert understanding of the data’s meaning. In this paper we investigate the use of current semantic tools as an aid to data integration, and identify the need to modify these tools to meet the needs of spatial data. We create a demonstrator based on the real world problem of predicting sources of diffuse pollution, illustrating the benefits of exposing the semantics of integration. 2222222\n",
      "Requirement engineering is emerging as an increasingly important discipline for supporting Web application development, as these are designed to satisfy diverse stakeholder needs, additional functional, information, multimedia and usability requirements as compared to traditional software applications. Moreover, when considering innovative e-commerce applications, value-based requirements engineering is an extremely relevant methodology which exploits the concept of economic value during the requirements engineering activity. In contrast, most of the methodologies proposed for the development of Web applications, primarily focus on the system design, and paying less attention to the requirements engineering, and specifically to value-based requirement engineering. Focusing this aspect, the paper presents integration of value-based requirement engineering models to WebML models using our recently proposed VIP Business Modeling Framework. The integration process is demonstrated using a well-known e-commerce application example by first presenting example VIP business models and then deriving WebML process, structural and other models from these business models. 1111111\n",
      "This demo presents a web application which implements a pipeline for searching and browsing through newspaper archives. It uses a combination of information extraction, enrichment and visualization algorithms to help the user to grasp large amount of articles normally collected in archives. Illustrative results show appropriateness of the proposed pipeline for searching and brows-ing news archives. 2222222\n",
      "Most of today's web content is designed for human consumption, which makes it difficult for software tools to access them readily. Even web content that is automatically generated from back-end databases is usually presented without the original structural information. In this paper, we present an automated information extraction algorithm that can extract the relevant attribute-value pairs from product descriptions across different sites. A notion, called structural-semantic entropy, is used to locate the data of interest on web pages, which measures the density of occurrence of relevant information on the DOM tree representation of web pages. Our approach is less labor-intensive and insensitive to changes in web-page format. Experimental results on a large number of real-life web page collections are encouraging and confirm the feasibility of the approach, which has been successfully applied to detect false drug advertisements on the Web due to its capacity in associating the attributes of records with their respective values. 1111111\n",
      "  2222222\n",
      "A picture is worth a thousand words, we often say, yet many areas are in demand of sophisticated visualization techniques, and the Semantic Web is not an exception. The size and complexity of ontologies and Linked Data in the Semantic Web constantly grows and the diverse backgrounds of the users and application areas multiply at the same time. Providing users with visual representations and sophisticated interaction techniques can significantly aid the exploration and understanding of the domains and knowledge represented by ontologies and Linked Data. There is no one-size-fits-all solution but different use cases demand different visualization and interaction techniques. Ultimately, providing better user interfaces, visual representations and interaction techniques will foster user engagement and likely lead to higher quality results in different applications employing ontologies and proliferate the consumption of Linked Data. 1111111\n",
      "  2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A picture is worth a thousand words, we often say, yet many areas are in demand of sophisticated visualization techniques, and the Semantic Web is not an exception. The size and complexity of ontologies and Linked Data in the Semantic Web constantly grows and the diverse backgrounds of the users and application areas multiply at the same time. Providing users with visual representations and sophisticated interaction techniques can significantly aid the exploration and understanding of the domains and knowledge represented by ontologies and Linked Data. There is no one-size-fits-all solution but different use cases demand different visualization and interaction techniques. Ultimately, providing better user interfaces, visual representations and interaction techniques will foster user engagement and likely lead to higher quality results in different applications employing ontologies and proliferate the consumption of Linked Data. 1111111\n",
      "This work addresses the problem of underwater archaeological surveys from the point of view of knowledge. We propose an approach based on underwater pho-togrammetry guided by a representation of the used knowledge, structured by on-tologies. Ontologies and photogrammetry are used together from the survey pro-cess to the graphical results. This paper focuses on the use of ontologies during the 3D results exploitation. A JAVA software dedicated to photogrammetry and archaeological survey has been mapped onto an OWL formalism. The use of pro-cedural attachment in a dual representation (JAVA - OWL) of the involved con-cepts allows to access computational facilities directly from OWL. The use of rules as SWRL illustrates very well this double formalism, also the use of com-putational capabilities of rules logical expression. We present an application that is able to read the ontology populated with a photogrammetric survey. Once the ontology is read, it is possible to produce a 3D representation of the individuals and observing graphically the results of logical spatial queries on the ontology. This work is done on a very important archaeological wreck in Malta named Xlendi, probably the most ancient Phoenician wreck of western Mediterranean Sea.  2222222\n",
      "A picture is worth a thousand words, we often say, yet many areas are in demand of sophisticated visualization techniques, and the Semantic Web is not an exception. The size and complexity of ontologies and Linked Data in the Semantic Web constantly grows and the diverse backgrounds of the users and application areas multiply at the same time. Providing users with visual representations and sophisticated interaction techniques can significantly aid the exploration and understanding of the domains and knowledge represented by ontologies and Linked Data. There is no one-size-fits-all solution but different use cases demand different visualization and interaction techniques. Ultimately, providing better user interfaces, visual representations and interaction techniques will foster user engagement and likely lead to higher quality results in different applications employing ontologies and proliferate the consumption of Linked Data. 1111111\n",
      "The potential of the semantic data available in the Web is enormous but in most cases it is very difficult for users to explore and use this data. Applying information visualization techniques to the Semantic Web helps users to easily explore large amounts of data and interact with them. We devise a formal Linked Data Visualization model (LDVM), which allows to dynamically connect data with visualizations. 2222222\n",
      "In order to reduce the cost of building domain ontologies manually, in this paper, we integrate a domain ontology development environment: DODDLE-OWL and an ontology search engine: Swoogle. We propose a method and a tool for domain ontology construction reusing texts and existing ontologies for a target domain extracted by Swoogle. In the evaluation, we applied the method to a particular field of law and evaluated the acquired ontologies. 1111111\n",
      "  2222222\n",
      "In order to reduce the cost of building domain ontologies manually, in this paper, we integrate a domain ontology development environment: DODDLE-OWL and an ontology search engine: Swoogle. We propose a method and a tool for domain ontology construction reusing texts and existing ontologies for a target domain extracted by Swoogle. In the evaluation, we applied the method to a particular field of law and evaluated the acquired ontologies. 1111111\n",
      "We present NoHR, a Protege plug-in that allows the user to take an EL ontology, add a set of non-monotonic (logic programming) rules - suitable e.g. to express defaults and exceptions - and query the combined knowledge base. Provided the given ontology alone is consistent, the system is capable of dealing with potential inconsistencies between the ontology and the rules, and, after an initial brief pre-processing period utilizing OWL 2 EL reasoner ELK, returns answers to queries at an interactive response time by means of XSB Prolog. 2222222\n",
      "In order to reduce the cost of building domain ontologies manually, in this paper, we integrate a domain ontology development environment: DODDLE-OWL and an ontology search engine: Swoogle. We propose a method and a tool for domain ontology construction reusing texts and existing ontologies for a target domain extracted by Swoogle. In the evaluation, we applied the method to a particular field of law and evaluated the acquired ontologies. 1111111\n",
      "  2222222\n",
      "The use of natural language identiﬁers as reference for ontology elements—in addition to the URIs required by the Semantic Web standards—is of utmost importance based on their predominance in the human everyday life, i.e. speech or print media. Depending on the context, different names can be chosen for one and the same element, and the same element can be referenced by different names. Here homonymy and synonymy are the main cause of the appearance of ambiguity in perceiving which concrete unique ontology element ought to be referenced by a speciﬁc natural language identiﬁer describing an entity. We propose a novel method to resolve entity references under the aspect of ambiguity which explores only formal background knowledge represented in RDF graph structures. The key idea of our domain independent approach is to build an entity network with the most likely referenced ontology elements by constructing spanning graphs based on spreading activation (SA). Additional to the exploitation of complex graph structures we devise a new ranking technique that characterises the likelihood of entities in  this network, i.e. interpretation contexts. Experiments in a highly polysemic domain show the ability of the algorithm to retrieve the correct ontology elements in almost all cases. 1111111\n",
      "  2222222\n",
      "The use of natural language identiﬁers as reference for ontology elements—in addition to the URIs required by the Semantic Web standards—is of utmost importance based on their predominance in the human everyday life, i.e. speech or print media. Depending on the context, different names can be chosen for one and the same element, and the same element can be referenced by different names. Here homonymy and synonymy are the main cause of the appearance of ambiguity in perceiving which concrete unique ontology element ought to be referenced by a speciﬁc natural language identiﬁer describing an entity. We propose a novel method to resolve entity references under the aspect of ambiguity which explores only formal background knowledge represented in RDF graph structures. The key idea of our domain independent approach is to build an entity network with the most likely referenced ontology elements by constructing spanning graphs based on spreading activation (SA). Additional to the exploitation of complex graph structures we devise a new ranking technique that characterises the likelihood of entities in  this network, i.e. interpretation contexts. Experiments in a highly polysemic domain show the ability of the algorithm to retrieve the correct ontology elements in almost all cases. 1111111\n",
      "  2222222\n",
      "The use of natural language identiﬁers as reference for ontology elements—in addition to the URIs required by the Semantic Web standards—is of utmost importance based on their predominance in the human everyday life, i.e. speech or print media. Depending on the context, different names can be chosen for one and the same element, and the same element can be referenced by different names. Here homonymy and synonymy are the main cause of the appearance of ambiguity in perceiving which concrete unique ontology element ought to be referenced by a speciﬁc natural language identiﬁer describing an entity. We propose a novel method to resolve entity references under the aspect of ambiguity which explores only formal background knowledge represented in RDF graph structures. The key idea of our domain independent approach is to build an entity network with the most likely referenced ontology elements by constructing spanning graphs based on spreading activation (SA). Additional to the exploitation of complex graph structures we devise a new ranking technique that characterises the likelihood of entities in  this network, i.e. interpretation contexts. Experiments in a highly polysemic domain show the ability of the algorithm to retrieve the correct ontology elements in almost all cases. 1111111\n",
      "With the wider use of ontologies in the Semantic Web and as part of production systems, multiple scenarios for ontology maintenance and evolution are emerging. For example, successive ontology versions can be posted on the (Semantic) Web, with users discovering the new versions serendipitously; ontology-development in a collaborative environment can be synchronous or asynchronous; managers of projects may exercise quality control, examining the changes from previous baseline versions and accepting or rejecting them before a new baseline is published, and so on. In this paper, we present the different scenarios for ontology maintenance and evolution that we have encountered in our own projects and in those of our collaborators. We define several dimensions that categorize different scenarios. For each scenario, we discuss the high-level tasks that an editing environment must support. We then present a unified comprehensive set of tools to support the different scenarios in a single framework, allowing users to switch between different modes easily. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our purpose is to provide end-users a means to query ontology based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. Our contribution is materialized in a system named SWIP, standing for Semantic Web Interface Using Patterns. The demo will present use cases of this system. 1111111\n",
      "  2222222\n",
      "Our purpose is to provide end-users a means to query ontology based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. Our contribution is materialized in a system named SWIP, standing for Semantic Web Interface Using Patterns. The demo will present use cases of this system. 1111111\n",
      "Powered by Semantic Web technologies the Linked Data paradigm aims at weaving a globally interconnected graph of raw data that transforms the ways we publish, retrieve, share, reuse, and integrate data from a variety of distributed and heterogeneous sources. In practice, however, this vision faces substantial challenges with respect to data quality, coverage, and longevity,  the amount of background knowledge required to query distant data, the  reproducibility of query results and their derived (scientific) findings, and the lack of computational capabilities required for many tasks.  One key issues underlying these challenges is the trade-off between storing data and computing them. Intuitively, data that is derived from already stored data, changes frequently in space and time,  or is the result of some workflow or procedure, should be compute. However, this functionality is not readily available on the Linked Data cloud and its technology stack. In this work, we introduce a  proxy that can transparently run on top of arbitrary SPARQL endpoints and enables the on-demand computation of Linked Data together with the provenance information required to understand how they were derived. While our work can be generalized to multiple domains, we focus on two geographic use case to showcase the proxy's capabilities. 2222222\n",
      "Our purpose is to provide end-users a means to query ontology based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. Our contribution is materialized in a system named SWIP, standing for Semantic Web Interface Using Patterns. The demo will present use cases of this system. 1111111\n",
      "  2222222\n",
      "The modelling of realistic emotional behaviour is needed for various applications in multimodal human-machine interaction such as the design of emotional conversational agents (Martin et al., 2005) or of emotional detection systems (Devillers and Vidrascu, 2007). Yet, building such models requires appropriate definition of various levels for representing the emotions themselves but also some contextual information such as the events that elicit these emotions. This paper presents a coding scheme that has been defined following annotations of a corpus of TV interviews (EmoTV). Deciding which events triggered or may trigger which emotion is a challenge for building efficient emotion eliciting protocols. In this paper, we present the protocol that we defined for collecting another corpus of spontaneous human-human interactions recorded in laboratory conditions (EmoTaboo). We discuss the events that we designed for eliciting emotions. Part of this scheme for coding emotional event is being included in the specifications that are currently defined by a working group of the W3C (the W3C Emotion Incubator Working group). This group is investigating the feasibility of working towards a standard representation of emotions and related states in technological contexts. 1111111\n",
      "Within the framework of the Dutch-Flemish programme STEVIN, the JASMIN-CGN (Jongeren, Anderstaligen en Senioren in Mens-machine Interactie  Corpus Gesproken Nederlands) project was carried out, which was aimed at collecting speech of children, non-natives and elderly people. The JASMIN-CGN project is an extension of the Spoken Dutch Corpus (CGN) along three dimensions. First, by collecting a corpus of contemporary Dutch as spoken by children of different age groups, elderly people and non-natives with different mother tongues, an extension along the age and mother tongue dimensions was achieved. In addition, we collected speech material in a communication setting that was not envisaged in the CGN: human-machine interaction. One third of the data was collected in Flanders and two thirds in the Netherlands. In this paper we report on our experiences in collecting this corpus and we describe some of the important decisions that we made in the attempt to combine efficiency and high quality. 2222222\n",
      "The modelling of realistic emotional behaviour is needed for various applications in multimodal human-machine interaction such as the design of emotional conversational agents (Martin et al., 2005) or of emotional detection systems (Devillers and Vidrascu, 2007). Yet, building such models requires appropriate definition of various levels for representing the emotions themselves but also some contextual information such as the events that elicit these emotions. This paper presents a coding scheme that has been defined following annotations of a corpus of TV interviews (EmoTV). Deciding which events triggered or may trigger which emotion is a challenge for building efficient emotion eliciting protocols. In this paper, we present the protocol that we defined for collecting another corpus of spontaneous human-human interactions recorded in laboratory conditions (EmoTaboo). We discuss the events that we designed for eliciting emotions. Part of this scheme for coding emotional event is being included in the specifications that are currently defined by a working group of the W3C (the W3C Emotion Incubator Working group). This group is investigating the feasibility of working towards a standard representation of emotions and related states in technological contexts. 1111111\n",
      "Large speech and text corpora are crucial to the development of a state-of-the-art speech recognition system. This paper reports on the construction and evaluation of the first Thai broadcast news speech and text corpora. Specifications and conventions used in the transcription process are described in the paper. The speech corpus contains about 17 hours of speech data while the text corpus was transcribed from around 35 hours of television broadcast news. The characteristics of the corpus were analyzed and shown in the paper. The speech corpus was split according to the evaluation focus condition used in the DARPA Hub-4 evaluation. An 18K-word Thai speech recognition system was setup to test with this speech corpus as a preliminary experiment. Acoustic model adaptations were performed to improve the system performance. The best system yielded a word error rate of about 20% for clean and planned speech, and below 30% for the overall condition. 2222222\n",
      "This paper describes Babylon, a system that attempts to overcome the shortage of parallel texts in low-density languages by supplementing existing parallel texts with texts gathered automatically from the Web. In addition to the identification of entire Web pages, we also propose a new feature specifically designed to find parallel text chunks within a single document. Experiments carried out on the Quechua-Spanish language pair show that the system is successful in automatically identifying a significant amount of parallel texts on the Web. Evaluations of a machine translation system trained on this corpus indicate that the Web-gathered parallel texts can supplement manually compiled parallel texts and perform significantly better than the manually compiled texts when tested on other Web-gathered data. 1111111\n",
      "  2222222\n",
      "This paper describes Babylon, a system that attempts to overcome the shortage of parallel texts in low-density languages by supplementing existing parallel texts with texts gathered automatically from the Web. In addition to the identification of entire Web pages, we also propose a new feature specifically designed to find parallel text chunks within a single document. Experiments carried out on the Quechua-Spanish language pair show that the system is successful in automatically identifying a significant amount of parallel texts on the Web. Evaluations of a machine translation system trained on this corpus indicate that the Web-gathered parallel texts can supplement manually compiled parallel texts and perform significantly better than the manually compiled texts when tested on other Web-gathered data. 1111111\n",
      "  2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper describes Babylon, a system that attempts to overcome the shortage of parallel texts in low-density languages by supplementing existing parallel texts with texts gathered automatically from the Web. In addition to the identification of entire Web pages, we also propose a new feature specifically designed to find parallel text chunks within a single document. Experiments carried out on the Quechua-Spanish language pair show that the system is successful in automatically identifying a significant amount of parallel texts on the Web. Evaluations of a machine translation system trained on this corpus indicate that the Web-gathered parallel texts can supplement manually compiled parallel texts and perform significantly better than the manually compiled texts when tested on other Web-gathered data. 1111111\n",
      "  2222222\n",
      "The primary challenge of machine perception is to define efficient computational methods to derive high-level knowledge from low-level sensor observation data. Emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing and perception, which enable advanced integration and interpretation of heterogeneous sensor data. The computational complexity of OWL, however, seriously limits its applicability and use within resource-constrained environments, such as mobile devices. To overcome this issue, we employ OWL to formally define the inference tasks needed for machine perception - explanation and discrimination - and then provide efficient algorithms for these tasks, using bit-vector encodings and operations. The applicability of our approach to machine perception is evaluated on a smart-phone mobile device, demonstrating dramatic improvements in both efficiency and scale. 1111111\n",
      "  2222222\n",
      "The primary challenge of machine perception is to define efficient computational methods to derive high-level knowledge from low-level sensor observation data. Emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing and perception, which enable advanced integration and interpretation of heterogeneous sensor data. The computational complexity of OWL, however, seriously limits its applicability and use within resource-constrained environments, such as mobile devices. To overcome this issue, we employ OWL to formally define the inference tasks needed for machine perception - explanation and discrimination - and then provide efficient algorithms for these tasks, using bit-vector encodings and operations. The applicability of our approach to machine perception is evaluated on a smart-phone mobile device, demonstrating dramatic improvements in both efficiency and scale. 1111111\n",
      "Nowadays portable devices such as smart phones can be used to capture the face of a user simultaneously with the voice input. Server based or even embedded dialogue system might utilize this additional information to detect whether the speaking user addresses the system or other parties or whether the listening user is focused on the display or not. Depending on these findings the dialogue system might change its strategy to interact with the user improving the overall communication between human and system. To develop and test methods for On/Off-Focus detection a multimodal corpus of user-machine interactions was recorded within the German SmartWeb project. The corpus comprises 99 recording sessions of a triad communication between the user, the system and a human companion. The user can address/watch/listen to the system but also talk to his companion, read from the display or simply talk to herself. Facial video is captured with a standard built-in video camera of a smart phone while voice input in being recorded by a high quality close microphone as well as over a realistic transmission line via Bluetooth and WCDMA. The resulting SmartWeb Video Corpus (SVC) can be obtained from the Bavarian Archive for Speech Signals. 2222222\n",
      "The primary challenge of machine perception is to define efficient computational methods to derive high-level knowledge from low-level sensor observation data. Emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing and perception, which enable advanced integration and interpretation of heterogeneous sensor data. The computational complexity of OWL, however, seriously limits its applicability and use within resource-constrained environments, such as mobile devices. To overcome this issue, we employ OWL to formally define the inference tasks needed for machine perception - explanation and discrimination - and then provide efficient algorithms for these tasks, using bit-vector encodings and operations. The applicability of our approach to machine perception is evaluated on a smart-phone mobile device, demonstrating dramatic improvements in both efficiency and scale. 1111111\n",
      "This paper explores the issue of detecting concepts for ontology learning from text. We investigate various metrics from graph theory and propose various voting schemes based on these metrics. The idea draws its root in social choice theory, and our objective is to mimic consensus in automatic learning methods and increase the confidence in concept extraction through the identification of the best performing metrics, the comparison of these metrics with standard information retrieval metrics (such as TF-IDF) and the evaluation of various voting schemes. Our results show that three graph-based metrics Degree, Reachability and HITS-hub were the most successful in identifying relevant concepts contained in two gold standard ontologies. 2222222\n",
      "This paper describes a database of 11 dependency treebanks which were unified by means of a two-dimensional graph format. The format was evaluated with respect to storage-complexity on the one hand, and efficiency of data access on the other hand. An example of how the treebanks can be integrated within a unique interface is given by means of the DTDB interface. 1111111\n",
      "In this paper, we present a linguistic resource that annotates event structures in texts. We consider an event structure as a collection of events that interact with each other in a given situation. We interpret the interactions between events as event relations. In this regard, we propose and annotate a set of six relations that best capture the concept of event structure. These relations are: subevent, reason, purpose, enablement, precedence and related. A document from this resource can encode multiple event structures and an event structure can be described across multiple documents. In order to unify event structures, we also annotate inter- and intra-document event coreference. Moreover, we provide methodologies for automatic discovery of event structures from texts. First, we group the events that constitute an event structure into event clusters and then, we use supervised learning frameworks to classify the relations that exist between events from the same cluster 2222222\n",
      "In the semantic web environment, where two or more independent ontologies can be used in order to describe knowledge and data, ontologies have to be aligned by defining mappings among the elements of one ontology and the elements of another ontology. Very often, mappings are not derived by the semantics of the ontologies that are compared, but, rather, by an evaluation of the similarity of the terminology used in the two ontologies or of their syntactic structure. Moreover, ontology mappings can be inaccurate, because ontology matching tools derive such mappings from inaccurate terminology or even because they are not specifically tailored for the domain at hand. In this paper, we propose a new mapping validation approach for interpreting similarity-based mappings as semantic relations, by coping also with inaccuracy situations. The idea is to see two independent ontologies as a unique distributed knowledge base and to assume a semantic interpretation of ontology mappings as probabilistic and hypothetical relations among ontology elements. We present and use a probabilistic reasoning tool in order to validate mappings and to possibly infer new relations among the ontologies. 1111111\n",
      "Ontology search and reuse is becoming increasingly important as the quest for methods to reduce the cost of constructing such knowledge structures continues. To this end, a number of ontology libraries and search engines are coming to existence to facilitate locating and retrieving potentially relevant ontologies. The number of ontologies available for reuse is steadily growing, and so is the need for methods to evaluate and rank existing ontologies in terms of their relevance to the needs of the knowledge engineer. This paper presents AKTiveRank, a prototype system for ranking ontologies based on a number of structural metrics. The paper describes those metrics, a ranking experiment and an evaluation of results. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the semantic web environment, where two or more independent ontologies can be used in order to describe knowledge and data, ontologies have to be aligned by defining mappings among the elements of one ontology and the elements of another ontology. Very often, mappings are not derived by the semantics of the ontologies that are compared, but, rather, by an evaluation of the similarity of the terminology used in the two ontologies or of their syntactic structure. Moreover, ontology mappings can be inaccurate, because ontology matching tools derive such mappings from inaccurate terminology or even because they are not specifically tailored for the domain at hand. In this paper, we propose a new mapping validation approach for interpreting similarity-based mappings as semantic relations, by coping also with inaccuracy situations. The idea is to see two independent ontologies as a unique distributed knowledge base and to assume a semantic interpretation of ontology mappings as probabilistic and hypothetical relations among ontology elements. We present and use a probabilistic reasoning tool in order to validate mappings and to possibly infer new relations among the ontologies. 1111111\n",
      "The evaluation of matching applications is becoming a major issue in the semantic web and it requires a suitable methodological approach as well as appropriate benchmarks. In particular, in order to evaluate a matching application under different experimental conditions, it is crucial to provide a test dataset characterized by a controlled variety of different heterogeneities among data that rarely occurs in real data repositories. In this paper, we propose SWING (Semantic Web INstance Generation), a disciplined approach to the semi-automatic generation of benchmarks to be used for the evaluation of matching applications. SWING is illustrated in the paper by presenting the specific benchmark we generated for the international instance matching contest at OAEI 2010 (called IIMB 2010) and by discussing the experimental results obtained on it with different matching algorithms. 2222222\n",
      "In the semantic web environment, where two or more independent ontologies can be used in order to describe knowledge and data, ontologies have to be aligned by defining mappings among the elements of one ontology and the elements of another ontology. Very often, mappings are not derived by the semantics of the ontologies that are compared, but, rather, by an evaluation of the similarity of the terminology used in the two ontologies or of their syntactic structure. Moreover, ontology mappings can be inaccurate, because ontology matching tools derive such mappings from inaccurate terminology or even because they are not specifically tailored for the domain at hand. In this paper, we propose a new mapping validation approach for interpreting similarity-based mappings as semantic relations, by coping also with inaccuracy situations. The idea is to see two independent ontologies as a unique distributed knowledge base and to assume a semantic interpretation of ontology mappings as probabilistic and hypothetical relations among ontology elements. We present and use a probabilistic reasoning tool in order to validate mappings and to possibly infer new relations among the ontologies. 1111111\n",
      "While much work on automated ontology enrichment has focused on mining text for concepts and relations, little attention has been paid to the task of enriching ontologies with complex axioms. In this paper, we focus on a form of text that is frequent in industry, namely system installation design principle (SIDP) and we present a framework which can be used both to map SIDPs to OWL DL axioms and to assess the quality of these automatically derived axioms. We present experimental results on a set of 960 SIDPs provided by Airbus which demonstrate (i) that the approach is robust (98% of the SIDPs can be parsed) and (ii) that DL axioms  assigned to full parses are very likely to be correct in 96% of the cases. 2222222\n",
      "Existing opinion retrieval techniques do not provide context-dependent relevant results. Most of the approaches used by state-of-the-art techniques are based on frequency of query terms, such that all documents containing query terms are retrieved, regardless of contextual relevance to the intent of the human seeking the opinion. However, in a particular opinionated document, words could occur in different contexts, yet meet the frequency attached to a certain opinion threshold, thus explicitly creating a bias in overall opinion retrieved. In this paper we propose a sentence-level contextual model for opinion retrieval using grammatical tree derivations and approval voting mechanism. Model evaluation performed between our contextual model, BM25, and language model shows that the model can be effective for contextual opinion retrieval such as faceted opinion retrieval. 1111111\n",
      "Vocabularies are increasingly developed on platforms for hosting version-controlled repositories, such as GitHub. However, these platforms lack important features that have proven useful in vocabulary development. We present VoCol, an integrated environment that supports the Git-based development of vocabularies. VoCol is based on a fundamental model of vocabulary development, consisting of the three core activities modeling, population and testing. We implemented VoCol using a loose coupling of validation, querying, analytics, visualization, and documentation generation components on top of a standard Git repository. All components, including the version-controlled repository, can be configured and replaced with little effort to cater for various use cases. We show the applicability of VoCol with an example and report on a user study that confirms its usability and usefulness. 2222222\n",
      "Existing opinion retrieval techniques do not provide context-dependent relevant results. Most of the approaches used by state-of-the-art techniques are based on frequency of query terms, such that all documents containing query terms are retrieved, regardless of contextual relevance to the intent of the human seeking the opinion. However, in a particular opinionated document, words could occur in different contexts, yet meet the frequency attached to a certain opinion threshold, thus explicitly creating a bias in overall opinion retrieved. In this paper we propose a sentence-level contextual model for opinion retrieval using grammatical tree derivations and approval voting mechanism. Model evaluation performed between our contextual model, BM25, and language model shows that the model can be effective for contextual opinion retrieval such as faceted opinion retrieval. 1111111\n",
      "The growing interest in free and open-source software which occurred over the last decades has accelerated the usage of versioning systems to help developers collaborating together in the same projects. As a consequence, specific tools such as git and specialized open-source on-line platforms gained importance. In this study, we introduce and share SemanGit which provides a resource at the crossroads of both Semantic Web and git web-based version control systems. SemanGit is actually the first collection of linked data extracted from GitHub based on a git ontology we designed and extended to include specific GitHub features. In this article, we present the dataset, describe the extraction process according to the ontology, show  some promising analyses of the data and outline how SemanGit could be linked with external datasets or enriched with new sources to allow for more complex analyses. 2222222\n",
      "Existing opinion retrieval techniques do not provide context-dependent relevant results. Most of the approaches used by state-of-the-art techniques are based on frequency of query terms, such that all documents containing query terms are retrieved, regardless of contextual relevance to the intent of the human seeking the opinion. However, in a particular opinionated document, words could occur in different contexts, yet meet the frequency attached to a certain opinion threshold, thus explicitly creating a bias in overall opinion retrieved. In this paper we propose a sentence-level contextual model for opinion retrieval using grammatical tree derivations and approval voting mechanism. Model evaluation performed between our contextual model, BM25, and language model shows that the model can be effective for contextual opinion retrieval such as faceted opinion retrieval. 1111111\n",
      "  2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity relations are at the foundation of the Semantic Web and the Linked Data initiative.  In many instances the classical interpretation of identity is too strong for practical purposes.  This is particularly the case when two entities are considered the same in some but not all contexts.  Unfortunately, modeling the specific contexts in which an identity relation holds is cumbersome and, due to arbitrary reuse and the Open World Assumption, it is impossible to anticipate all contexts in which an entity will be used.  We propose an alternative semantics for owl:sameAs that partitions the original relation into a hierarchy of subrelations.  The subrelation to which an identity statement belongs depends on the dataset in which the statement occurs.  Adding future assertions may change the subrelation to which an identity statement belongs, resulting in a context-dependent and non-monotone semantics.  We show that this more complicated semantics is better able to characterize the pragmatic use of owl:sameAs as observed in Linked Open Datasets. 1111111\n",
      "Most of the data on the Web is stored in relational databases. In order to make the Semantic Web grow we need to provide easy-to-use tools to convert those databases into linked data, so that even people with little knowledge of the semantic web can use them. Some programs able to convert relational databases into RDF files have been developed, but the user still has to link manually the database attribute names to existing ontology properties and this generated 'linked data' is not actually linked with external relevant data. We propose here a method to associate automatically attribute names to existing ontology entities in order to complete the automation of the conversion of databases. We also present a way - rather basic, but with low error rate - to add links automatically to relevant data from other data sets. 2222222\n",
      "Identity relations are at the foundation of the Semantic Web and the Linked Data initiative.  In many instances the classical interpretation of identity is too strong for practical purposes.  This is particularly the case when two entities are considered the same in some but not all contexts.  Unfortunately, modeling the specific contexts in which an identity relation holds is cumbersome and, due to arbitrary reuse and the Open World Assumption, it is impossible to anticipate all contexts in which an entity will be used.  We propose an alternative semantics for owl:sameAs that partitions the original relation into a hierarchy of subrelations.  The subrelation to which an identity statement belongs depends on the dataset in which the statement occurs.  Adding future assertions may change the subrelation to which an identity statement belongs, resulting in a context-dependent and non-monotone semantics.  We show that this more complicated semantics is better able to characterize the pragmatic use of owl:sameAs as observed in Linked Open Datasets. 1111111\n",
      "Discovering mappings between concept hierarchies is widely regarded as one of the hardest and most urgent problems facing the Semantic Web. The problem is even harder in domains where concepts are inherently vague and ill-defined, and cannot be given a crisp definition. A notion of approximate concept mapping is required in such domains, but until now, no such notion is available.<br /><br /> The first contribution of this paper is a definition for concepts is decomposed into a number of submappings, and a emph{sloppiness value} determines the fraction of these submappings that can be ignored when establishing the mapping.<br /><br /> A potential problem of such a definition is that with an increasing sloppiness value, it will gradually allow mappings between any two arbitrary concepts. To improve on this trivial behaviour, we need to design a heuristic weighting which minimises the sloppiness required to conclude desirable matches, but at the same time maximises the sloppiness required to conclude undesirable matches. The second contribution of this paper is to show that a emph{Google-based similarity measure} has exactly these desirable properties.<br /><br /> We establish these results by emph{experimental validation in the domain of musical genres}. We show that this domain does suffer from ill-defined concepts. We take two real-life genre hierarchies from the Web, we compute approximate mappings between them at varying levels of sloppiness, and we validate our results against a hand-crafted Gold Standard.<br /><br /> Our method makes use of the huge amount of knowledge that is implicit in the current Web, and exploits this knowledge as a heuristic for establishing approximate mappings between ill-defined concepts. 2222222\n",
      "Identity relations are at the foundation of the Semantic Web and the Linked Data initiative.  In many instances the classical interpretation of identity is too strong for practical purposes.  This is particularly the case when two entities are considered the same in some but not all contexts.  Unfortunately, modeling the specific contexts in which an identity relation holds is cumbersome and, due to arbitrary reuse and the Open World Assumption, it is impossible to anticipate all contexts in which an entity will be used.  We propose an alternative semantics for owl:sameAs that partitions the original relation into a hierarchy of subrelations.  The subrelation to which an identity statement belongs depends on the dataset in which the statement occurs.  Adding future assertions may change the subrelation to which an identity statement belongs, resulting in a context-dependent and non-monotone semantics.  We show that this more complicated semantics is better able to characterize the pragmatic use of owl:sameAs as observed in Linked Open Datasets. 1111111\n",
      "  2222222\n",
      "This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms. First, the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks. Second, automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts, which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms. The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks. Finally, a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list. Our experiments on a large-scale database of web pages have provided very positive results. 1111111\n",
      "Online advertising offers significantly finer granularity, which has been leveraged in state-of-the-art targeting methods, like Behavioral Targeting (BT). Such methods have been further complemented by recent work in Look-alike Modeling (LAM) which helps in creating models which are customized according to each advertiser s requirements and each campaign s characteristics, and which show ads to users who are most likely to convert on them, not just click them. In Look-alike Modeling given data about converters and nonconverters, obtained from advertisers, we would like to train models automatically for each ad campaign. Such custom models would help target more users who are similar to the set of converters the advertiser provides. The advertisers get more freedom to define their preferred sets of users which should be used as a basis to build custom targeting models. In behavioral data, the number of conversions (positive class) per campaign is very small (conversions per impression for the advertisers in our data set are much less than 10e 4), giving rise to a highly skewed training dataset, which has most records pertaining to the negative class. Campaigns with very few conversions are called as tail campaigns, and those with many conversions are called head campaigns. Creation of Look-alike Models for tail campaigns is very challenging and tricky using popular classifiers like Linear SVM and GBDT, because of the very few number of positive class examples such campaigns contain. In this paper, we present an Associative Classification (AC) approach to LAM for tail campaigns. Pairs of features are used to derive rules to build a Rule-based Associative Classifier, with the rules being sorted by frequency-weighted log-likelihood ratio (F-LLR). The top k rules, sorted by F-LLR, are then applied to any test record to score it. Individual features can also form rules by themselves, though the number of such rules in the top k rules and the whole rule-set is very small. Our algorithm is based on Hadoop, and is thus very efficient in terms of speed. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms. First, the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks. Second, automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts, which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms. The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks. Finally, a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list. Our experiments on a large-scale database of web pages have provided very positive results. 1111111\n",
      "  2222222\n",
      "This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms. First, the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks. Second, automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts, which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms. The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks. Finally, a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list. Our experiments on a large-scale database of web pages have provided very positive results. 1111111\n",
      "  2222222\n",
      "This paper describes the development of a written corpus of argumentative reasoning. Arguments in the corpus have been analysed using state of the art techniques from argumentation theory and have been marked up using an open, reusable markup language. A number of the key challenges enountered during the process are explored, and preliminary observations about features such as inter-coder reliability and corpus statistics are discussed. In addition, several examples are offered of how this kind of language resource can be used in linguistic, computational and philosophical research, and in particular, how the corpus has been used to initiate a programme investigating the automatic detection of argumentative structure. 1111111\n",
      "Fixed, limited budgets often constrain the amount of expert annotation that can go into the construction of annotated corpora. Estimating the cost of annotation is the first step toward using annotation resources wisely. We present here a study of the cost of annotation. This study includes the participation of annotators at various skill levels and with varying backgrounds. Conducted over the web, the study consists of tests that simulate machine-assisted pre-annotation, requiring correction by the annotator rather than annotation from scratch. The study also includes tests representative of an annotation scenario involving Active Learning as it progresses from a naïve model to a knowledgeable model; in particular, annotators encounter pre-annotation of varying degrees of accuracy. The annotation interface lists tags considered likely by the annotation model in preference to other tags. We present the experimental parameters of the study and report both descriptive and inferential statistics on the results of the study. We conclude with a model for estimating the hourly cost of annotation for annotators of various skill levels. We also present models for two granularities of annotation: sentence at a time and word at a time. 2222222\n",
      "This paper describes the development of a written corpus of argumentative reasoning. Arguments in the corpus have been analysed using state of the art techniques from argumentation theory and have been marked up using an open, reusable markup language. A number of the key challenges enountered during the process are explored, and preliminary observations about features such as inter-coder reliability and corpus statistics are discussed. In addition, several examples are offered of how this kind of language resource can be used in linguistic, computational and philosophical research, and in particular, how the corpus has been used to initiate a programme investigating the automatic detection of argumentative structure. 1111111\n",
      "News articles about the same event published over time have properties that challenge NLP and IR applications. A cluster of such texts typically exhibits instances of paraphrase and contradiction, as sources update the facts surrounding the story, often due to an ongoing investigation. The current hypothesis is that the stories  evolve  over time, beginning with the first text published on a given topic. This is tested using a phylogenetic approach as well as one based on language modeling. The fit of the evolutionary models is evaluated with respect to how well they facilitate the recovery of chronological relationships between the documents. Over all data clusters, the language modeling approach consistently outperforms the phylogenetics model. However, on manually collected clusters in which the documents are published within short time spans of one another, both have a similar performance, and produce statistically significant results on the document chronology recovery evaluation. 2222222\n",
      "The huge amount of the available information in the Web creates the need of effective information extraction systems that are able to produce metadata that satisfy user s information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn extraction models. The production of such corpora can be significantly facilitated by annotation tools that are able to annotate, according to a defined ontology, not only named entities but most importantly relations between them. This paper describes the BOEMIE ontology-based annotation tool which is able to locate blocks of text that correspond to specific types of named entities, fill tables corresponding to ontology concepts with those named entities and link the filled tables based on relations defined in the domain ontology. Additionally, it can perform annotation of blocks of text that refer to the same topic. The tool has a user-friendly interface, supports automatic pre-annotation, annotation comparison as well as customization to other annotation schemata. The annotation tool has been used in a large scale annotation task involving 3,000 web pages regarding athletics. It has also been used in another annotation task involving 503 web pages with medical information, in different languages. 1111111\n",
      "Everyday individual wants to record, annotate, and manipulate digital media in their own ways. Semantic web supports users to represent annotations in forms that bear personal semantic meaning. An essential feature of personal media management systems is to give individual users significant control over representation, annotation and query their media information. In our framework, na�ve users create their personalized ontology for describing their media information and construct semantically rich metadata collections using the personalized media ontology. Our system allows users to provide expressive queries in their own ontology. Queries are represented by OWL classes or instances. The system uses a uniform representation of personalized ontology, metadata, and queries. Such a uniform representation enables the system to exploit description logic reasoner. 2222222\n",
      "The huge amount of the available information in the Web creates the need of effective information extraction systems that are able to produce metadata that satisfy user s information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn extraction models. The production of such corpora can be significantly facilitated by annotation tools that are able to annotate, according to a defined ontology, not only named entities but most importantly relations between them. This paper describes the BOEMIE ontology-based annotation tool which is able to locate blocks of text that correspond to specific types of named entities, fill tables corresponding to ontology concepts with those named entities and link the filled tables based on relations defined in the domain ontology. Additionally, it can perform annotation of blocks of text that refer to the same topic. The tool has a user-friendly interface, supports automatic pre-annotation, annotation comparison as well as customization to other annotation schemata. The annotation tool has been used in a large scale annotation task involving 3,000 web pages regarding athletics. It has also been used in another annotation task involving 503 web pages with medical information, in different languages. 1111111\n",
      "  2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The huge amount of the available information in the Web creates the need of effective information extraction systems that are able to produce metadata that satisfy user s information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn extraction models. The production of such corpora can be significantly facilitated by annotation tools that are able to annotate, according to a defined ontology, not only named entities but most importantly relations between them. This paper describes the BOEMIE ontology-based annotation tool which is able to locate blocks of text that correspond to specific types of named entities, fill tables corresponding to ontology concepts with those named entities and link the filled tables based on relations defined in the domain ontology. Additionally, it can perform annotation of blocks of text that refer to the same topic. The tool has a user-friendly interface, supports automatic pre-annotation, annotation comparison as well as customization to other annotation schemata. The annotation tool has been used in a large scale annotation task involving 3,000 web pages regarding athletics. It has also been used in another annotation task involving 503 web pages with medical information, in different languages. 1111111\n",
      "Significant efforts have focused in the past years on bringing large amounts of metadata online and the success of these efforts can be seen by the impressive number of web sites exposing data in RDFa or RDF/XML. However, little is known about the extent to which this data fits the needs of ordinary web users with everyday information needs. In this paper we study what we perceive as the semantic gap between the supply of data on the Semantic Web and the needs of web users as expressed in the queries submitted to a major Web search engine. We perform our analysis on both the level of instances and ontologies. First, we first look at how much data is actually relevant to Web queries and what kind of data is it. Second, we provide a generic method to extract the attributes that Web users are searching for regarding particular classes of entities. This method allows to contrast class definitions found in Semantic Web vocabularies with the attributes of objects that users are interested in. Our findings are crucial to measuring the potential of semantic search, but also speak to the state of the Semantic Web in general. 2222222\n",
      "The paper gives a comprehensive overview over the results, the concepts and the methods which were developed and used to create the Pronouncing Dictionary of Austrian German (ÖAWB) and the Austrian Pronouncing Database ADABA. The ÖAWB contains 42,000 entries which are based on a large audio corpus of 75,964 realisations of two model speakers each from Austria, Germany and Switzerland. The ADABA database provides 9 different ways to search the data. It also contains 24 model texts and another 30 texts showing linguistic and phonetic variation in Austria and in the other German speaking countries. The codification of Austrian standard pronunciation was based on the concept of German as a pluricentric language and on the concept of  media presentation language . Austrian pronunciation forms are presented in parallel with those of Germany and Switzerland to allow the comparison of differences between linguistically close national varieties of a language. The paper also gives a detailed characterisation of the software (transcriber, database) which was developed during the project that was supported by the Austrian national broadcasting corporation ORF and the University for Music and Dramatic Arts in Graz. Some of the software and the data can be obtained from the web site www.adaba.at. 1111111\n",
      "We analyze knowledge production in Computer Science by means of coauthorship networks. For this, we consider 30 graduate programs of different regions of the world, being 8 programs in Brazil, 16 in North America (3 in Canada and 13 in the United States), and 6 in Europe (2 in France, 1 in Switzerland and 3 in the United Kingdom). We use a dataset that consists of 176,537 authors and 352,766 publication entries distributed among 2,176 publication venues. The results obtained for different metrics of collaboration social networks indicate the process of knowledge production has a ˘changed differently for each region. Research is increasingly done in teams across different fields of Computer Science. The size of the giant component indicates the existence of isolated collaboration groups in the European network, contrasting to the degree of connectivity found in the Brazilian and North-American counterparts. We also analyzed the temporal evolution of the social networks representing the three regions. The number of authors per paper experienced an increase in a time span of 12 years. We observe that the number of collaborations between authors grows faster than the number of authors, benefiting from the existing network structure. The temporal evolution shows differences between well-established fields, such as Databases and Computer Architecture, and emerging fields, like Bioinformatics and Geoinformatics. The patterns of collaboration analyzed in this paper contribute to an overall understanding of Computer Science research in different geographical regions that could not be achieved without the use of complex networks and a large publication database. 2222222\n",
      "The paper gives a comprehensive overview over the results, the concepts and the methods which were developed and used to create the Pronouncing Dictionary of Austrian German (ÖAWB) and the Austrian Pronouncing Database ADABA. The ÖAWB contains 42,000 entries which are based on a large audio corpus of 75,964 realisations of two model speakers each from Austria, Germany and Switzerland. The ADABA database provides 9 different ways to search the data. It also contains 24 model texts and another 30 texts showing linguistic and phonetic variation in Austria and in the other German speaking countries. The codification of Austrian standard pronunciation was based on the concept of German as a pluricentric language and on the concept of  media presentation language . Austrian pronunciation forms are presented in parallel with those of Germany and Switzerland to allow the comparison of differences between linguistically close national varieties of a language. The paper also gives a detailed characterisation of the software (transcriber, database) which was developed during the project that was supported by the Austrian national broadcasting corporation ORF and the University for Music and Dramatic Arts in Graz. Some of the software and the data can be obtained from the web site www.adaba.at. 1111111\n",
      "We present a success story on the adoption of semantic technologies for the library of the second biggest university hospital of France. This project was divided into three parts: preprocessing, semantic enrichment and data integration. This abstract introduces the research challenges faced in the project as well as the outcomes obtained so far. 2222222\n",
      "The paper gives a comprehensive overview over the results, the concepts and the methods which were developed and used to create the Pronouncing Dictionary of Austrian German (ÖAWB) and the Austrian Pronouncing Database ADABA. The ÖAWB contains 42,000 entries which are based on a large audio corpus of 75,964 realisations of two model speakers each from Austria, Germany and Switzerland. The ADABA database provides 9 different ways to search the data. It also contains 24 model texts and another 30 texts showing linguistic and phonetic variation in Austria and in the other German speaking countries. The codification of Austrian standard pronunciation was based on the concept of German as a pluricentric language and on the concept of  media presentation language . Austrian pronunciation forms are presented in parallel with those of Germany and Switzerland to allow the comparison of differences between linguistically close national varieties of a language. The paper also gives a detailed characterisation of the software (transcriber, database) which was developed during the project that was supported by the Austrian national broadcasting corporation ORF and the University for Music and Dramatic Arts in Graz. Some of the software and the data can be obtained from the web site www.adaba.at. 1111111\n",
      "This paper describes the alpha Urban LarKC, one of the first Urban Computing applications built with Semantic Web technologies. It is based on the LarKC platform and makes use of the publicly available data sources on the Web which refer to interesting information about a urban environment (the city of Milano in Italy). 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a challenging task to match similar or related terms/expressions in NLP and Text Mining applications. Two typical areas in need for such work are terminology and ontology constructions, where terms and concepts are extracted and organized into certain structures with various semantic relations. In the EU BOOTSTrep Project we test various techniques for matching terms that can assist human domain experts in building and enriching ontologies. This paper reports on a work in which we evaluated a text comparing and clustering tool for this task. Particularly, we explore the feasibility of matching related terms with their definitions. Ontology terms, such as Gene Ontology terms, are often assigned with detailed definitions, which provide a fundamental information source for detecting relations between terms. Here we focus on the exploitation of term definitions for the term matching task. Our experiment shows that the tool is capable of grouping many related terms using their definitions. 1111111\n",
      "While ontologies are widely accepted internationally as knowledge management mechanism across disciplines, the ability to reason over knowledge bases regardless of the natural languages used in them has become a pressing issue in digital content management. To enable knowledge sharing and reuse, ontology mapping techniques must be able to work with otherwise isolated ontologies that are labelled in diverse natural languages. Machine translation techniques are often employed by cross-lingual ontology mapping approaches to turn a cross-lingual mapping problem into a monolingual mapping problem which can then be solved by state of the art monolingual ontology matching tools. However in the process of doing so, complications introduced by machine translation tools can compromise the performance of the subsequent monolingual matching techniques. In this paper, a novel approach to improve the quality of cross-lingual ontology mapping is presented and evaluated. The proposed approach adopts the pseudo feedback technique that is similar to the well understood relevance feedback mechanism used in the field of information retrieval. It is shown through the evaluation that the pseudo feedback feature can enhance the effectiveness of machine translation and monolingual matching techniques in a cross-lingual ontology mapping scenario. 2222222\n",
      "It is a challenging task to match similar or related terms/expressions in NLP and Text Mining applications. Two typical areas in need for such work are terminology and ontology constructions, where terms and concepts are extracted and organized into certain structures with various semantic relations. In the EU BOOTSTrep Project we test various techniques for matching terms that can assist human domain experts in building and enriching ontologies. This paper reports on a work in which we evaluated a text comparing and clustering tool for this task. Particularly, we explore the feasibility of matching related terms with their definitions. Ontology terms, such as Gene Ontology terms, are often assigned with detailed definitions, which provide a fundamental information source for detecting relations between terms. Here we focus on the exploitation of term definitions for the term matching task. Our experiment shows that the tool is capable of grouping many related terms using their definitions. 1111111\n",
      "  2222222\n",
      "It is a challenging task to match similar or related terms/expressions in NLP and Text Mining applications. Two typical areas in need for such work are terminology and ontology constructions, where terms and concepts are extracted and organized into certain structures with various semantic relations. In the EU BOOTSTrep Project we test various techniques for matching terms that can assist human domain experts in building and enriching ontologies. This paper reports on a work in which we evaluated a text comparing and clustering tool for this task. Particularly, we explore the feasibility of matching related terms with their definitions. Ontology terms, such as Gene Ontology terms, are often assigned with detailed definitions, which provide a fundamental information source for detecting relations between terms. Here we focus on the exploitation of term definitions for the term matching task. Our experiment shows that the tool is capable of grouping many related terms using their definitions. 1111111\n",
      "Modern search engines are expected to make documents searchable shortly after they appear on the ever changing Web. To satisfy this requirement, the Web is frequently crawled. Due to the sheer size of their indexes, search engines distribute the crawled documents among thousands of servers in a scheme called local index-partitioning, such that each server indexes only several million pages. To ensure documents from the same host (e.g., www.nytimes.com) are distributed uniformly over the servers, for load balancing purposes, random routing of documents to servers is common. To expedite the time documents become searchable after being crawled, documents may be simply appended to the existing index partitions. However, indexing by merely appending documents, results in larger index sizes since document reordering for index compactness is no longer performed. This, in turn, degrades search query processing performance which depends heavily on index sizes. A possible way to balance quick document indexing with efficient query processing, is to deploy online document routing strategies that are designed to reduce index sizes. This work considers the effects of several online document routing strategies on the aggregated partitioned index size. We show that there exists a tradeoff between the compression of a partitioned index and the distribution of documents from the same host across the index partitions (i.e., host distribution). We suggest and evaluate several online routing strategies with regard to their compression, host distribution, and complexity. In particular, we present a term based routing algorithm which is shown analytically to provide better compression results than the industry standard random routing scheme. In addition, our algorithm demonstrates comparable compression performance and host distribution while having much better running time complexity than other document routing heuristics. Our findings are validated by experimental evaluation performed on a large benchmark collection of Web pages. 2222222\n",
      "With the proliferation of knowledge intensive applications, there is a vivid research in the domain of knowledge representation. Description Logics are designed to be a convenient means for such representation task. One of their main advantages over other formalisms is a clearly defined semantics which opens the possibility to provide reasoning services with mathematical rigorousness.  My PhD work is concerned with Description Logic reasoning. I am particularly interested in ABox reasoning when the available data is really large. This domain is much less explored than TBox reasoning. Nevertheless, reasoning over large ABoxes is useful for problems like web-based reasoning.  I am one of the developers of the DLog data reasoner which implements a two phase reasoning: the first phase uses complex reasoning to turn the TBox into a set of simple rules, while the second phase is geared towards very fast query answering over large ABoxes. DLog currently supports the SHIQ DL language. We are trying to extend the reasoner to more expressive languages, hopefully until SROIQ, the logic behind OWL 2. 1111111\n",
      "We introduce DLR+, an extension of the n-ary propositionally closed description logic DLR to deal with attribute-labelled tuples (generalising the positional notation), projections of relations, and global and local objectification of relations, able to express inclusion, functional, key, and external uniqueness dependencies. The logic is equipped with both TBox and ABox axioms. We show how a simple syntactic restriction on the appearance of projections sharing common attributes in a DLR+ knowledge base makes reasoning in the language decidable with the same computational complexity as DLR. The obtained DLR+- n-ary description logic is able to encode more thoroughly conceptual data models such as EER, UML, and ORM. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the proliferation of knowledge intensive applications, there is a vivid research in the domain of knowledge representation. Description Logics are designed to be a convenient means for such representation task. One of their main advantages over other formalisms is a clearly defined semantics which opens the possibility to provide reasoning services with mathematical rigorousness.  My PhD work is concerned with Description Logic reasoning. I am particularly interested in ABox reasoning when the available data is really large. This domain is much less explored than TBox reasoning. Nevertheless, reasoning over large ABoxes is useful for problems like web-based reasoning.  I am one of the developers of the DLog data reasoner which implements a two phase reasoning: the first phase uses complex reasoning to turn the TBox into a set of simple rules, while the second phase is geared towards very fast query answering over large ABoxes. DLog currently supports the SHIQ DL language. We are trying to extend the reasoner to more expressive languages, hopefully until SROIQ, the logic behind OWL 2. 1111111\n",
      "  2222222\n",
      "With the proliferation of knowledge intensive applications, there is a vivid research in the domain of knowledge representation. Description Logics are designed to be a convenient means for such representation task. One of their main advantages over other formalisms is a clearly defined semantics which opens the possibility to provide reasoning services with mathematical rigorousness.  My PhD work is concerned with Description Logic reasoning. I am particularly interested in ABox reasoning when the available data is really large. This domain is much less explored than TBox reasoning. Nevertheless, reasoning over large ABoxes is useful for problems like web-based reasoning.  I am one of the developers of the DLog data reasoner which implements a two phase reasoning: the first phase uses complex reasoning to turn the TBox into a set of simple rules, while the second phase is geared towards very fast query answering over large ABoxes. DLog currently supports the SHIQ DL language. We are trying to extend the reasoner to more expressive languages, hopefully until SROIQ, the logic behind OWL 2. 1111111\n",
      "Statical ontology learning from large text corpora is a well understood task while evolving ontologies dynamically from user-input has rarely been adressed so far. Evolution of ontologies has to deal with vague or incomplete information. Accordingly, the formalism used for knowledge representation must be able to deal with this kind of information. Classical logical approaches such as description logics are particularly poor in adressing uncertainty. Ontology evolution may benefit from exploring probabilistic or fuzzy approaches to knowledge representation. In this thesis an approach to evolve and update ontologies is developed which uses explicit and implicit user-input and extends probabilistic approaches to ontology engineering. 2222222\n",
      "Requirement engineering is emerging as an increasingly important discipline for supporting Web application development, as these are designed to satisfy diverse stakeholder needs, additional functional, information, multimedia and usability requirements as compared to traditional software applications. Moreover, when considering innovative e-commerce applications, value-based requirements engineering is an extremely relevant methodology which exploits the concept of economic value during the requirements engineering activity. In contrast, most of the methodologies proposed for the development of Web applications, primarily focus on the system design, and paying less attention to the requirements engineering, and specifically to value-based requirement engineering. Focusing this aspect, the paper presents integration of value-based requirement engineering models to WebML models using our recently proposed VIP Business Modeling Framework. The integration process is demonstrated using a well-known e-commerce application example by first presenting example VIP business models and then deriving WebML process, structural and other models from these business models. 1111111\n",
      "Online reputation addresses trust relationships amongst agents in the dynamic open systems which can appear as ratings, recommendations, referrals and feedbacks. Several reputation models and rating aggregation algorithms have been proposed. However, finding a trusted entity on the web is still an issue as all reputation systems work individually. The aim of this project is to introduce a global reputation system that aggregates people’s opinions from different resources (e.g. e-commerce websites, review websites) with the help federated search techniques. Then it will analyze opinions using sentiment analysis approach to extract high quality opinions and generate trust in the search result. 2222222\n",
      "Requirement engineering is emerging as an increasingly important discipline for supporting Web application development, as these are designed to satisfy diverse stakeholder needs, additional functional, information, multimedia and usability requirements as compared to traditional software applications. Moreover, when considering innovative e-commerce applications, value-based requirements engineering is an extremely relevant methodology which exploits the concept of economic value during the requirements engineering activity. In contrast, most of the methodologies proposed for the development of Web applications, primarily focus on the system design, and paying less attention to the requirements engineering, and specifically to value-based requirement engineering. Focusing this aspect, the paper presents integration of value-based requirement engineering models to WebML models using our recently proposed VIP Business Modeling Framework. The integration process is demonstrated using a well-known e-commerce application example by first presenting example VIP business models and then deriving WebML process, structural and other models from these business models. 1111111\n",
      "  2222222\n",
      "Requirement engineering is emerging as an increasingly important discipline for supporting Web application development, as these are designed to satisfy diverse stakeholder needs, additional functional, information, multimedia and usability requirements as compared to traditional software applications. Moreover, when considering innovative e-commerce applications, value-based requirements engineering is an extremely relevant methodology which exploits the concept of economic value during the requirements engineering activity. In contrast, most of the methodologies proposed for the development of Web applications, primarily focus on the system design, and paying less attention to the requirements engineering, and specifically to value-based requirement engineering. Focusing this aspect, the paper presents integration of value-based requirement engineering models to WebML models using our recently proposed VIP Business Modeling Framework. The integration process is demonstrated using a well-known e-commerce application example by first presenting example VIP business models and then deriving WebML process, structural and other models from these business models. 1111111\n",
      "AliMe is an intelligent assistant that offers question answering service in the E-commerce customer service field. By representing knowledge as question answer (QA) pairs, AliMe is able to serve millions of customer questions per day and address 90%+ of them. However, in regulation-oriented scenarios, questions of type “why”, “whether”, “what if ” and “how next” often require knowledge reasoning to obtain a specific or precise answer, and QA-style knowledge representation turns out to be insufficient. To enable AliMe to better understand and serve customer questions, we propose to represent regulation knowledge as event graph, design systematic approach to map customer questions to events, and perform reasoning on event graph according to business rules. We launch our new system in the “counterfeiting penalty” scenario. Online results suggest that our new approach is able to gain better resolution. 2222222\n",
      "Most of today's web content is designed for human consumption, which makes it difficult for software tools to access them readily. Even web content that is automatically generated from back-end databases is usually presented without the original structural information. In this paper, we present an automated information extraction algorithm that can extract the relevant attribute-value pairs from product descriptions across different sites. A notion, called structural-semantic entropy, is used to locate the data of interest on web pages, which measures the density of occurrence of relevant information on the DOM tree representation of web pages. Our approach is less labor-intensive and insensitive to changes in web-page format. Experimental results on a large number of real-life web page collections are encouraging and confirm the feasibility of the approach, which has been successfully applied to detect false drug advertisements on the Web due to its capacity in associating the attributes of records with their respective values. 1111111\n",
      "  2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most of today's web content is designed for human consumption, which makes it difficult for software tools to access them readily. Even web content that is automatically generated from back-end databases is usually presented without the original structural information. In this paper, we present an automated information extraction algorithm that can extract the relevant attribute-value pairs from product descriptions across different sites. A notion, called structural-semantic entropy, is used to locate the data of interest on web pages, which measures the density of occurrence of relevant information on the DOM tree representation of web pages. Our approach is less labor-intensive and insensitive to changes in web-page format. Experimental results on a large number of real-life web page collections are encouraging and confirm the feasibility of the approach, which has been successfully applied to detect false drug advertisements on the Web due to its capacity in associating the attributes of records with their respective values. 1111111\n",
      "We present the multi-task web platform YAM++ online for ontology↵and thesaurus matching, featuring a mapping validation and enrichment interface.↵The online matcher is based on the highly performant YAM++ system, while the↵validator allows to visualize an alignment, edit the relation type and add new↵mappings discovered through a keyword-based search by a domain expert. 2222222\n",
      "A picture is worth a thousand words, we often say, yet many areas are in demand of sophisticated visualization techniques, and the Semantic Web is not an exception. The size and complexity of ontologies and Linked Data in the Semantic Web constantly grows and the diverse backgrounds of the users and application areas multiply at the same time. Providing users with visual representations and sophisticated interaction techniques can significantly aid the exploration and understanding of the domains and knowledge represented by ontologies and Linked Data. There is no one-size-fits-all solution but different use cases demand different visualization and interaction techniques. Ultimately, providing better user interfaces, visual representations and interaction techniques will foster user engagement and likely lead to higher quality results in different applications employing ontologies and proliferate the consumption of Linked Data. 1111111\n",
      "With a growing number of ontologies used on the semantic web, agents can fully make sense of different datasets only if correspondences between those ontologies are known. Ontology matching tools have been proposed to find such correspondences. While the current research focus is mainly on fully automatic matching tools, some approaches have been proposed that involve the user in the matching process. However, there are currently no benchmarks and test methods to compare such tools. In this paper, we introduce a number of quality measures for interactive ontology matching tools, and we discuss means to automatically run benchmark tests for such tools. To demonstrate those evaluations, we show examples on assessing the quality of interactive matching tools which involve the user in matcher selection and matcher parametrization. 2222222\n",
      "A picture is worth a thousand words, we often say, yet many areas are in demand of sophisticated visualization techniques, and the Semantic Web is not an exception. The size and complexity of ontologies and Linked Data in the Semantic Web constantly grows and the diverse backgrounds of the users and application areas multiply at the same time. Providing users with visual representations and sophisticated interaction techniques can significantly aid the exploration and understanding of the domains and knowledge represented by ontologies and Linked Data. There is no one-size-fits-all solution but different use cases demand different visualization and interaction techniques. Ultimately, providing better user interfaces, visual representations and interaction techniques will foster user engagement and likely lead to higher quality results in different applications employing ontologies and proliferate the consumption of Linked Data. 1111111\n",
      "  2222222\n",
      "A picture is worth a thousand words, we often say, yet many areas are in demand of sophisticated visualization techniques, and the Semantic Web is not an exception. The size and complexity of ontologies and Linked Data in the Semantic Web constantly grows and the diverse backgrounds of the users and application areas multiply at the same time. Providing users with visual representations and sophisticated interaction techniques can significantly aid the exploration and understanding of the domains and knowledge represented by ontologies and Linked Data. There is no one-size-fits-all solution but different use cases demand different visualization and interaction techniques. Ultimately, providing better user interfaces, visual representations and interaction techniques will foster user engagement and likely lead to higher quality results in different applications employing ontologies and proliferate the consumption of Linked Data. 1111111\n",
      "The Semantic Web of the future will be characterized by using a very large number of ontologies embedded in ontology networks. It is important to provide strong methodological support for collaborative and context-sensitive development of networks of ontologies. This methodological support includes the identification and definition of which activities should be carried out when ontology networks are collaboratively built. In this paper we present the consensus reaching process followed within the NeOn consortium for the identification and definition of the activities involved in the ontology network development process. The consensus reaching process here presented produces as a result the NeOn Glossary of Activities. This work was conceived due to the lack of standardization in the Ontology Engineering terminology, which clearly contrasts with the Software Engineering field. Our future aim is to standardize the NeOn Glossary of Activities. 2222222\n",
      "In order to reduce the cost of building domain ontologies manually, in this paper, we integrate a domain ontology development environment: DODDLE-OWL and an ontology search engine: Swoogle. We propose a method and a tool for domain ontology construction reusing texts and existing ontologies for a target domain extracted by Swoogle. In the evaluation, we applied the method to a particular field of law and evaluated the acquired ontologies. 1111111\n",
      "  2222222\n",
      "In order to reduce the cost of building domain ontologies manually, in this paper, we integrate a domain ontology development environment: DODDLE-OWL and an ontology search engine: Swoogle. We propose a method and a tool for domain ontology construction reusing texts and existing ontologies for a target domain extracted by Swoogle. In the evaluation, we applied the method to a particular field of law and evaluated the acquired ontologies. 1111111\n",
      "  2222222\n",
      "In order to reduce the cost of building domain ontologies manually, in this paper, we integrate a domain ontology development environment: DODDLE-OWL and an ontology search engine: Swoogle. We propose a method and a tool for domain ontology construction reusing texts and existing ontologies for a target domain extracted by Swoogle. In the evaluation, we applied the method to a particular field of law and evaluated the acquired ontologies. 1111111\n",
      "  2222222\n",
      "The use of natural language identiﬁers as reference for ontology elements—in addition to the URIs required by the Semantic Web standards—is of utmost importance based on their predominance in the human everyday life, i.e. speech or print media. Depending on the context, different names can be chosen for one and the same element, and the same element can be referenced by different names. Here homonymy and synonymy are the main cause of the appearance of ambiguity in perceiving which concrete unique ontology element ought to be referenced by a speciﬁc natural language identiﬁer describing an entity. We propose a novel method to resolve entity references under the aspect of ambiguity which explores only formal background knowledge represented in RDF graph structures. The key idea of our domain independent approach is to build an entity network with the most likely referenced ontology elements by constructing spanning graphs based on spreading activation (SA). Additional to the exploitation of complex graph structures we devise a new ranking technique that characterises the likelihood of entities in  this network, i.e. interpretation contexts. Experiments in a highly polysemic domain show the ability of the algorithm to retrieve the correct ontology elements in almost all cases. 1111111\n",
      "  2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The use of natural language identiﬁers as reference for ontology elements—in addition to the URIs required by the Semantic Web standards—is of utmost importance based on their predominance in the human everyday life, i.e. speech or print media. Depending on the context, different names can be chosen for one and the same element, and the same element can be referenced by different names. Here homonymy and synonymy are the main cause of the appearance of ambiguity in perceiving which concrete unique ontology element ought to be referenced by a speciﬁc natural language identiﬁer describing an entity. We propose a novel method to resolve entity references under the aspect of ambiguity which explores only formal background knowledge represented in RDF graph structures. The key idea of our domain independent approach is to build an entity network with the most likely referenced ontology elements by constructing spanning graphs based on spreading activation (SA). Additional to the exploitation of complex graph structures we devise a new ranking technique that characterises the likelihood of entities in  this network, i.e. interpretation contexts. Experiments in a highly polysemic domain show the ability of the algorithm to retrieve the correct ontology elements in almost all cases. 1111111\n",
      "The success of systems making use of ontology schemas de- pend mainly on the quality of their underlying ontologies. This has been acknowledged by researchers who responded by suggesting metrics to measure different aspects of quality. Tools have also been designed, but determining the set of quality metrics to use may not be a straightfor- ward task. Research on ontology quality shows that detection of problems at an early stage of the ontology development cycle is necessary to re- duce costs and maintenance at later stages, which is more difficult to achieve and requires more effort. Assessment using the right metrics is therefore crucial to identify key quality problems. This ensures that the data and instances of the ontology schema are sound and fit for purpose. Our contribution is a systematic survey on quality metrics applicable to ontologies in the Semantic Web, and preliminary investigation towards methods to visualise quality problems in ontologies. 2222222\n",
      "The use of natural language identiﬁers as reference for ontology elements—in addition to the URIs required by the Semantic Web standards—is of utmost importance based on their predominance in the human everyday life, i.e. speech or print media. Depending on the context, different names can be chosen for one and the same element, and the same element can be referenced by different names. Here homonymy and synonymy are the main cause of the appearance of ambiguity in perceiving which concrete unique ontology element ought to be referenced by a speciﬁc natural language identiﬁer describing an entity. We propose a novel method to resolve entity references under the aspect of ambiguity which explores only formal background knowledge represented in RDF graph structures. The key idea of our domain independent approach is to build an entity network with the most likely referenced ontology elements by constructing spanning graphs based on spreading activation (SA). Additional to the exploitation of complex graph structures we devise a new ranking technique that characterises the likelihood of entities in  this network, i.e. interpretation contexts. Experiments in a highly polysemic domain show the ability of the algorithm to retrieve the correct ontology elements in almost all cases. 1111111\n",
      "  2222222\n",
      "Our purpose is to provide end-users a means to query ontology based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. Our contribution is materialized in a system named SWIP, standing for Semantic Web Interface Using Patterns. The demo will present use cases of this system. 1111111\n",
      "This research explores three SPARQL-based techniques to solve Semantic Web tasks that often require similarity measures, such as semantic data integration, ontology mapping, and Semantic Web service matchmaking. Our aim is to see how far it is possible to integrate customized similarity functions (CSF) into SPARQL to achieve good results for these tasks. Our first approach exploits virtual triples calling property functions to establish virtual relations among resources under comparison; the second approach uses extension functions to filter out resources that do not meet the requested similarity criteria; finally, our third technique applies new solution modifiers to post-process a SPARQL solution sequence. The semantics of the three approaches are formally elaborated and discussed. We close the paper with a demonstration of the usefulness of our iSPARQL framework in the context of a data integration and an ontology mapping experiment. 2222222\n",
      "Our purpose is to provide end-users a means to query ontology based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. Our contribution is materialized in a system named SWIP, standing for Semantic Web Interface Using Patterns. The demo will present use cases of this system. 1111111\n",
      "  2222222\n",
      "Our purpose is to provide end-users a means to query ontology based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. Our contribution is materialized in a system named SWIP, standing for Semantic Web Interface Using Patterns. The demo will present use cases of this system. 1111111\n",
      "  2222222\n",
      "The modelling of realistic emotional behaviour is needed for various applications in multimodal human-machine interaction such as the design of emotional conversational agents (Martin et al., 2005) or of emotional detection systems (Devillers and Vidrascu, 2007). Yet, building such models requires appropriate definition of various levels for representing the emotions themselves but also some contextual information such as the events that elicit these emotions. This paper presents a coding scheme that has been defined following annotations of a corpus of TV interviews (EmoTV). Deciding which events triggered or may trigger which emotion is a challenge for building efficient emotion eliciting protocols. In this paper, we present the protocol that we defined for collecting another corpus of spontaneous human-human interactions recorded in laboratory conditions (EmoTaboo). We discuss the events that we designed for eliciting emotions. Part of this scheme for coding emotional event is being included in the specifications that are currently defined by a working group of the W3C (the W3C Emotion Incubator Working group). This group is investigating the feasibility of working towards a standard representation of emotions and related states in technological contexts. 1111111\n",
      "The success of wikis for collaborative knowledge construction is triggering the development of a number of tools for collaborative conceptual modeling based on them. In this paper we present a completely revised version of MoKi, a tool for modelling ontologies and business process models in an integrated way. 2222222\n",
      "The modelling of realistic emotional behaviour is needed for various applications in multimodal human-machine interaction such as the design of emotional conversational agents (Martin et al., 2005) or of emotional detection systems (Devillers and Vidrascu, 2007). Yet, building such models requires appropriate definition of various levels for representing the emotions themselves but also some contextual information such as the events that elicit these emotions. This paper presents a coding scheme that has been defined following annotations of a corpus of TV interviews (EmoTV). Deciding which events triggered or may trigger which emotion is a challenge for building efficient emotion eliciting protocols. In this paper, we present the protocol that we defined for collecting another corpus of spontaneous human-human interactions recorded in laboratory conditions (EmoTaboo). We discuss the events that we designed for eliciting emotions. Part of this scheme for coding emotional event is being included in the specifications that are currently defined by a working group of the W3C (the W3C Emotion Incubator Working group). This group is investigating the feasibility of working towards a standard representation of emotions and related states in technological contexts. 1111111\n",
      "  2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modelling of realistic emotional behaviour is needed for various applications in multimodal human-machine interaction such as the design of emotional conversational agents (Martin et al., 2005) or of emotional detection systems (Devillers and Vidrascu, 2007). Yet, building such models requires appropriate definition of various levels for representing the emotions themselves but also some contextual information such as the events that elicit these emotions. This paper presents a coding scheme that has been defined following annotations of a corpus of TV interviews (EmoTV). Deciding which events triggered or may trigger which emotion is a challenge for building efficient emotion eliciting protocols. In this paper, we present the protocol that we defined for collecting another corpus of spontaneous human-human interactions recorded in laboratory conditions (EmoTaboo). We discuss the events that we designed for eliciting emotions. Part of this scheme for coding emotional event is being included in the specifications that are currently defined by a working group of the W3C (the W3C Emotion Incubator Working group). This group is investigating the feasibility of working towards a standard representation of emotions and related states in technological contexts. 1111111\n",
      "Despite numerous outstanding results, highly complex and specialized multimedia algorithms have not been able to fulfill the promise of fully automated multimedia interpretation. An essential problem is that they are insufficiently aware of the context they operate in. Algorithms that do take a form of context in consideration, often function in a domain-specific environment. The generic framework proposed in this paper stimulates algorithm collaboration on an interpretation task by continuously actualizing the context of the multimedia item under interpretation. Semantic Web knowledge, combined with reasoning methods, forms the corner stone of the integration of these various interacting agents. We believe that this framework will enable an advanced interpretation of multimedia data that goes beyond the capabilities of individual algorithms. A basic platform implementation already indicates the potential of the concept, clearing the path for even more complex interpretation scenarios. 2222222\n",
      "This paper describes Babylon, a system that attempts to overcome the shortage of parallel texts in low-density languages by supplementing existing parallel texts with texts gathered automatically from the Web. In addition to the identification of entire Web pages, we also propose a new feature specifically designed to find parallel text chunks within a single document. Experiments carried out on the Quechua-Spanish language pair show that the system is successful in automatically identifying a significant amount of parallel texts on the Web. Evaluations of a machine translation system trained on this corpus indicate that the Web-gathered parallel texts can supplement manually compiled parallel texts and perform significantly better than the manually compiled texts when tested on other Web-gathered data. 1111111\n",
      "In this paper we report in the Multilingual Web initiative which is a collaboration between the Internationalization Activity of the W3C and the European Commission (EC), realized as a series of EC-funded projects. We review the outcomes of the first of these projects, “Multilingual Web”, which conducted a successful workshop series aimed at analyzing borders or “gaps” within Web technology standardization that currently hinder multilinguality on the Web. The resulting insight into the scientific, industrial and user stakeholders involved led to a further project “MultilingualWeb-LT”. This project has established a cross-sector W3C Working Group that will address some of the gaps that have been identified through standardization of meta-data. 2222222\n",
      "This paper describes Babylon, a system that attempts to overcome the shortage of parallel texts in low-density languages by supplementing existing parallel texts with texts gathered automatically from the Web. In addition to the identification of entire Web pages, we also propose a new feature specifically designed to find parallel text chunks within a single document. Experiments carried out on the Quechua-Spanish language pair show that the system is successful in automatically identifying a significant amount of parallel texts on the Web. Evaluations of a machine translation system trained on this corpus indicate that the Web-gathered parallel texts can supplement manually compiled parallel texts and perform significantly better than the manually compiled texts when tested on other Web-gathered data. 1111111\n",
      "The importance of evaluation in promoting research and development in the information retrieval and natural language processing domains has long been recognised but is this sufficient? In many areas there is still a considerable gap between the results achieved by the research community and their implementation in commercial applications. This is particularly true for the cross-language or multilingual retrieval areas. Despite the strong demand for and interest in multilingual IR functionality, there are still very few operational systems on offer. The Cross Language Evaluation Forum (CLEF) is now taking steps aimed at changing this situation. The paper provides a critical assessment of the main results achieved by CLEF so far and discusses plans now underway to extend its activities in order to have a more direct impact on the application sector. 2222222\n",
      "This paper describes Babylon, a system that attempts to overcome the shortage of parallel texts in low-density languages by supplementing existing parallel texts with texts gathered automatically from the Web. In addition to the identification of entire Web pages, we also propose a new feature specifically designed to find parallel text chunks within a single document. Experiments carried out on the Quechua-Spanish language pair show that the system is successful in automatically identifying a significant amount of parallel texts on the Web. Evaluations of a machine translation system trained on this corpus indicate that the Web-gathered parallel texts can supplement manually compiled parallel texts and perform significantly better than the manually compiled texts when tested on other Web-gathered data. 1111111\n",
      "Question answering (QA) systems provide a user-friendly way to obtain information from data sources. In particular, the generation of SPARQL queries on a particular RDF knowledge base from natural language queries has gained momentum, since they only require minimal user effort. However, none of the currently existing QA system can reliably generate such queries over large and heterogeneous knowledge base, which we can commonly find in the web of data. In this paper, we propose an approach, which is based on generating templates from natural language questions. Those templates are instantiated to form a set of SPARQL queries, which are then tested using several optimisations. We show the feasibility of the approach by successfully applying it on an existing QA benchmark. 2222222\n",
      "The primary challenge of machine perception is to define efficient computational methods to derive high-level knowledge from low-level sensor observation data. Emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing and perception, which enable advanced integration and interpretation of heterogeneous sensor data. The computational complexity of OWL, however, seriously limits its applicability and use within resource-constrained environments, such as mobile devices. To overcome this issue, we employ OWL to formally define the inference tasks needed for machine perception - explanation and discrimination - and then provide efficient algorithms for these tasks, using bit-vector encodings and operations. The applicability of our approach to machine perception is evaluated on a smart-phone mobile device, demonstrating dramatic improvements in both efficiency and scale. 1111111\n",
      "  2222222\n",
      "The primary challenge of machine perception is to define efficient computational methods to derive high-level knowledge from low-level sensor observation data. Emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing and perception, which enable advanced integration and interpretation of heterogeneous sensor data. The computational complexity of OWL, however, seriously limits its applicability and use within resource-constrained environments, such as mobile devices. To overcome this issue, we employ OWL to formally define the inference tasks needed for machine perception - explanation and discrimination - and then provide efficient algorithms for these tasks, using bit-vector encodings and operations. The applicability of our approach to machine perception is evaluated on a smart-phone mobile device, demonstrating dramatic improvements in both efficiency and scale. 1111111\n",
      "  2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary challenge of machine perception is to define efficient computational methods to derive high-level knowledge from low-level sensor observation data. Emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing and perception, which enable advanced integration and interpretation of heterogeneous sensor data. The computational complexity of OWL, however, seriously limits its applicability and use within resource-constrained environments, such as mobile devices. To overcome this issue, we employ OWL to formally define the inference tasks needed for machine perception - explanation and discrimination - and then provide efficient algorithms for these tasks, using bit-vector encodings and operations. The applicability of our approach to machine perception is evaluated on a smart-phone mobile device, demonstrating dramatic improvements in both efficiency and scale. 1111111\n",
      "  2222222\n",
      "In the semantic web environment, where two or more independent ontologies can be used in order to describe knowledge and data, ontologies have to be aligned by defining mappings among the elements of one ontology and the elements of another ontology. Very often, mappings are not derived by the semantics of the ontologies that are compared, but, rather, by an evaluation of the similarity of the terminology used in the two ontologies or of their syntactic structure. Moreover, ontology mappings can be inaccurate, because ontology matching tools derive such mappings from inaccurate terminology or even because they are not specifically tailored for the domain at hand. In this paper, we propose a new mapping validation approach for interpreting similarity-based mappings as semantic relations, by coping also with inaccuracy situations. The idea is to see two independent ontologies as a unique distributed knowledge base and to assume a semantic interpretation of ontology mappings as probabilistic and hypothetical relations among ontology elements. We present and use a probabilistic reasoning tool in order to validate mappings and to possibly infer new relations among the ontologies. 1111111\n",
      "Ontologies are becoming so large in their coverage that no single person or a small group of people can develop them effectively and ontology development becomes a community-based enterprise. We present Collaborative Protege - an extension of the Protege ontology editor that we have designed specifically to support the collaboration process for a community of users. During the ontology-development process, Collaborative Protege allows users to hold discussions about the ontology components and changes using typed annotations; it tracks the change history of the ontology entities; it provides a chat and search functionality. Users edit simultaneously an ontology stored in a common repository. All changes made by a user are seen immediately by other users. Collaborative Protege is open source and distributed with the full installation of Protege. 2222222\n",
      "In the semantic web environment, where two or more independent ontologies can be used in order to describe knowledge and data, ontologies have to be aligned by defining mappings among the elements of one ontology and the elements of another ontology. Very often, mappings are not derived by the semantics of the ontologies that are compared, but, rather, by an evaluation of the similarity of the terminology used in the two ontologies or of their syntactic structure. Moreover, ontology mappings can be inaccurate, because ontology matching tools derive such mappings from inaccurate terminology or even because they are not specifically tailored for the domain at hand. In this paper, we propose a new mapping validation approach for interpreting similarity-based mappings as semantic relations, by coping also with inaccuracy situations. The idea is to see two independent ontologies as a unique distributed knowledge base and to assume a semantic interpretation of ontology mappings as probabilistic and hypothetical relations among ontology elements. We present and use a probabilistic reasoning tool in order to validate mappings and to possibly infer new relations among the ontologies. 1111111\n",
      "As knowledge bases move into the landscape of larger ontologies and have terabytes of related data, we must work on optimizing the performance of our tools. We are easily tempted to buy bigger machines or to fill rooms with armies of little ones to address the scalability problem. Yet, careful analysis and evaluation of the characteristics of our data-using metrics-often leads to dramatic improvements in performance. Firstly, are current scalable systems scalable enough? We found that for large or deep ontologies (some as large as 500,000 classes) it is hard to say because benchmarks obscure the load-time costs for materialization. Therefore, to expose those costs, we have synthesized a set of more representative ontologies. Secondly, in designing for scalability, how do we manage knowledge over time? By optimizing for data distribution and ontology evolution, we have reduced the population time, including materialization, for the NCBO Resource Index, a knowledge base of 16.4 billion annotations linking 2.4 million terms from 200 ontologies to 3.5 million data elements, from one week to less than one hour for one of the large datasets on the same machine. 2222222\n",
      "In the semantic web environment, where two or more independent ontologies can be used in order to describe knowledge and data, ontologies have to be aligned by defining mappings among the elements of one ontology and the elements of another ontology. Very often, mappings are not derived by the semantics of the ontologies that are compared, but, rather, by an evaluation of the similarity of the terminology used in the two ontologies or of their syntactic structure. Moreover, ontology mappings can be inaccurate, because ontology matching tools derive such mappings from inaccurate terminology or even because they are not specifically tailored for the domain at hand. In this paper, we propose a new mapping validation approach for interpreting similarity-based mappings as semantic relations, by coping also with inaccuracy situations. The idea is to see two independent ontologies as a unique distributed knowledge base and to assume a semantic interpretation of ontology mappings as probabilistic and hypothetical relations among ontology elements. We present and use a probabilistic reasoning tool in order to validate mappings and to possibly infer new relations among the ontologies. 1111111\n",
      "Heterogeneity of ontologies on the web of data is very important problem. To solve this problem, there are a lot of researches about ontology mapping/alignment/matching. This paper shows an application called SPARQLoid that is using a query rewriting method to enable the users to query any SPARQL endpoint with the users own ontology even when their mappings are not reliable enough. Often ontology matching is very difficult problem and it sometimes produces mappings under a certain reliability. Based on the given reliability degrees on those mappings, SPARQLoid allows users to query data in the target SPARQL endpoints by using their own (or a specified certain) ontology under a control of sorting order based on their mapping reliability. 2222222\n",
      "Identity relations are at the foundation of the Semantic Web and the Linked Data initiative.  In many instances the classical interpretation of identity is too strong for practical purposes.  This is particularly the case when two entities are considered the same in some but not all contexts.  Unfortunately, modeling the specific contexts in which an identity relation holds is cumbersome and, due to arbitrary reuse and the Open World Assumption, it is impossible to anticipate all contexts in which an entity will be used.  We propose an alternative semantics for owl:sameAs that partitions the original relation into a hierarchy of subrelations.  The subrelation to which an identity statement belongs depends on the dataset in which the statement occurs.  Adding future assertions may change the subrelation to which an identity statement belongs, resulting in a context-dependent and non-monotone semantics.  We show that this more complicated semantics is better able to characterize the pragmatic use of owl:sameAs as observed in Linked Open Datasets. 1111111\n",
      "  2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity relations are at the foundation of the Semantic Web and the Linked Data initiative.  In many instances the classical interpretation of identity is too strong for practical purposes.  This is particularly the case when two entities are considered the same in some but not all contexts.  Unfortunately, modeling the specific contexts in which an identity relation holds is cumbersome and, due to arbitrary reuse and the Open World Assumption, it is impossible to anticipate all contexts in which an entity will be used.  We propose an alternative semantics for owl:sameAs that partitions the original relation into a hierarchy of subrelations.  The subrelation to which an identity statement belongs depends on the dataset in which the statement occurs.  Adding future assertions may change the subrelation to which an identity statement belongs, resulting in a context-dependent and non-monotone semantics.  We show that this more complicated semantics is better able to characterize the pragmatic use of owl:sameAs as observed in Linked Open Datasets. 1111111\n",
      "  2222222\n",
      "Identity relations are at the foundation of the Semantic Web and the Linked Data initiative.  In many instances the classical interpretation of identity is too strong for practical purposes.  This is particularly the case when two entities are considered the same in some but not all contexts.  Unfortunately, modeling the specific contexts in which an identity relation holds is cumbersome and, due to arbitrary reuse and the Open World Assumption, it is impossible to anticipate all contexts in which an entity will be used.  We propose an alternative semantics for owl:sameAs that partitions the original relation into a hierarchy of subrelations.  The subrelation to which an identity statement belongs depends on the dataset in which the statement occurs.  Adding future assertions may change the subrelation to which an identity statement belongs, resulting in a context-dependent and non-monotone semantics.  We show that this more complicated semantics is better able to characterize the pragmatic use of owl:sameAs as observed in Linked Open Datasets. 1111111\n",
      "  2222222\n",
      "This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms. First, the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks. Second, automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts, which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms. The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks. Finally, a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list. Our experiments on a large-scale database of web pages have provided very positive results. 1111111\n",
      "In this paper we present Triplify – a simplistic but effective approach to publish Linked Data from relational databases. Triplify is based on mapping HTTP-URI requests onto relational database queries. Triplify transforms the resulting relations into RDF statements and publishes the data on the Web in various RDF serializations, in particular as Linked Data. The rationale for developing Triplify is that the largest part of information on the Web is already stored in structured form, often as data contained in relational databases, but usually published by Web applications only as HTML mixing structure, layout and content. In order to reveal the pure structured information behind the current Web, we have implemented Triplify as a light-weight software component, which can be easily integrated into and deployed by the numerous, widely installed Web applications. Our approach includes a method for publishing update logs to enable incremental crawling of linked data sources. Triplify is complemented by a library of configurations for common relational schemata and a REST-enabled data source registry. Triplify configurations containing mappings are provided for many popular Web applications, including osCommerce, WordPress, Drupal, Gallery, and phpBB. We will show that despite its light-weight architecture Triplify is usable to publish very large datasets, such as 160GB of geo data from the OpenStreetMap project. 2222222\n",
      "This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms. First, the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks. Second, automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts, which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms. The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks. Finally, a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list. Our experiments on a large-scale database of web pages have provided very positive results. 1111111\n",
      "An analysis of the leading social video sharing platform YouTube reveals a high amount of community feedback through comments for published videos as well as through meta ratings for these comments. In this paper, we present an in-depth study of commenting and comment rating behavior on a sample of more than 6 million comments from about 67,000 YouTube videos for which we analyzed various dependencies between comments, views, comment ratings and topic categories. Furthermore, we also study the influence of sentiment expressed in comments on the ratings for these comments using the SentiWordNet thesaurus, a lexical WordNet-based resource containing sentiment annotations. Finally, to predict community acceptance for comments not yet rated, we build different classifiers for the prediction of ratings for these comments. The results of our large-scale evaluations are promising and indicate that community feedback on already rated comments can help to filter new unrated comments or suggest particularly useful but still unrated comments. 2222222\n",
      "The huge amount of the available information in the Web creates the need of effective information extraction systems that are able to produce metadata that satisfy user s information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn extraction models. The production of such corpora can be significantly facilitated by annotation tools that are able to annotate, according to a defined ontology, not only named entities but most importantly relations between them. This paper describes the BOEMIE ontology-based annotation tool which is able to locate blocks of text that correspond to specific types of named entities, fill tables corresponding to ontology concepts with those named entities and link the filled tables based on relations defined in the domain ontology. Additionally, it can perform annotation of blocks of text that refer to the same topic. The tool has a user-friendly interface, supports automatic pre-annotation, annotation comparison as well as customization to other annotation schemata. The annotation tool has been used in a large scale annotation task involving 3,000 web pages regarding athletics. It has also been used in another annotation task involving 503 web pages with medical information, in different languages. 1111111\n",
      "  2222222\n",
      "The huge amount of the available information in the Web creates the need of effective information extraction systems that are able to produce metadata that satisfy user s information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn extraction models. The production of such corpora can be significantly facilitated by annotation tools that are able to annotate, according to a defined ontology, not only named entities but most importantly relations between them. This paper describes the BOEMIE ontology-based annotation tool which is able to locate blocks of text that correspond to specific types of named entities, fill tables corresponding to ontology concepts with those named entities and link the filled tables based on relations defined in the domain ontology. Additionally, it can perform annotation of blocks of text that refer to the same topic. The tool has a user-friendly interface, supports automatic pre-annotation, annotation comparison as well as customization to other annotation schemata. The annotation tool has been used in a large scale annotation task involving 3,000 web pages regarding athletics. It has also been used in another annotation task involving 503 web pages with medical information, in different languages. 1111111\n",
      "Can we automatically generate representative and diverse views of the world's landmarks from community-contributed collections on the web? Community-contributed collections of media on the web are a becoming a vast, rich resource for image and video on a long-tailed array of topics.  We use a combination of context- and content-based tools to generate representative sets of images for location-driven features and landmarks, a common search task. To do that, we using location and other metadata, as well as tags associated with images, and the images' visual features. We present an approach to extracting tags that represent landmarks. We show how to use unsupervised methods to extract representative views and images for each landmark. This approach can potentially scale to provide better search and representation for every landmark, worldwide.  We evaluate the system in the context of web image search using a real-life dataset of 110,000 images from the San Francisco area. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The huge amount of the available information in the Web creates the need of effective information extraction systems that are able to produce metadata that satisfy user s information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn extraction models. The production of such corpora can be significantly facilitated by annotation tools that are able to annotate, according to a defined ontology, not only named entities but most importantly relations between them. This paper describes the BOEMIE ontology-based annotation tool which is able to locate blocks of text that correspond to specific types of named entities, fill tables corresponding to ontology concepts with those named entities and link the filled tables based on relations defined in the domain ontology. Additionally, it can perform annotation of blocks of text that refer to the same topic. The tool has a user-friendly interface, supports automatic pre-annotation, annotation comparison as well as customization to other annotation schemata. The annotation tool has been used in a large scale annotation task involving 3,000 web pages regarding athletics. It has also been used in another annotation task involving 503 web pages with medical information, in different languages. 1111111\n",
      "  2222222\n",
      "The paper gives a comprehensive overview over the results, the concepts and the methods which were developed and used to create the Pronouncing Dictionary of Austrian German (ÖAWB) and the Austrian Pronouncing Database ADABA. The ÖAWB contains 42,000 entries which are based on a large audio corpus of 75,964 realisations of two model speakers each from Austria, Germany and Switzerland. The ADABA database provides 9 different ways to search the data. It also contains 24 model texts and another 30 texts showing linguistic and phonetic variation in Austria and in the other German speaking countries. The codification of Austrian standard pronunciation was based on the concept of German as a pluricentric language and on the concept of  media presentation language . Austrian pronunciation forms are presented in parallel with those of Germany and Switzerland to allow the comparison of differences between linguistically close national varieties of a language. The paper also gives a detailed characterisation of the software (transcriber, database) which was developed during the project that was supported by the Austrian national broadcasting corporation ORF and the University for Music and Dramatic Arts in Graz. Some of the software and the data can be obtained from the web site www.adaba.at. 1111111\n",
      "We analyze knowledge production in Computer Science by means of coauthorship networks. For this, we consider 30 graduate programs of different regions of the world, being 8 programs in Brazil, 16 in North America (3 in Canada and 13 in the United States), and 6 in Europe (2 in France, 1 in Switzerland and 3 in the United Kingdom). We use a dataset that consists of 176,537 authors and 352,766 publication entries distributed among 2,176 publication venues. The results obtained for different metrics of collaboration social networks indicate the process of knowledge production has a ˘changed differently for each region. Research is increasingly done in teams across different fields of Computer Science. The size of the giant component indicates the existence of isolated collaboration groups in the European network, contrasting to the degree of connectivity found in the Brazilian and North-American counterparts. We also analyzed the temporal evolution of the social networks representing the three regions. The number of authors per paper experienced an increase in a time span of 12 years. We observe that the number of collaborations between authors grows faster than the number of authors, benefiting from the existing network structure. The temporal evolution shows differences between well-established fields, such as Databases and Computer Architecture, and emerging fields, like Bioinformatics and Geoinformatics. The patterns of collaboration analyzed in this paper contribute to an overall understanding of Computer Science research in different geographical regions that could not be achieved without the use of complex networks and a large publication database. 2222222\n",
      "The paper gives a comprehensive overview over the results, the concepts and the methods which were developed and used to create the Pronouncing Dictionary of Austrian German (ÖAWB) and the Austrian Pronouncing Database ADABA. The ÖAWB contains 42,000 entries which are based on a large audio corpus of 75,964 realisations of two model speakers each from Austria, Germany and Switzerland. The ADABA database provides 9 different ways to search the data. It also contains 24 model texts and another 30 texts showing linguistic and phonetic variation in Austria and in the other German speaking countries. The codification of Austrian standard pronunciation was based on the concept of German as a pluricentric language and on the concept of  media presentation language . Austrian pronunciation forms are presented in parallel with those of Germany and Switzerland to allow the comparison of differences between linguistically close national varieties of a language. The paper also gives a detailed characterisation of the software (transcriber, database) which was developed during the project that was supported by the Austrian national broadcasting corporation ORF and the University for Music and Dramatic Arts in Graz. Some of the software and the data can be obtained from the web site www.adaba.at. 1111111\n",
      "LC-STAR II is a follow-up project of the EU funded project LC-STAR (Lexica and Corpora for Speech-to-Speech Translation Components, IST-2001-32216). LC-STAR II develops large lexica containing information for speech processing in ten languages targeting especially automatic speech recognition and text to speech synthesis but also other applications like speech-to-speech translation and tagging. The project follows by large the specifications developed within the scope of LC-STAR covering thirteen languages: Catalan, Finnish, German, Greek, Hebrew, Italian, Mandarin Chinese, Russian, Turkish, Slovenian, Spanish, Standard Arabic and US-English. The ten new LC-STAR II languages are: Brazilian-Portuguese, Cantonese, Czech, English-UK, French, Hindi, Polish, Portuguese, Slovak, and Urdu. The project started in 2006 with a lifetime of two years. The project is funded by a consortium, which includes Microsoft (USA), Nokia (Finland), NSC (Israel), Siemens (Germany) and Harmann/Becker (Germany). The project is coordinated by UPC (Spain) and validation is performed by SPEX (The Netherlands), and CST (Denmark). The developed language resources will be shared among partners.This paper presents a summary of the creation of word lists and lexica and an overview of adaptations of the specifications and conceptual representation model from LC-STAR to the new languages. The validation procedure will be presented too. 2222222\n",
      "The paper gives a comprehensive overview over the results, the concepts and the methods which were developed and used to create the Pronouncing Dictionary of Austrian German (ÖAWB) and the Austrian Pronouncing Database ADABA. The ÖAWB contains 42,000 entries which are based on a large audio corpus of 75,964 realisations of two model speakers each from Austria, Germany and Switzerland. The ADABA database provides 9 different ways to search the data. It also contains 24 model texts and another 30 texts showing linguistic and phonetic variation in Austria and in the other German speaking countries. The codification of Austrian standard pronunciation was based on the concept of German as a pluricentric language and on the concept of  media presentation language . Austrian pronunciation forms are presented in parallel with those of Germany and Switzerland to allow the comparison of differences between linguistically close national varieties of a language. The paper also gives a detailed characterisation of the software (transcriber, database) which was developed during the project that was supported by the Austrian national broadcasting corporation ORF and the University for Music and Dramatic Arts in Graz. Some of the software and the data can be obtained from the web site www.adaba.at. 1111111\n",
      "The first release of the German Ph@ttSessionz speech database contains read and spontaneous speech from 864 adolescent speakers and is the largest database of its kind for German. It was recorded via the WWW in over 40 public schools in all dialect regions of Germany. In this paper, we present a cross-sectional study of f0 measurements on this database. The study documents the profound changes in male voices at the age 13-15. Furthermore, it shows that on a perceptive mel-scale, there is little difference in the relative f0 variability for male and female speakers. A closer analysis reveals that f0 variability is dependent on the speech style and both the length and the type of the utterance. The study provides statistically reliable voice parameters of adolescent speakers for German. The results may contribute to making spoken dialog systems more robust by restricting user input to utterances with low f0 variability. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opinionated social media like product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or demote some target products. For reviews to reflect genuine user experiences and opinions, such spam reviews should be detected. Prior works on opinion spam focused on detecting fake reviews and individual spammers. However, a spammer group (a group of reviewers who work collaboratively to write fake reviews) is even more damaging and can take total control of the sentiment on the target product due to its size. This paper studies spam detection in the collaborative setting, i.e., to discover review spammer groups. The proposed method first uses a frequent itemset mining method to find a set of candidate groups. It then uses several behavioral models derived from the collusion phenomenon among spammers and relation models based on inter-relationships among products, groups, and individual reviewers to detect spammer groups. Additionally, we also built a labeled group spam dataset. To the best of our knowledge, this is the first labeled dataset on group opinion spam. Experimental results show that the proposed method is highly effective and it outperforms multiple strong baselines including both the state-of-the-art learning to rank and supervised classification algorithms. 1111111\n",
      "  2222222\n",
      "Opinionated social media like product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or demote some target products. For reviews to reflect genuine user experiences and opinions, such spam reviews should be detected. Prior works on opinion spam focused on detecting fake reviews and individual spammers. However, a spammer group (a group of reviewers who work collaboratively to write fake reviews) is even more damaging and can take total control of the sentiment on the target product due to its size. This paper studies spam detection in the collaborative setting, i.e., to discover review spammer groups. The proposed method first uses a frequent itemset mining method to find a set of candidate groups. It then uses several behavioral models derived from the collusion phenomenon among spammers and relation models based on inter-relationships among products, groups, and individual reviewers to detect spammer groups. Additionally, we also built a labeled group spam dataset. To the best of our knowledge, this is the first labeled dataset on group opinion spam. Experimental results show that the proposed method is highly effective and it outperforms multiple strong baselines including both the state-of-the-art learning to rank and supervised classification algorithms. 1111111\n",
      "Biomedical Knowledge Organization Systems (KOS) play a key role in enriching information in order to make them machine understandable. This is done through semantic annotation which consists in the association of concept labels taken from KOS with pieces of digital information taken from the source to annotate. However, the dynamic nature of these KOS directly impacts on the annotations, creating a mismatch between the enriched data and the concept labels. This PhD study addresses the evolution of semantic annotations due to the evolution of KOS and aims at proposing an approach to automatize the maintenance of semantic annotations. 2222222\n",
      "Opinionated social media like product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or demote some target products. For reviews to reflect genuine user experiences and opinions, such spam reviews should be detected. Prior works on opinion spam focused on detecting fake reviews and individual spammers. However, a spammer group (a group of reviewers who work collaboratively to write fake reviews) is even more damaging and can take total control of the sentiment on the target product due to its size. This paper studies spam detection in the collaborative setting, i.e., to discover review spammer groups. The proposed method first uses a frequent itemset mining method to find a set of candidate groups. It then uses several behavioral models derived from the collusion phenomenon among spammers and relation models based on inter-relationships among products, groups, and individual reviewers to detect spammer groups. Additionally, we also built a labeled group spam dataset. To the best of our knowledge, this is the first labeled dataset on group opinion spam. Experimental results show that the proposed method is highly effective and it outperforms multiple strong baselines including both the state-of-the-art learning to rank and supervised classification algorithms. 1111111\n",
      "In this work we propose a new strategy for the authorship identification problem and we test it on an example from Romanian literature: did Radu Albala found the continuation of Mateiu Caragiale s novel Sub pecetea tainei, or did he write himself the respective continuation? The proposed strategy is based on the similarity of rankings of function words; we compare the obtained results with the results obtained by a learning method (namely Support Vector Machines -SVM- with a string kernel). 2222222\n",
      "It is a challenging task to match similar or related terms/expressions in NLP and Text Mining applications. Two typical areas in need for such work are terminology and ontology constructions, where terms and concepts are extracted and organized into certain structures with various semantic relations. In the EU BOOTSTrep Project we test various techniques for matching terms that can assist human domain experts in building and enriching ontologies. This paper reports on a work in which we evaluated a text comparing and clustering tool for this task. Particularly, we explore the feasibility of matching related terms with their definitions. Ontology terms, such as Gene Ontology terms, are often assigned with detailed definitions, which provide a fundamental information source for detecting relations between terms. Here we focus on the exploitation of term definitions for the term matching task. Our experiment shows that the tool is capable of grouping many related terms using their definitions. 1111111\n",
      "This paper reports on the design and construction of a bio-event annotated corpus which was developed with a specific view to the acquisition of semantic frames from biomedical corpora. We describe the adopted annotation scheme and the annotation process, which is supported by a dedicated annotation tool. The annotated corpus contains 677 abstracts of biomedical research articles. 2222222\n",
      "It is a challenging task to match similar or related terms/expressions in NLP and Text Mining applications. Two typical areas in need for such work are terminology and ontology constructions, where terms and concepts are extracted and organized into certain structures with various semantic relations. In the EU BOOTSTrep Project we test various techniques for matching terms that can assist human domain experts in building and enriching ontologies. This paper reports on a work in which we evaluated a text comparing and clustering tool for this task. Particularly, we explore the feasibility of matching related terms with their definitions. Ontology terms, such as Gene Ontology terms, are often assigned with detailed definitions, which provide a fundamental information source for detecting relations between terms. Here we focus on the exploitation of term definitions for the term matching task. Our experiment shows that the tool is capable of grouping many related terms using their definitions. 1111111\n",
      "Reasoning on OWL ontologies is known to be intractable in the worst-case, which is a serious problem because in practice, most OWL ontologies have large Aboxes, i.e., numerous assertions about individuals and their relations. We propose a technique that uses a summary of the ontology (summary Abox ) to reduce reasoning to a small subset of the original Abox, and prove that our techniques are sound and complete.We demonstrate the scalability of this technique for consistency detection in 4 ontologies, the largest of which has 6.5 million role assertions. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a challenging task to match similar or related terms/expressions in NLP and Text Mining applications. Two typical areas in need for such work are terminology and ontology constructions, where terms and concepts are extracted and organized into certain structures with various semantic relations. In the EU BOOTSTrep Project we test various techniques for matching terms that can assist human domain experts in building and enriching ontologies. This paper reports on a work in which we evaluated a text comparing and clustering tool for this task. Particularly, we explore the feasibility of matching related terms with their definitions. Ontology terms, such as Gene Ontology terms, are often assigned with detailed definitions, which provide a fundamental information source for detecting relations between terms. Here we focus on the exploitation of term definitions for the term matching task. Our experiment shows that the tool is capable of grouping many related terms using their definitions. 1111111\n",
      "  2222222\n",
      "Requirement engineering is emerging as an increasingly important discipline for supporting Web application development, as these are designed to satisfy diverse stakeholder needs, additional functional, information, multimedia and usability requirements as compared to traditional software applications. Moreover, when considering innovative e-commerce applications, value-based requirements engineering is an extremely relevant methodology which exploits the concept of economic value during the requirements engineering activity. In contrast, most of the methodologies proposed for the development of Web applications, primarily focus on the system design, and paying less attention to the requirements engineering, and specifically to value-based requirement engineering. Focusing this aspect, the paper presents integration of value-based requirement engineering models to WebML models using our recently proposed VIP Business Modeling Framework. The integration process is demonstrated using a well-known e-commerce application example by first presenting example VIP business models and then deriving WebML process, structural and other models from these business models. 1111111\n",
      "The Linked Open Data (LOD) initiative has been one of the successful manifestations of Semantic Web efforts over the last two decades, with near-exponential growth of LOD datasets in the initial years. Entities and datasets on LOD are naturally discrete, making them amenable to both well-defined reasoning and retrieval procedures that ultimately return lists or sets of resource identifiers fulfilling some criteria (whether stating user intent or using pattern-matching query languages like SPARQL). In recent years, representation learning algorithms have witnessed a powerful ascent in mainstream Artificial Intelligence, fueled in part by the adoption and refinement of neural network architectures like Recurrent Neural Nets and skip-grams, and by empirical successes such as achieved in the natural language processing and knowledge discovery communities by word and graph embeddings. Large datasets, which are almost always required by such algorithms, make it possible to train and release models openly. In some cases, open models can even be released based on proprietary datasets like Twitter corpora. In this paper, we propose that the Semantic Web community position it- self as a pre-eminent research leader in this space by leveraging the vast and diverse collection of structured datasets that are currently available on Linked Open Data, to build out a corresponding continuous-space equivalent. 2222222\n",
      "Requirement engineering is emerging as an increasingly important discipline for supporting Web application development, as these are designed to satisfy diverse stakeholder needs, additional functional, information, multimedia and usability requirements as compared to traditional software applications. Moreover, when considering innovative e-commerce applications, value-based requirements engineering is an extremely relevant methodology which exploits the concept of economic value during the requirements engineering activity. In contrast, most of the methodologies proposed for the development of Web applications, primarily focus on the system design, and paying less attention to the requirements engineering, and specifically to value-based requirement engineering. Focusing this aspect, the paper presents integration of value-based requirement engineering models to WebML models using our recently proposed VIP Business Modeling Framework. The integration process is demonstrated using a well-known e-commerce application example by first presenting example VIP business models and then deriving WebML process, structural and other models from these business models. 1111111\n",
      "Traditional approaches to ontology development have a large lapse between the time when a user using the ontology has found a need to extend it and the time when it does get extended, which can be weeks or months. We propose a new approach to ontology development and data annotation where users can extend the ontology on the fly as they add metadata, creating terms that can be immediately adopted by others and eventually become standardized. This approach combines traditional ontology development where terms are discussed and where agreement must be reached before they become part of the core standard, and a crowdsourced approach where expert users (the crowd) can add terms as needed to support their work. We have implemented this approach as a socio-technical system that includes: 1) a crowdsourcing platform to support metadata creation and addition of new terms, 2) a range of social editorial processes to make standardization decisions for those new terms, and 3) a framework for ontology revision and updates to the metadata created with the previous version of the ontology. We present a prototype implementation of this approach for the paleoclimate community, the Linked Earth Platform, containing over 600 datasets and with over 50 active contributors. Users are doing science with their data while extending the ontology, therefore producing useful high-quality metadata. 2222222\n",
      "Most of today's web content is designed for human consumption, which makes it difficult for software tools to access them readily. Even web content that is automatically generated from back-end databases is usually presented without the original structural information. In this paper, we present an automated information extraction algorithm that can extract the relevant attribute-value pairs from product descriptions across different sites. A notion, called structural-semantic entropy, is used to locate the data of interest on web pages, which measures the density of occurrence of relevant information on the DOM tree representation of web pages. Our approach is less labor-intensive and insensitive to changes in web-page format. Experimental results on a large number of real-life web page collections are encouraging and confirm the feasibility of the approach, which has been successfully applied to detect false drug advertisements on the Web due to its capacity in associating the attributes of records with their respective values. 1111111\n",
      "  2222222\n",
      "Most of today's web content is designed for human consumption, which makes it difficult for software tools to access them readily. Even web content that is automatically generated from back-end databases is usually presented without the original structural information. In this paper, we present an automated information extraction algorithm that can extract the relevant attribute-value pairs from product descriptions across different sites. A notion, called structural-semantic entropy, is used to locate the data of interest on web pages, which measures the density of occurrence of relevant information on the DOM tree representation of web pages. Our approach is less labor-intensive and insensitive to changes in web-page format. Experimental results on a large number of real-life web page collections are encouraging and confirm the feasibility of the approach, which has been successfully applied to detect false drug advertisements on the Web due to its capacity in associating the attributes of records with their respective values. 1111111\n",
      "Although personalized search has been proposed for many years and many personalization strategies have been investigated, it is still unclear whether personalization is consistently effective on different queries for different users, and under different search contexts. In this paper, we study the problem and get some preliminary conclusions. We present a large-scale personalized search evaluation framework based on search logs and then evaluate five personalized search strategies (including two click-based and three profile-based ones) using 12-day MSN search logs. By analyzing the results, we reveal that personalized search has significant improvement over common web search on some queries but it also has little effect on other queries (e.g., queries with small click entropy) and even harms search accuracy under some situations. Furthermore, we show that click-based personalization strategies perform consistently and considerablely well while profile-based ones are unstable in our experiments. We also reveal that both long-term and short-term contexts are very important in improving search performance for profile-based personalized search strategies. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most of today's web content is designed for human consumption, which makes it difficult for software tools to access them readily. Even web content that is automatically generated from back-end databases is usually presented without the original structural information. In this paper, we present an automated information extraction algorithm that can extract the relevant attribute-value pairs from product descriptions across different sites. A notion, called structural-semantic entropy, is used to locate the data of interest on web pages, which measures the density of occurrence of relevant information on the DOM tree representation of web pages. Our approach is less labor-intensive and insensitive to changes in web-page format. Experimental results on a large number of real-life web page collections are encouraging and confirm the feasibility of the approach, which has been successfully applied to detect false drug advertisements on the Web due to its capacity in associating the attributes of records with their respective values. 1111111\n",
      "  2222222\n",
      "A picture is worth a thousand words, we often say, yet many areas are in demand of sophisticated visualization techniques, and the Semantic Web is not an exception. The size and complexity of ontologies and Linked Data in the Semantic Web constantly grows and the diverse backgrounds of the users and application areas multiply at the same time. Providing users with visual representations and sophisticated interaction techniques can significantly aid the exploration and understanding of the domains and knowledge represented by ontologies and Linked Data. There is no one-size-fits-all solution but different use cases demand different visualization and interaction techniques. Ultimately, providing better user interfaces, visual representations and interaction techniques will foster user engagement and likely lead to higher quality results in different applications employing ontologies and proliferate the consumption of Linked Data. 1111111\n",
      "With the LUCID Endpoint, we demonstrate how companies can utilize Linked Data technology to provide major data items for their business partners in a timely manner, machine readable and with open and extensible schemata. The main idea is to provide a Linked Data infrastructure which enables all partners to fetch, as well as to clone and to synchronize datasets from other partners over the network. This concept allows for building of networks of business partners much like as social network but in a distributed manner. It furthermore provides a technical infrastructure for business communication acts such as supply chain communication or master data management. 2222222\n",
      "A picture is worth a thousand words, we often say, yet many areas are in demand of sophisticated visualization techniques, and the Semantic Web is not an exception. The size and complexity of ontologies and Linked Data in the Semantic Web constantly grows and the diverse backgrounds of the users and application areas multiply at the same time. Providing users with visual representations and sophisticated interaction techniques can significantly aid the exploration and understanding of the domains and knowledge represented by ontologies and Linked Data. There is no one-size-fits-all solution but different use cases demand different visualization and interaction techniques. Ultimately, providing better user interfaces, visual representations and interaction techniques will foster user engagement and likely lead to higher quality results in different applications employing ontologies and proliferate the consumption of Linked Data. 1111111\n",
      "Ontodia is an open-source diagramming and visual exploration tool for linked data and ontologies. Here, we present an extension of the Ontodia data query functionalities. We evaluate different types and configurations of word embeddings for improving recall and flexibility of the Ontodia natural language interface. The demonstration will focus especially on the new query functionalities, where Ontodia will be applied to Wikidata as underlying dataset. 2222222\n",
      "A picture is worth a thousand words, we often say, yet many areas are in demand of sophisticated visualization techniques, and the Semantic Web is not an exception. The size and complexity of ontologies and Linked Data in the Semantic Web constantly grows and the diverse backgrounds of the users and application areas multiply at the same time. Providing users with visual representations and sophisticated interaction techniques can significantly aid the exploration and understanding of the domains and knowledge represented by ontologies and Linked Data. There is no one-size-fits-all solution but different use cases demand different visualization and interaction techniques. Ultimately, providing better user interfaces, visual representations and interaction techniques will foster user engagement and likely lead to higher quality results in different applications employing ontologies and proliferate the consumption of Linked Data. 1111111\n",
      "Are we in the semantic web/linked data community effectively attempting to make possible a new literacy - one of data rather than document analysis? By opening up data beyond the now familiar hand crafted Web 2 mash up of data about X plus geography, what are we trying to do, really? Is the goal at least in part to enable net citizens rather than only geeks the ability to pick up, explore, blend, interogate and represent data sources so that we may draw our own statistically informed conclusions about information, and thereby build new knowledge in ways not readily possible before without access to these data seas? If we want citizens rather than just scientists or statisticians or journalists for that matter to be able to pour over data and ask statistically sophisticated questions of comparison and contrast betewen times, places and people, does that mission re-order our research priorities at all? If the goal is to enpower citizens to be able to make use of data, what do we need to make this vision real beyond attending to Tim Berners-Lee's call to 'free your data'? The purpose of this talk therefore will be to look at key interaction issues around defining and delivering a useful, usable *data explorotron* for citizens. In particular, we'll consider who is a 'citizen user' and what access to and tools for linked data sense making means in this case. From that perspective, we'll consider research issues around discovery, exploration, interrogation and representation of data for not only a single wild data source but especially for multiple wild heterogeneous data sources. I hope this talk may help frame some stepping stones towards useful and usable interaction with linked data, and look forward to input from the community to refine such a new literacy agenda further. 2222222\n",
      "In order to reduce the cost of building domain ontologies manually, in this paper, we integrate a domain ontology development environment: DODDLE-OWL and an ontology search engine: Swoogle. We propose a method and a tool for domain ontology construction reusing texts and existing ontologies for a target domain extracted by Swoogle. In the evaluation, we applied the method to a particular field of law and evaluated the acquired ontologies. 1111111\n",
      "  2222222\n",
      "In order to reduce the cost of building domain ontologies manually, in this paper, we integrate a domain ontology development environment: DODDLE-OWL and an ontology search engine: Swoogle. We propose a method and a tool for domain ontology construction reusing texts and existing ontologies for a target domain extracted by Swoogle. In the evaluation, we applied the method to a particular field of law and evaluated the acquired ontologies. 1111111\n",
      "For ontology reuse and integration, a number of approaches have been devised that aim at identifying modules, i.e., suitably small sets of “relevant” axioms from ontologies. Here we consider three logically sound notions of modules: MEX modules, only applicable to inexpressive ontologies; modules based on semantic locality, a sound approximation of the ﬁrst; and modules based on syntactic locality, a sound approximation of the second (and thus the ﬁrst), widely used since these modules can be extracted from OWL DL ontologies in time polynomial in the size of the ontology. In this paper we investigate the quality of both approximations over a large corpus of ontologies, using our own implementation of semantic locality, which is the ﬁrst to our knowledge. In particular, we show with statistical signiﬁcance that, in most cases, there is no diﬀerence between the two module notions based on locality; where they diﬀer, the additional axioms can either be easily ruled out or their number is relatively small. We classify the axioms that explain the rare diﬀerences into four kinds of “culprits” and discuss which of those can be avoided by extending the deﬁnition of syntactic locality. Finally, we show that diﬀerences between MEX and locality-based modules occur for a minority of ontologies from our corpus and largely aﬀect (approximations of ) expressive ontologies – this conclusion relies on a much larger and more diverse sample than existing comparisons between MEX and syntactic locality-based modules. 2222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to reduce the cost of building domain ontologies manually, in this paper, we integrate a domain ontology development environment: DODDLE-OWL and an ontology search engine: Swoogle. We propose a method and a tool for domain ontology construction reusing texts and existing ontologies for a target domain extracted by Swoogle. In the evaluation, we applied the method to a particular field of law and evaluated the acquired ontologies. 1111111\n",
      "  2222222\n",
      "The use of natural language identiﬁers as reference for ontology elements—in addition to the URIs required by the Semantic Web standards—is of utmost importance based on their predominance in the human everyday life, i.e. speech or print media. Depending on the context, different names can be chosen for one and the same element, and the same element can be referenced by different names. Here homonymy and synonymy are the main cause of the appearance of ambiguity in perceiving which concrete unique ontology element ought to be referenced by a speciﬁc natural language identiﬁer describing an entity. We propose a novel method to resolve entity references under the aspect of ambiguity which explores only formal background knowledge represented in RDF graph structures. The key idea of our domain independent approach is to build an entity network with the most likely referenced ontology elements by constructing spanning graphs based on spreading activation (SA). Additional to the exploitation of complex graph structures we devise a new ranking technique that characterises the likelihood of entities in  this network, i.e. interpretation contexts. Experiments in a highly polysemic domain show the ability of the algorithm to retrieve the correct ontology elements in almost all cases. 1111111\n",
      "The Semantic Web was designed to unambiguously define and use ontologies to encode data and knowledge on the Web. Many people find it difficult, however, to write complex RDF statements and queries because it requires familiarity with the appropriate ontologies and the terms they define. We describe a framework that eases the experiences in authoring and querying RDF data, in which we focus on automatically finding a set of appropriate Semantic Web ontology terms from a set of words used as the labels of nodes and edges in an incoming semantic graph. 2222222\n",
      "The use of natural language identiﬁers as reference for ontology elements—in addition to the URIs required by the Semantic Web standards—is of utmost importance based on their predominance in the human everyday life, i.e. speech or print media. Depending on the context, different names can be chosen for one and the same element, and the same element can be referenced by different names. Here homonymy and synonymy are the main cause of the appearance of ambiguity in perceiving which concrete unique ontology element ought to be referenced by a speciﬁc natural language identiﬁer describing an entity. We propose a novel method to resolve entity references under the aspect of ambiguity which explores only formal background knowledge represented in RDF graph structures. The key idea of our domain independent approach is to build an entity network with the most likely referenced ontology elements by constructing spanning graphs based on spreading activation (SA). Additional to the exploitation of complex graph structures we devise a new ranking technique that characterises the likelihood of entities in  this network, i.e. interpretation contexts. Experiments in a highly polysemic domain show the ability of the algorithm to retrieve the correct ontology elements in almost all cases. 1111111\n",
      "  2222222\n",
      "The use of natural language identiﬁers as reference for ontology elements—in addition to the URIs required by the Semantic Web standards—is of utmost importance based on their predominance in the human everyday life, i.e. speech or print media. Depending on the context, different names can be chosen for one and the same element, and the same element can be referenced by different names. Here homonymy and synonymy are the main cause of the appearance of ambiguity in perceiving which concrete unique ontology element ought to be referenced by a speciﬁc natural language identiﬁer describing an entity. We propose a novel method to resolve entity references under the aspect of ambiguity which explores only formal background knowledge represented in RDF graph structures. The key idea of our domain independent approach is to build an entity network with the most likely referenced ontology elements by constructing spanning graphs based on spreading activation (SA). Additional to the exploitation of complex graph structures we devise a new ranking technique that characterises the likelihood of entities in  this network, i.e. interpretation contexts. Experiments in a highly polysemic domain show the ability of the algorithm to retrieve the correct ontology elements in almost all cases. 1111111\n",
      "With the fast development of Semantic Web, more and more RDF and OWL ontologies are created and shared. The effective management, such as storage, inference and query, of these ontologies on databases gains increasing attentions. This paper addresses ontology query answering by means of a Datalog program, specifically tailored to bridge ontologies and databases. Introducing meta integrity constraints inspired by epistemic interpretations, we believe such a Datalog program suitable to capture ontologies in the DB favor, while keeping reasoning tractable -- Here, we present a logical equivalent knowledge base whose (sound and complete) inference system appears to a Datalog program. As such, a deductive RDF database is responsible for SPARQL query answering involved in OWL and SWRL. Bi-directional strategies, taking advantage of both forward and backward chaining, are then studied to support for this kind of customized Datalog programs, returning exactly answers to the query with respect to its logical framework. 2222222\n"
     ]
    }
   ],
   "source": [
    "vaDF = vmDF.values.tolist()\n",
    "paperVA = pd.DataFrame()\n",
    "conta = 0\n",
    "vaVacia = ''\n",
    "for i in vaDF:\n",
    "    if (conta == 3):\n",
    "        conta = 0\n",
    "        conta = conta + 1\n",
    "    indice = i[1]\n",
    "    uriBas = i[4]\n",
    "    abstractBas = i[0]\n",
    "    uriSim = i[16]\n",
    "    abstractSim = i[7]\n",
    "    scoreGraphDB = i[14]\n",
    "    rankGraphDB = i[5]\n",
    "    valorSpacy = scoreSPACY(abstractBas, abstractSim)\n",
    "    paperVA =  paperVA.append({'index':indice,'uri paper bas':uriBas,'abstract paper bas':abstractBas,'uri paper sim':uriSim,'abstract paper sim':abstractSim,'Score GraphDB':scoreGraphDB,'Rank GraphDB':rankGraphDB,'Score Spacy':valorSpacy,'Rank Spacy':conta,'error':vaVacia}, ignore_index=True)\n",
    "    abstractSim = ''\n",
    "    valorSpacy = 0\n",
    "    conta = 0\n",
    "    #print(uriBas,\"**\",abstractBas,\"**\",uriSim,\"**\",abstractSim,\"**\",scoreGraphDB,\"**\",rankGraphDB)\n",
    "paperVA.to_excel('validacion_automatica.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumen1 = 'The recent emergence of mashup tools has refueled research on end user development, i.e., on enabling end users without programming skills to compose own applications. Yet, similar to what happened with analogous promises in web service composition and business process management, research has mostly focused on technology and, as a consequence, has failed its objective. Plain technology (e.g., SOAP/WSDL web services) or simple modeling languages (e.g., Yahoo! Pipes) dont convey enough meaning to non-programmers.We propose a domain-specific approach to mashups that speaks the language of the user, i.e., that is aware of the terminology, concepts, rules, and conventions (the domain) the user is comfortable with. We show what developing a domain-specific mashup tool means, which role the mashup meta-model and the domain model play and how these can be merged into a domain-specific mashup meta-model.We apply the approach implementing a mashup tool for the research evaluation domain. Our user study confirms that domain-specific mashup tools indeed lower the entry barrier to mashup development.'\n",
    "resumen2 = 'In this demonstration, we present ResEval Mash, a mashup platform for research evaluation, i.e., for the assessment of the productivity or quality of researchers, teams, institutions, journals, and the like - a topic most of us are acquainted with. The platform is specically tailored to the need of sourcing data about scientic publications and researchers from the Web, aggregating them, computing metrics (also complex and ad-hoc ones), and visualizing them. ResEval Mash is a hosted mashup platform with a client side editor and runtime engine, both running inside a common web browser. It supports the processing of also large amounts of data, a feature that is achieved via the sensible distribution of the respective computation steps over client and server. Our preliminary user study shows that ResEval Mash indeed has the power to enable domain experts to develop own mashups (research evaluation metrics); other mashup platforms rather support skilled developers. The reason for this success is ResEval Mashs domain-specicity.'\n",
    "valor = scoreSPACY(resumen1, resumen2)\n",
    "numero = valor[0][0]\n",
    "print(valor)\n",
    "print(numero)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
